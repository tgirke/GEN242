


















































































[{"body":"\nOverview This course introduces algorithms, statistical methods and data analysis programming routines relevant for genome biology. It consists of three main components: lectures, hands-on practicals and student course projects. The lecture topics cover databases, sequence (NGS) analysis, phylogenetics, comparative genomics, genome-wide profiling methods, network biology and more. The hands-on practicals include homework assignments and course projects focusing on data analysis programming of next generation genome data using command-line tools on a computer cluster and the programming environment R. Depending on student interests, one or more specialty topics may be included, such as the analysis of single cell (e.g. scRNA-Seq) experiments, multi-omics data, or the development of web-based analysis tools (e.g. Shiny Apps).\nWho should take this class? Students with a strong interest and motivation in acquiring the skills required for mastering the computational aspects of modern genome research. The class is mainly targeting graduate students but senior undergraduate students are welcome to enroll as well. The main audience of this class usually includes students from bioscience, biomedical and bioengineering programs as well as CS and statistics students with interest in computational biology.\nCan I audit this class? It is possible to audit this class. However, due to the emphasis on active participation in practicals and course projects, students usually learn much more if they enroll into the class rather than auditing it in a passive manner.\nInclusive Classroom Code of Conduct The instructors, organizers and participants of this class are committed to create and maintain an environment in which everyone can learn and thrive in ways inclusive of their diverse backgrounds and identities. All teaching material, activities, virtual or face-to-face interactions related to this course are intended to provide a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, Veteran status, citizenship status, personal appearance, race, caste, color, religion, or sexual identity and orientation. Violations of this code of conduct should be reported to the main instructor (Thomas Girke, thomas.girke@ucr.edu). For additional information about this important topic please visit the website of UCR’s Office of Diversity, Equity \u0026 Inclusion (DEI).\nAccessibility UC Riverside is committed to creating a learning environment that meets the needs of its diverse student body. If students anticipate or experience any barriers to learning in this course, they are welcome to discuss their concerns with the instructor(s). If students have a disability, they may want to contact the Student Disability Resource Center to accommodate their specific needs. As faculty we are responsible for collaborating with SDRC to ensure that the students enrolled in our classes have access to the SDRC approved classroom accommodations and services outlined in each student’s Letter of Accommodation. A Letter of Accommodation may be issued at any time during the quarter. In addition, Disability Services are free, voluntary, private, and not part of your transcript.\n","categories":"","description":"","excerpt":"\nOverview This course introduces algorithms, statistical methods and …","ref":"/about/introduction/","tags":"","title":"Introduction"},{"body":"  GitHub in GEN242  Note, this class will make heavy use of GitHub Homework assignments will be submitted and graded on GitHub Classroom Course projects will also use private GitHub repositories: one repository for each course project (shared among students of each project) Each student will need a personal GitHub account. They can be created here. GitHub provides an unlimited number of free public repositories to each user. Via GitHub Education students can sign up for an extended number of free private GitHub accounts (see here). For beginners this quick guide may be useful  What are Git and GitHub?  Git is a version control system similar to SVN GitHub is an online social coding service based on Git Combined Git/GitHub: environment for version control and social coding  Installing Git  Install on Windows, OS X and Linux When using it from RStudio, it needs to find the Git executable  Git Basics from Command-Line Also try interactive git tutorial.\n  Finding help from command-line\ngit \u003ccommand\u003e --help    Initialize a directory as a Git repository\ngit init    Add specific files to Git repository (staging area)\ngit add myfile    Add all files recursively\nTo ignore specific files (e.g. temp files), list them in a .gitignore file in your repository’s root directory. Regular expressions are supported. See here for more details.\ngit add -A :/    After editing file(s) in your repos, record a snapshot of the staging area\ngit commit -am \"some edits\"    GitHub Basics from Command-Line   Generate a new remote repository on GitHub online or use hub or GitHub CLI command-line wrappers for this. To avoid errors with the online method, do not initialize the new repository with README, license, or .gitignore files. You can add these files after your project has been pushed to GitHub.\ngit remote add origin git@github.com:\u003cuser_name\u003e/\u003crepos_name\u003e.git    Push updates to remote. Next time one can just use git push\ngit push -u origin master    Clone existing remote repository\ngit clone git@github.com:\u003cuser_name\u003e/\u003crepos_name\u003e.git    Before working on project, update local git repos\ngit pull    Make changes and recommit local to remote\ngit commit -am \"some edits\"; git push -u origin master    Note, in order to work with private GitHub repositories, users need to activate in their GitHub account under Settings\nas authentication method either a personal access token or an ssh key. The latter ssh key method is usually preferred. Instructions are given in the corresponding links of the previous sentence.\nTo commit to a private GitHub repository from the HPCC cluster, you need to generate an ssh key from your home account using the standard Linux ssh-keygen method as described here, and then upload the newly generated public ssh key of your HPCC account located under ~/.ssh/id_rsa.pub to GitHub.\nExercise Run the following git/github excercise from the command-line. Do this after creating a GitHub repos according to the instructions above or online as outlined here.\ngit clone git@github.com:\u003cuser or org\u003e/\u003crepo name\u003e cd \u003crepo name\u003e git pull touch test # Creates empty file for testing git add test # or use '-A' for all git commit -am \"some edits\" git push ##-\u003e Edit test file online and then run `git pull` to inspect changes  Online file upload Useful for new users who want to upload their homework assignments to GitHub but are not familiar enough with the command-line yet.\n Press Add file button on your repository, and then Upload files. Under the file path window add required subdirectory structure and a dummy file name (e.g. Homework/HW1/dummy.txt) After this press Upload files and upload any file (e.g. homework) to the newly create directory. After this the initial dummy file can be deleted. The latter is necessary since empty directories are not visible on GitHub.  Using GitHub from RStudio   After installing Git (see here), set path to Git executable in Rstudio:\n Tools \u003e Global Options \u003e Git/SVN    If needed, log in to GitHub account and create repository. Use option Initialize this repository with a README.\n  Clone repository by copying \u0026 pasting URL from repository into RStudio’s ‘Clone Git Repository’ window:\n File \u003e New Project \u003e Version Control \u003e Git \u003e Provide URL    Now do some work (e.g. add an R script), commit and push changes as follows:\n Tools \u003e Version Control \u003e Commit    Check files in staging area and press Commit Button\n  To commit changes to GitHub, press Push Button\n  Shortcuts to automate above routines are here\n  To resolve password issues, follow instructions here.\n  ","categories":"","description":"","excerpt":"  GitHub in GEN242  Note, this class will make heavy use of GitHub …","ref":"/tutorials/github/github/","tags":"","title":"GitHub Introduction"},{"body":"Course title Data Analysis in Genome Biology GEN242 - Spring 2022\nPrintable syllabus See Google Doc version here.\nInstructor Name: Thomas Girke Email: thomas.girke@ucr.edu Office location: virtual via Zoom Office hour: Tue 4:30 - 5:30 PM \u0026 Fri 4:00 - 5:00 PM Zoom URL: privately shared\nDescription Introduction to algorithms, statistical methods and data analysis programming routines relevant for genome biology. The class consists of three main components: lectures, hands-on practicals and student course projects. The lecture topics cover databases, sequence (NGS) analysis, phylogenetics, comparative genomics, genome-wide profiling methods, network biology and more. The hands-on practicals include homework assignments and course projects focusing on data analysis programming of next generation genome data using command-line tools on a computer cluster and the programming environment R. Credit: 4 units (2x 1.5 hours lectures, 1 hour discussion)\nObjectives of course  Acquire understanding of algorithms used in bioinformatics Obtain hands-on experience in large scale data analysis.  Prerequisites The main prerequisite for this course is a strong interest in acquiring the skills required for mastering the computational aspects of modern genome research.\nStructure of course Two lectures per week (1.5 hours each) plus one discussion section (1 hour). During the first weeks the discussion section will be used for data analysis tutorials using Linux command-line tools and R.\nTime Lecture: Tue/Thu 2:00-3:20 PM Discussion: Thu 3:30-4:20 PM\nLocation Online via video conferencing software\nGrading  Homework assignments: 40% Scientific paper presentation: 20% Course project presentations: 20% Final project report: 20%  Additional details about the grading system are provided in this table (see both tabs).\nGrading policy: Given the diverse educational background of the students in GEN242, all assignments are designed to be solvable by students from both experimental and quantitative disciplines, including those with no or only limited prior experience in programming and/or data modeling. The weight of each of the four gradable components in this class is given above in percent. (1) The homeworks include 8-10 assignments throughout the class. They cover algorithms and data analysis programming problems using the R language. The grading of these assignments is mainly based on correctness, reproducibility and reusability of the analysis code. (2-4) Students will work on a Challenge Project (individually or in group) addressing a specific data analysis problem in genome data sciences. As part of their project, students will present a scientific paper (2) closely related to their project (see reading list for details). The results of the Challenge Projects (3) will be presented and discussed by each student at the end of the course. In addition, each student will write a detailed analysis report (4) of the assigned course project. The latter will be written in the style of a scientific publication and should include a detailed description of the results including all analysis code to fully reproduce the project results followed by a critical discussion of the outcomes. The grading of both the paper and project presentations (2-3) includes anonymous feedback from all students as well as the instructor, where understanding of the material, clarity of the oral presentations and critical thinking are the main grading criteria. The final project reports (4) will be graded by the instructor with an emphasis on scientific and coding accuracy, overall understanding of the topic, as well as reproducibility of the results.\nMaterials needed Students are expected to bring to each class meeting a laptop with a functional wireless connection and a recent internet browser version (e.g. Firefox, Chrome or Safari) preinstalled. Tablet computers with mobile operating systems are not suitable for running the required software. User accounts on a research computer cluster will be provided at the beginning of the course. To log in to the cluster, students also need to install a terminal application for their operating system (e.g. iTerm2 on OS X, and PuTTY or MobaXterm on Windows) as well as a file exchange software such as FileZilla. In addition, a recent version of R and RStudio should be installed.\nIf possible students may want to attend class sessions from a monitor setup with either one large monitor (wide enough to display several windows) or two separate monitors. This allows simultaneous viewing of presentations on one screen and following along hands-on practicals on the other screen.\nSchedule    Week Topic     Week 1 Course Introduction    Databases and Software for Genome Biology    Discussion: Introduction to Linux and HPC    Reading: A1, T1, T2   Week 2 Sequencing Technologies    Discussion: Introduction to R    Reading: A2-A4, T3   Week 3 Sequence Alignments and Searching    Multiple Sequence Alignments    Discussion: Programming in R and Parallel Evaluations    Reading: A5-A6, T4-T5   Week 4 Short Read Alignment Algorithms    Discussion: Basics of NGS Analysis    Reading: A7-A10, T6   Week 5 Gene Expression Analysis, Microarrays, bulk RNA-Seq and scRNA-Seq    Discussion: NGS Workflow Overview; RNA-Seq Analysis    Reading: A11-A15, T7-T8   Week 6 Analysis of ChIP-Seq Experiments    Discussion: ChIP-Seq Analysis    Reading: A16-A18, T9-T10   Week 7 Students present publication related to their chosen course project    Discussion: Q\u0026A about papers    Reading: A19-A23   Week 8 Clustering algorithms    Pathway and GO annotation systems    Discussion: Gene Set Enrichment Analysis    Reading: A24-A26, T7 (Sec 3.14-3.15), T11   Week 9 Genome and Transcriptome Assembly Algorithms    Profile HMMs for Protein Family Modeling    Introduction to Phylogenetics    Discussion: Graphics and Data Visualization    Reading: A27-A29, T12   Week 10 Final presentations of student data analysis projects    Discussion: Tips and tricks for efficient data analysis programming    Reading: A30-A31, T3 (Sec 12,13-17)    Reading list Journal articles A1. Huber W, Carey VJ, Gentleman R, Anders S, Carlson M, Carvalho BS, Bravo HC, Davis S, Gatto L, Girke T, et al (2015) Orchestrating high-throughput genomic analysis with Bioconductor. Nat Methods 12: 115–121\nA2. Metzker, M. L., Jan 2010. Sequencing technologies - the next generation. Nat Rev Genet 11 (1), 31–46.\nA3. Needleman SB, Wunsch CD (1970) A general method applicable to the search for similarities in the amino acid sequence of two proteins. J Mol Biol 48, 443-453.\nA4. Smith TF, Waterman MS (1981) Identification of common molecular subsequences. J Mol Biol 147, 195-197.\nA5. Corpet F (1988) Multiple sequence alignment with hierarchical clustering. Nucleic Acids Res 16, 10881-90.\nA6. Altschul, S. F., Gish, W., Miller, W., Myers, E. W., Lipman, D. J., Oct 1990. Basic local alignment search tool. J Mol Biol 215 (3), 403–410.\nA7. Li, H, Durbin, R (2009) Fast and accurate short read alignment with Burrows-Wheeler transform. Bioinformatics, 25: 1754-1760.\nA8. Dobin, A., Davis, C.A., Schlesinger, F., Drenkow, J., Zaleski, C., Jha, S., Batut, P., Chaisson, M., Gingeras, T.R., 2012. STAR: ultrafast universal RNA-seq aligner. Bioinformatics 29, 15–21.\nA9. Langmead, B, Salzberg, S L (2012) Fast gapped-read alignment with Bowtie 2. Nat Methods, 9: 357-359.\nA10. Kim D, Langmead B, Salzberg SL (2015) HISAT: a fast spliced aligner with low memory requirements. Nat Methods 12: 357–360\nA11. Bray NL, Pimentel H, Melsted P, Pachter L (2016) Near-optimal probabilistic RNA-seq quantification. Nat Biotechnol. doi: 10.1038/nbt.3519\nA12. Love MI, Huber W, Anders S (2014) Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol 15: 550\nA13. Zhou X, Lindsay H, Robinson MD (2014) Robustly detecting differential expression in RNA sequencing data using observation weights. Nucleic Acids Res 42: e91\nA14. Anders, S, Reyes, A, Huber, W (2012) Detecting differential usage of exons from RNA-seq data. Genome Res, 22: 2008-2017.\nA15. Soneson, C, Delorenzi, M (2013) A comparison of methods for differential expression analysis of RNA-seq data. BMC Bioinformatics, 14: 91-91.\nA16. Zhang Y, Liu T, Meyer CA, Eeckhoute J, Johnson DS, Bernstein BE, Nussbaum C, Myers RM, Brown M, Li W, et al (2008) Model-based analysis of ChIP-Seq (MACS). Genome Biol. doi: 10.1186/gb-2008-9-9-r137\nA17. Wilbanks EG, Facciotti MT (2010) Evaluation of algorithm performance in ChIP-seq peak detection. PLoS One. doi: 10.1371/journal.pone.0011471.\nA18. Landt et al. (2012) ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia. Genome Res, 22: 1813-1831.\nA19. McLeay, Robert C, and Timothy L Bailey. 2010. “Motif Enrichment Analysis: A Unified Framework and an Evaluation on ChIP Data.” BMC Bioinformatics 11: 165.\nA20. Machanick, P, Bailey, T L (2011) MEME-ChIP: motif analysis of large DNA datasets. Bioinformatics, 27: 1696-1697.\nA21. Tompa, M, N Li, T L Bailey, G M Church, B De Moor, E Eskin, A V Favorov, et al. 2005. “Assessing Computational Tools for the Discovery of Transcription Factor Binding Sites.” Nature Biotechnology 23 (1): 137–44.\nA22. DePristo MA, Banks E, Poplin R, Garimella KV, Maguire JR, Hartl C, Philippakis AA, del Angel G, Rivas MA, Hanna M, et al (2011) A framework for variation discovery and genotyping using next-generation DNA sequencing data. Nat Genet 43: 491–498.\nA23. Shihab HA, Rogers MF, Gough J, Mort M, Cooper DN, Day INM, Gaunt TR, Campbell C (2015) An integrative approach to predicting the functional effects of non-coding and coding sequence variation. Bioinformatics 31: 1536–1543.\nA24. Raymond JW, Blankley CJ, Willett P (2003) Comparison of chemical clustering methods using graph- and fingerprint-based similarity measures. J Mol Graph Model 21: 421–433.\nA25. Subramanian A, Tamayo P, Mootha VK, Mukherjee S, Ebert BL, Gillette MA, Paulovich A, Pomeroy SL, Golub TR, Lander ES, et al (2005) Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc Natl Acad Sci U S A 102: 15545–15550.\nA26. Ashburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, Davis AP, Dolinski K, Dwight SS, Eppig JT, et al (2000) Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet 25: 25–29.\nA27. Zyla J, Marczyk M, Domaszewska T, Kaufmann SHE, Polanska J, Weiner J (2019) Gene set enrichment for reproducible science: comparison of CERNO and eight other algorithms. Bioinformatics 35: 5146–5154\nA28. Alkan, C, Sajjadian, S, Eichler, E E (2011) Limitations of next-generation genome sequence assembly. Nat Methods, 8: 61-65.\nA29. Eddy SR (1998) Profile hidden Markov models. Bioinformatics 14: 755–763.\nA30. Grabherr, M G, Haas, B J, Yassour, M, Levin, J Z, Thompson, D A, Amit, I, Adiconis, X, Fan, L, Raychowdhury, R, Zeng, Q, Chen, Z, Mauceli, E, Hacohen, N, Gnirke, A, Rhind, N, di Palma, F, Birren, B W, Nusbaum, C, Lindblad-Toh, K, Friedman, N, Regev, A (2011) Full-length transcriptome assembly from RNA-Seq data without a reference genome. Nat Biotechnol, 29: 644-652.\nA31. Zeitouni, B, Boeva, V, Janoueix-Lerosey, I, Loeillet, S, Legoix-ne, P, Nicolas, A, Delattre, O, Barillot, E (2010) SVDetect: a tool to identify genomic structural variations from paired-end and mate-pair sequencing data. Bioinformatics, 26: 1895-1896.\nA32. Ronquist F, Teslenko M, van der Mark P, Ayres DL, Darling A, Höhna S, Larget B, Liu L, Suchard MA, Huelsenbeck JP (2012) MrBayes 3.2: efficient Bayesian phylogenetic inference and model choice across a large model space. Syst Biol 61: 539–542.\nA33. Law CW, Zeglinski K, Dong X, Alhamdoosh M, Smyth GK, Ritchie ME (2020) A guide to creating design matrices for gene expression experiments. F1000Res 9: 1444\nA34. Langfelder P, Luo R, Oldham MC, Horvath S (2011) Is my network module preserved and reproducible? PLoS Comput Biol 7: e1001057\nA35. McInnes L, Healy J, Melville J (2018) UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv [stat.ML]\nA36. Rodriguez MZ, Comin CH, Casanova D, Bruno OM, Amancio DR, Costa L da F, Rodrigues FA (2019) Clustering algorithms: A comparative approach. PLoS One 14: e0210236\nA37. Shulse CN, Cole BJ, Ciobanu D, Lin J, Yoshinaga Y, Gouran M, Turco GM, Zhu Y, O’Malley RC, Brady SM, et al (2019) High-Throughput Single-Cell Transcriptome Profiling of Plant Cell Types. Cell Rep 27: 2241–2247.e4\nA38. Subramanian A, Tamayo P, Mootha VK, Mukherjee S, Ebert BL, Gillette MA, Paulovich A, Pomeroy SL, Golub TR, Lander ES, et al (2005) Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc Natl Acad Sci U S A 102: 15545–15550\nA39. Sun S, Zhu J, Ma Y, Zhou X (2019) Accuracy, robustness and scalability of dimensionality reduction methods for single-cell RNA-seq analysis. Genome Biol 20: 269\nTutorials T1. GitHub Introduction\nT2. Introduction to Computer Clusters and Linux\nT3. Introduction to R\nT4. Programming in R\nT5. Parallel R\nT6. NGS Analysis Basics\nT7. NGS Workflows\nT8. RNA-Seq Workflow\nT9. scRNA-Seq Embedding Methods\nT10. ChIP-Seq Workflow\nT11. R Markdown\nT12. Functional Enrichment Analysis\nT13. Clustering and Network Analysis\nT14. Project Data\nT15. Data Visualization\nT16. Shiny Apps\nT17. Building R Packages\nT18. dplyr, tidyr and some SQLite\nT19. Advanced: Common Workflow Language (CWL)\nBooks Note: there is no need to purchase any books for this course as most reading material will be based on journal articles!\nGeneral Jonathan Pevsner (2009) Bioinformatics and Functional Genomics. Wiley-Blackwell; 2nd Edition, 992 pages.\nAlgorithms Jones N and Pevzner P (2004) An Introduction to Bioinformatics Algorithms. MIT Press, Massachusetts, 435 pages.\nSequence Analysis Durbin, R, Eddy, S, Krogh, A, Mitchison, G. (1998) Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press, UK, 356 pages.\nParida L (2008) Pattern Discovery in Bioinformatics: Theory \u0026 Algorithms. CRC Press, London, 526 pages.\nProfiling Bioinformatics Gentleman, R, Carey, V, Dudoit, S, Irizarry, R, Huber, W (2005) Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Springer, New York, 473 pages.\nPhylogenetics Felsenstein, J (2004) Inferring Phylogenies. Sinauer, Massachusetts, 664 pages.\nParadis (2006) Analysis of Phylogenetics and Evolution with R. Springer, New York, 211 pages.\n","categories":"","description":"","excerpt":"Course title Data Analysis in Genome Biology GEN242 - Spring 2022 …","ref":"/about/syllabus/","tags":"","title":"Syllabus - GEN242"},{"body":"  HPCC Cluster Overview The HPCC Cluster (formerly called biocluster) is a shared research computing system available at UCR. The HPCC website is available here.\nWhat Is a Computer Cluster?   A computer cluster is an assembly of CPU units, so called computer nodes that work together to perform many computations in parallel. To achieve this, an internal network (e.g. Infiniband interconnect) connects the nodes to a larger unit, while a head node controls the load and traffic across the entire system.\n  Usually, users log into the head node to submit their computer requests via srun to a queuing system provided by resource management and scheduling software, such as SGE, Slurm or TORQUE/MAUI. The queuing system distributes the processes to the computer nodes in a controlled fashion.\n  Because the head node controls the entire system, users should never run computing jobs on the head node directly!\n  For code testing purposes, one can log into one of the nodes with srun --pty bash -l and run jobs interactively. Alternatively, one can log into the test node owl via ssh.\n  Hardware Infrastructure Computer nodes  Over 8,000 CPU cores 130 Intel, AMD and GPU nodes 32-128 CPU cores per node 256-1,024 GB of RAM per node 12 GPU nodes, each with total of over 80,000 cuda cores  Interconnect  FDR IB @56Gbs  Storage  Parallel GPFS storage system with 3.0 PB usable space File system scales to over 50 PB Backup of same architecture and similar amount  User traffic  Computing tasks need to be submitted via sbatch or srun HPCC Cluster headnode only for login, not for computing tasks! Monitor cluster activity: squeue or jobMonitor (qstatMonitor)  Manuals  HPCC Cluster Manual Linux Manual  Linux Basics Log into HPCC Cluster  Login command on OS X or Linux  ssh -XY user@cluster.hpcc.ucr.edu  Type password\n  Windows: provide same information in a terminal application like MobaXterm (Putty is outdated and not recommended anymore). Here is an annimated usage introduction for MobaXterm.\n Host name: cluster.hpcc.ucr.edu User name: … Password: …    Mac OS X: use the built-in Terminal or iTerm2. For remote X11 graphics display support, XQuartz needs to be intalled from here (also see video here).\n  Additional login information can be found on the corresponding HPCC manuals:\n Login page: here SSH Keys: here Duo Multifactor Authenication: here UCR Duo Manual: here    Important Linux Commands The following provides a short overview of important shell commands. Much more detailed information can be found on HPCC’s Linux tutorials.\nFinding help\nman \u003cprogram_name\u003e  List content of current directory\nls  Print current working directory\npwd pwd -P # returns physical location in case one followed symbolic link  Search in files and directories\ngrep  Word count\nwc  Create directory\nmkdir  Delete files and directories\nrm  Move and rename files\nmv  Copy files from internet to pwd\nwget  Viewing files\nless  File Exchange GUI applications\n Windows: WinSCP Mac OS X: CyberDuck Win/OS X/Linux: FileZilla   FileZilla settings with an SSH key. For generating SSH keys see here. \n\nSCP: via command-line (Manual)\nAdvantages of this method include: batch up/downloads and ease of automation.\nscp file user@remotehost:/home/user/ # From local to remote scp user@remotehost:/home/user/file . # From remote to local  RSYNC: via command-line (Manual)\nAdvantages of this method include: same as SCP plus differential update options and viewing of directory content.\nPrint (view) content of remote directory\nrsync user@remotehost:~/somedirectory/*  Download directory or file(s)\nrsync -avzhe ssh user@remotehost:~/somedirectory . # -a: recursive archive mode (thus -r not required), also preserves permissions, time stamps, etc # -v: verbose # -z: compress data during transfer # -h: print messages in human-readable format # -e: specifies transfer protocol; using ssh here provides encryption during transfer # --delete: files that were deleted on source will be deleted also in backup-destination # -n: for testing use this dry-run option, but drop '-e ssh' in this case  Upload directory or file(s)\nrsync -avzhe ssh somedirectory user@hostname:~/  STD IN/OUT/ERR, Redirect \u0026 Wildcards Wildcard * to specify many files\nfile.*  Redirect ls output to file\nls \u003e file  Specify file as input to command\ncommand \u003c myfile  Append output of command to file\ncommand \u003e\u003e myfile  Pipe STDOUT of one command to another command\ncommand1 | command2  Turn off progress info\ncommand \u003e /dev/null  Pipe output of grep to wc\ngrep pattern file | wc  Print STDERR to file\ngrep pattern nonexistingfile 2 \u003e mystderr  Homework Assignment (HW2) See HW2 page here.\nPermissions and ownership List directories and files\nls -al  The previous command shows something like this for each file/dir: drwxrwxrwx. The meaning of this syntax is as follows:\n d: directory rwx: read, write and execute permissions, respectively  first triplet: user permissions (u) second triplet: group permissions (g) third triplet: world permissions (o)    Example for assigning write and execute permissions to user, group and world\nchmod ugo+rx my_file   + causes the permissions selected to be added - causes them to be removed = causes them to be the only permissions that the file has.  When performing the same operation on many files with subdirectories then one can use -R for recursive behavior.\nchmod -R ugo+rx my_dir  Since directories have to be executable the capital X option can be useful which applies only to directories but not to files. The following will assign drwxr-xr-x to directories and -rw-r--r-- to files and hidden files.\nchmod -R ugo-x,u+rwX,go+rX,go-w ./* ./.[!.]*  Syntax for changing user \u0026 group ownership\nchown \u003cuser\u003e:\u003cgroup\u003e \u003cfile or dir\u003e  Symbolic Links Symbolic links are short nicknames to files and directories that save typing of their full paths.\nln -s original_filename new_nickname  Software and module system  Over 2,000 software tools are currently installed on HPCC Cluster Custom installs in user accounts via various mechanisms, e.g. environment management systems such as conda Most common research databases used in bioinformatics are available Support of most common programming languages used in research computing A module system is used to facilitate the management of software tools. This includes any number of versions of each software. New software install requests can be sent to support@hpcc.ucr.edu. To use software manged under the module system, users need to learn using some basic commands. The most common commands are listed below.  Print available modules\nmodule avail  Print available modules starting with R\nmodule avail R  Load default module R\nmodule load R  Unload specific module R\nmodule unload R/4.2.0  Load specific R version\nmodule unload R/4.1.2  List loaded modules\nmodule list  Big data storage Each user account on HPCC Cluster comes only with 20GB of disk space. Much more disk space is available in a dedicated bigdata directory. How much space depends on the subscription of each user group. The path of bigdata and bigdata-shared is as follows:\n /bigdata/labname/username /bigdata/labname/shared  All lab members share the same bigdata pool. The course number gen242 is used as labname for user accounts adminstered under GEN242 (here /bigdata/gen242/shared).\nThe disk usage of home and bigdata can be monitored on the HPCC Cluster Dashboard.\nQueuing system: Slurm HPCC Cluster uses Slurm as queuing and load balancing system. To control user traffic, any type of compute intensive jobs need to be submitted via sbatch or srun (see below) to the computer nodes. Much more detailed information on this topic can be found on these sites:\n UCR HPCC Manual Slurm Documentation Torque/Slurm Comparison Switching from Torque to Slurm Slurm Quick Start Tutorial  Job submission with sbatch Print information about queues/partitions available on a cluster.\nsinfo  Compute jobs are submitted with sbatch via a submission script (here script_name.sh).\nsbatch script_name.sh  The following sample submission script (script_name.sh) executes an R script named my_script.R.\n#!/bin/bash -l #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1G #SBATCH --time=1-00:15:00 # 1 day and 15 minutes #SBATCH --mail-user=useremail@address.com #SBATCH --mail-type=ALL #SBATCH --job-name=\"some_test\" #SBATCH -p batch # Choose queue/parition from: intel, batch, highmem, gpu, short Rscript my_script.R  STDOUT and STDERROR of jobs will be written to files named slurm-\u003cjobid\u003e.out or to a custom file specified under #SBATCH --output in the submission script.\nInteractive sessions with srun This option logs a user in to a computer node of a specified partition (queue), while Slurm monitors and controls the resource request.\nsrun --pty bash -l  Interactive session with specific resource requests\nsrun --x11 --partition=short --mem=2gb --cpus-per-task 4 --ntasks 1 --time 1:00:00 --pty bash -l  The argument --mem limits the amount of RAM, --cpus the number of CPU cores, --time the time how long a session will be active. Under --parition one can choose among different queues and node architectures. Current options under --partition for most users of the HPCC cluster are: intel, batch, highmem, gpu, and short. The latter has a time limit of 2 hours. Note, --x11 will only work when logged in with X11 support. This requires the -X argument when logging in via ssh (see above). On OS X system X11 support is provided by XQuartz which needs to be installed and running on a system prior to loging in to a remote system. If X11 support is not available or broken then one can still connect via srun by dropping the --x11 argument form the srun command.\nMonitoring jobs with squeue List all jobs in queue\nsqueue  List jobs of a specific user\nsqueue -u \u003cuser\u003e  Print more detailed information about a job\nscontrol show job \u003cJOBID\u003e scontrol show jobid -dd \u003cJOBID\u003e  Custom command to summarize and visualize cluster activity\njobMonitor  Deleting and altering jobs Delete a single job\nscancel -i \u003cJOBID\u003e  Delete all jobs of a user\nscancel -u \u003cusername\u003e  Delete all jobs of a certain name\nscancel --name \u003cmyJobName\u003e  Altering jobs with scontrol update. The below example changes the walltime (\u003cNEW_TIME\u003e) of a specific job (\u003cJOBID\u003e).\nscontrol update jobid=\u003cJOBID\u003e TimeLimit=\u003cNEW_TIME\u003e  Resource limits Resourse limits for users can be viewed as follows.\nsacctmgr show account $GROUP format=Account,User,Partition,GrpCPUs,GrpMem,GrpNodes --ass | grep $USER  Similarly, one can view the limits of the group a user belongs to.\nsacctmgr show account $GROUP format=Account,User,Partition,GrpCPUs,GrpMem,GrpNodes,GrpTRES%30 --ass | head -3  Text/code editors The following list includes examples of several widely used code editors.\n Vi/Vim/Neovim: Non-graphical (terminal-based) editor. Vi is guaranteed to be available on any system. Vim and Nvim (Neovim) are the improved versions of vi. Emacs: Non-graphical or window-based editor. You still need to know keystroke commands to use it. Installed on all Linux distributions and on most other Unix systems. Pico: Simple terminal-based editor available on most versions of Unix. Uses keystroke commands, but they are listed in logical fashion at bottom of screen. Nano: A simple terminal-based editor which is default on modern Debian systems. Atom: Modern text editor developed by GitHub project.  Why does it matter? To work efficiently on remote systems like a computer cluster, it is essential to learn how to work in a pure command-line interface. GUI environments like RStudio and similar coding environments are not suitable for this. In addition, there is a lot of value of knowing how to work in an environment that is not restricted to a specific programming language. Therefore, this class embraces RStudio where it is useful, but for working on remote systems like HPCC Cluster, it uses Nvim and Tmux. Both are useful for many programming languages. Combinded with the nvim-r plugin they also provide a powerful command-line working environment for R. The following provides a brief introduction to this environment.\nVim overview The following opens a file (here myfile) with nvim (or vim)\nnvim myfile.txt # for neovim (or 'vim myfile.txt' for vim)  Once you are in Nvim, there are three main modes: normal, insert and command mode. The most important commands for switching between the three modes are:\n i: The i key brings you from the normal mode to the insert mode. The latter is used for typing. Esc: The Esc key brings you from the insert mode back to the normal mode. :: The : key starts the command mode at the bottom of the screen.  Use the arrow keys to move your cursor in the text. Using Fn Up/Down key allows to page through the text quicker. In the following command overview, all commands starting with : need to be typed in the command mode. All other commands are typed in the normal mode after pushing the Esc key.\nImportant modifier keys to control vim/nvim\n :w: save changes to file. If you are in editing mode you have to hit Esc first. :q: quit file that has not been changed :wq: save and quit file :!q: quit file without saving any changes  Useful resources for learning vim/nvim  Interactive Vim Tutorial Official Vim Documentation HPCC Linux Manual  Nvim-R-Tmux essentials Terminal-based Working Environment for R: Nvim-R-Tmux.\n Nvim-R-Tmux IDE for R Basics Tmux is a terminal multiplexer that allows to split terminal windows and to detach/reattach to existing terminal sessions. Combinded with the nvim-r plugin it provides a powerful command-line working environment for R where users can send code from a script to the R console or command-line. Both tmux and the nvim-r plugin need to be installed on a system. On HPCC Cluster both are configured in each user account. If this is not the case then follow the quick configuration instructions given in the following subsection.\nQuick configuration in user accounts of UCR’s HPCC Skip these steps if Nvim-R-Tmux is already configured in your account. Or follow the detailed instructions to install Nvim-R-Tmux from scratch on your own system.\n Log in to your user account on HPCC and execute Install_Nvim-R_Tmux (old version: install_nvimRtmux). Alternatively, follow these step-by-step install commands. To enable the nvim-R-tmux environment, log out and in again. Follow usage instructions of next section.  Basic usage of Nvim-R-Tmux The official and much more detailed user manual for Nvim-R is available here. The following gives a short introduction into the basic usage of Nvim-R-Tmux:\n1. Start tmux session (optional)\nNote, running Nvim from within a tmux session is optional. Skip this step if tmux functionality is not required (e.g. reattaching to sessions on remote systems).\ntmux # starts a new tmux session tmux a # attaches to an existing session  2. Open nvim-connected R session\nOpen a *.R or *.Rmd file with nvim and intialize a connected R session with \\rf. This command can be remapped to other key combinations, e.g. uncommenting lines 10-12 in .config/nvim/init.vim will remap it to the F2 key. Note, the resulting split window among Nvim and R behaves like a split viewport in nvim or vim meaning the usage of Ctrl-w w followed by i and Esc is important for navigation.\nnvim myscript.R # or *.Rmd file  3. Send R code from nvim to the R pane\nSingle lines of code can be sent from nvim to the R console by pressing the space bar. To send several lines at once, one can select them in nvim’s visual mode and then hit the space bar. Please note, the default command for sending code lines in the nvim-r-plugin is \\l. This key binding has been remapped in the provided .config/nvim/init.vim file to the space bar. Most other key bindings (shortcuts) still start with the \\ as LocalLeader, e.g. \\rh opens the help for a function/object where the curser is located in nvim. More details on this are given below.\nImportant keybindings for nvim The main advantages of Neovim compared to Vim are its better performance and its built-in terminal emulator facilitating the communication among Neovim and interactive programming environments such as R. Since the Vim and Neovim environments are managed independently, one can run them in parallel on the same system without interfering with each other. The usage of Neovim is almost identical to Vim.\nNvim commands\n \\rf: opens vim-connected R session. If you do this the first time in your user account, you might be asked to create an R directory under ~/. If so approve this action by pressing y. spacebar: sends code from vim to R; here remapped in init.vim from default \\l :split or :vsplit: splits viewport (similar to pane split in tmux) gz: maximizes size of viewport in normal mode (similar to Tmux’s Ctrl-a z zoom utility) Ctrl-w w: jumps cursor to R viewport and back; toggle between insert (i) and command (Esc) mode is required for navigation and controlling the environment. Ctrl-w r: swaps viewports Ctrl-w =: resizes splits to equal size :resize \u003c+5 or -5\u003e: resizes height by specified value :vertical resize \u003c+5 or -5\u003e: resizes width by specified value Ctrl-w H or Ctrl-w K: toggles between horizontal/vertical splits Ctrl-spacebar: omni completion for R objects/functions when nvim is in insert mode. Note, this has been remapped in init.vim from difficult to type default Ctrl-x Ctrl-o. :set mouse=a: enables mouse support and : set mouse-=a disables it :h nvim-R: opens nvim-R’s user manual; navigation works the same as for any Vim/Nvim help document :Rhelp fct_name: opens help for a function from nvim’s command mode with text completion support Ctrl-s and Ctrl-x: freezes/unfreezes vim (some systems)  Important keybindings for tmux Pane-level commands\n Ctrl-a %: splits pane vertically Ctrl-a \": splits pane horizontally Ctrl-a o: jumps cursor to next pane Ctrl-a Ctrl-o: swaps panes Ctrl-a \u003cspace bar\u003e: rotates pane arrangement Ctrl-a Alt \u003cleft or right\u003e: resizes to left or right Ctrl-a Esc \u003cup or down\u003e: resizes to left or right  Window-level comands\n Ctrl-a n: switches to next tmux window Ctrl-a Ctrl-a: switches to previous tmux window Ctrl-a c: creates a new tmux window Ctrl-a 1: switches to specific tmux window selected by number  Session-level comands\n Ctrl-a d: detaches from current session Ctrl-a s: switch between available tmux sesssions $ tmux new -s \u003cname\u003e: starts new session with a specific name $ tmux ls: lists available tmux session(s) $ tmux attach -t \u003cid\u003e: attaches to specific tmux session $ tmux attach: reattaches to session $ tmux kill-session -t \u003cid\u003e: kills a specific tmux session Ctrl-a : kill-session: kills a session from tmux command mode that can be initiated with Ctrl-a :  Nvim IDEs for other languages For other languages, such as Bash, Python and Ruby, one can use the vimcmdline plugin for nvim (or vim). To install it, one needs to copy from the vimcmdline resository the directories ftplugin, plugin and syntax and their files to ~/.config/nvim/. For user accounts of UCR’s HPCC, the above install script install_nvimRtmux includes the install of vimcmdline (since 09-Jun-18).\nThe usage of vimcmdline is very similar to nvim-R. To start a connected terminal session, one opens with nvim a code file with the extension of a given language (e.g. *.sh for Bash or *.py for Python), while the corresponding interactive interpreter session is initiated by pressing the key sequence \\s (corresponds to \\rf under nvim-R). Subsequently, code lines can be sent with the space bar. More details are available here.\n","categories":"","description":"","excerpt":"  HPCC Cluster Overview The HPCC Cluster (formerly called biocluster) …","ref":"/tutorials/linux/linux/","tags":"","title":"Introduction to HPCC Cluster and Linux"},{"body":"Detailed course schedule  ","categories":"","description":"","excerpt":"Detailed course schedule  ","ref":"/about/schedule/","tags":"","title":"Course Schedule"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: [ .Rmd ] [ .R ]\n Overview What is R? R is a powerful statistical environment and programming language for the analysis and visualization of data. The associated Bioconductor and CRAN package repositories provide many additional R packages for statistical data analysis for a wide array of research areas. The R software is free and runs on all common operating systems.\nWhy Using R?  Complete statistical environment and programming language Efficient functions and data structures for data analysis Powerful graphics Access to fast growing number of analysis packages Most widely used language in bioinformatics Is standard for data mining and biostatistical analysis Technical advantages: free, open-source, available for all OSs  Books and Documentation  simpleR - Using R for Introductory Statistics (John Verzani, 2004) - URL Bioinformatics and Computational Biology Solutions Using R and Bioconductor (Gentleman et al., 2005) - URL More on this see “Finding Help” section in UCR Manual - URL  R Working Environments    R Projects and Interfaces\n Some R working environments with support for syntax highlighting and utilities to send code to the R console:\n RStudio: excellent choice for beginners (Cheat Sheet) Basic R code editors provided by Rguis gedit, Rgedit, RKWard, Eclipse, Tinn-R, Notepad++, NppToR Vim-R-Tmux: R working environment based on vim and tmux Emacs (ESS add-on package)  Example: RStudio New integrated development environment (IDE) for R. Highly functional for both beginners and advanced.\n   RStudio IDE\n Some userful shortcuts: Ctrl+Enter (send code), Ctrl+Shift+C (comment/uncomment), Ctrl+1/2 (switch window focus)\nExample: Nvim-R-Tmux Terminal-based Working Environment for R: Nvim-R-Tmux.\n   Nvim-R-Tmux IDE for R\n Install and usage instructions for Nvim-R are provided in this slide show and this tutorial. The most detailed instructions can be found on the Nvim-R_Tmux GitHub repos.\nR Package Repositories  CRAN (\u003e14,000 packages) general data analysis - URL Bioconductor (\u003e2,000 packages) bioscience data analysis - URL Omegahat (\u003e90 packages) programming interfaces - URL RStudio packages - URL  Working routine for tutorials This section provides a short overview of the standard working routine users should use to load R-based tutorials of this website into an R IDE (Nvim-R or RStudio). For Nvim-R on HPCC users can visit the Quick Demo slide here.\n  Download *.Rmd or *.R file. These so called source files are always linked on the top right corner of each tutorial. The ones for this tutorial are here. The file download can be accomplished via wget from the command-line or with the save function in a user’s web browser.\n  Load *.Rmd or *.R file in Neovim (Nvim-R) or RStudio.\n  Send code from code editor to R console by pushing space bar in Neovim (Nvim-R) or Ctrl + Enter in RStudio. In *.Rmd files the code lines are in so called code chunks and only those ones can be sent to the console. To obtain in Neovim a connected R session one has to initiate by pressing the \\rf key combination. For details see here.\n  Installation of R, RStudio and R Packages   Install R for your operating system from CRAN.\n  Install RStudio from RStudio.\n  Install CRAN Packages from R console like this:\ninstall.packages(c(\"pkg1\", \"pkg2\")) install.packages(\"pkg.zip\", repos=NULL)    Install Bioconductor packages as follows:\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") # Installs BiocManager if not available yet BiocManager::version() # Reports Bioconductor version BiocManager::install(c(\"pkg1\", \"pkg2\")) # Installs packages specified under \"pkg1\"    For more details consult the Bioc Install page and BiocInstaller package.\n  Getting Around Startup and Closing Behavior   Starting R: The R GUI versions, including RStudio, under Windows and Mac OS X can be opened by double-clicking their icons. Alternatively, one can start it by typing R in a terminal (default under Linux).\n  Startup/Closing Behavior: The R environment is controlled by hidden files in the startup directory: .RData, .Rhistory and .Rprofile (optional).\n  Closing R:\n  q()  Save workspace image? [y/n/c]:   Note: When responding with y, then the entire R workspace will be written to the .RData file which can become very large. Often it is better to select n here, because a much better working pratice is to save an analysis protocol to an R or Rmd source file. This way one can quickly regenerate all data sets and objects needed in a future session.  Navigating directories List objects in current R session\nls()  Return content of current working directory\ndir()  Return path of current working directory\ngetwd()  Change current working directory\nsetwd(\"/home/user\")  Basic Syntax Create an object with the assignment operator \u003c- or =\nobject \u003c- ...  General R command syntax\nobject \u003c- function_name(arguments) object \u003c- object[arguments]  Instead of the assignment operator one can use the assign function\nassign(\"x\", function(arguments))  To simplify chaining of serveral operations, dplyr (magrittr) provides the %\u003e% (pipe) operator, where x %\u003e% f(y) turns into f(x, y). This way one can pipe together multiple operations by writing them from left-to-right or top-to-bottom. This makes for easy to type and readable code. Details on this are provided in the dplyr tutorial here.\n... %\u003e% ...  Finding help\n?function_name  Load a library/package\nlibrary(\"my_library\")  List functions defined by a library\nlibrary(help=\"my_library\")  Load library manual (PDF or HTML file)\nvignette(\"my_library\")  Execute an R script from within R\nsource(\"my_script.R\")  Execute an R script from command-line (the first of the three options is preferred)\n$ Rscript my_script.R $ R CMD BATCH my_script.R $ R --slave \u003c my_script.R  Data Types Numeric data Example: 1, 2, 3, ...\nx \u003c- c(1, 2, 3) x  ## [1] 1 2 3  is.numeric(x)  ## [1] TRUE  as.character(x)  ## [1] \"1\" \"2\" \"3\"  Character data Example: \"a\", \"b\", \"c\", ...\nx \u003c- c(\"1\", \"2\", \"3\") x  ## [1] \"1\" \"2\" \"3\"  is.character(x)  ## [1] TRUE  as.numeric(x)  ## [1] 1 2 3  Complex data Example: mix of both\nc(1, \"b\", 3)  ## [1] \"1\" \"b\" \"3\"  Logical data Example: TRUE of FALSE\nx \u003c- 1:10 \u003c 5 x  ## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE  !x  ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE  which(x) # Returns index for the 'TRUE' values in logical vector  ## [1] 1 2 3 4  Data Objects Object types  List of common object types  vectors: ordered collection of numeric, character, complex and logical values. factors: special type vectors with grouping information of its components data.frames including modern variants DataFrame, tibbles, etc.: two dimensional structures with different data types matrices: two dimensional structures with data of same type arrays: multidimensional arrays of vectors lists: general form of vectors with different types of elements functions: piece of code Many more …   Simple rules for naming objects and their components  Object, row and column names should not start with a number Avoid spaces in object, row and column names Avoid special characters like ‘#’    Vectors (1D) Definition: numeric or character\nmyVec \u003c- 1:10; names(myVec) \u003c- letters[1:10] myVec \u003c- setNames(1:10, letters[1:10]) # Same as above in single step myVec[1:5]  ## a b c d e ## 1 2 3 4 5  myVec[c(2,4,6,8)]  ## b d f h ## 2 4 6 8  myVec[c(\"b\", \"d\", \"f\")]  ## b d f ## 2 4 6  Factors (1D) Definition: vectors with grouping information\nfactor(c(\"dog\", \"cat\", \"mouse\", \"dog\", \"dog\", \"cat\"))  ## [1] dog cat mouse dog dog cat ## Levels: cat dog mouse  Matrices (2D) Definition: two dimensional structures with data of same type\nmyMA \u003c- matrix(1:30, 3, 10, byrow = TRUE) class(myMA)  ## [1] \"matrix\" \"array\"  myMA[1:2,]  ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 2 3 4 5 6 7 8 9 10 ## [2,] 11 12 13 14 15 16 17 18 19 20  myMA[1, , drop=FALSE]  ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 2 3 4 5 6 7 8 9 10  Data Frames (2D) Definition: data.frames are two dimensional objects with data of variable types\nmyDF \u003c- data.frame(Col1=1:10, Col2=10:1) myDF[1:2, ]  ## Col1 Col2 ## 1 1 10 ## 2 2 9  Tibbles Tibbles are a more modern version of data.frames. Among many other advantages, one can see here that tibbles have a nicer printing bahavior. Much more detailed information on this object class is provided in the dplyr/tidyverse manual section.\nlibrary(tidyverse) as_tibble(iris)  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cfct\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows  Arrays Definition: data structure with one, two or more dimensions\nLists Definition: containers for any object type\nmyL \u003c- list(name=\"Fred\", wife=\"Mary\", no.children=3, child.ages=c(4,7,9)) myL  ## $name ## [1] \"Fred\" ## ## $wife ## [1] \"Mary\" ## ## $no.children ## [1] 3 ## ## $child.ages ## [1] 4 7 9  myL[[4]][1:2]  ## [1] 4 7  Functions Definition: piece of code\nmyfct \u003c- function(arg1, arg2, ...) { function_body }  Subsetting of data objects (1.) Subsetting by positive or negative index/position numbers\nmyVec \u003c- 1:26; names(myVec) \u003c- LETTERS myVec[1:4]  ## A B C D ## 1 2 3 4  (2.) Subsetting by same length logical vectors\nmyLog \u003c- myVec \u003e 10 myVec[myLog]  ## K L M N O P Q R S T U V W X Y Z ## 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  (3.) Subsetting by field names\nmyVec[c(\"B\", \"K\", \"M\")]  ## B K M ## 2 11 13  (4.) Subset with $ sign: references a single column or list component by its name\niris$Species[1:8]  ## [1] setosa setosa setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica  Important Utilities Combining Objects The c function combines vectors and lists\nc(1, 2, 3)  ## [1] 1 2 3  x \u003c- 1:3; y \u003c- 101:103 c(x, y)  ## [1] 1 2 3 101 102 103  The cbind and rbind functions can be used to append columns and rows, respecively.\nma \u003c- cbind(x, y) ma  ## x y ## [1,] 1 101 ## [2,] 2 102 ## [3,] 3 103  rbind(ma, ma)  ## x y ## [1,] 1 101 ## [2,] 2 102 ## [3,] 3 103 ## [4,] 1 101 ## [5,] 2 102 ## [6,] 3 103  Accessing Dimensions of Objects Length and dimension information of objects\nlength(iris$Species)  ## [1] 150  dim(iris)  ## [1] 150 5  Accessing Name Slots of Objects Accessing row and column names of 2D objects\nrownames(iris)[1:8]  ## [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\"  colnames(iris)  ## [1] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\" \"Species\"  Return name field of vectors and lists\nnames(myVec)  ## [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" ## [25] \"Y\" \"Z\"  names(myL)  ## [1] \"name\" \"wife\" \"no.children\" \"child.ages\"  Sorting Objects The function sort returns a vector in ascending or descending order\nsort(10:1)  ## [1] 1 2 3 4 5 6 7 8 9 10  The function order returns a sorting index for sorting an object\nsortindex \u003c- order(iris[,1], decreasing = FALSE) sortindex[1:12]  ## [1] 14 9 39 43 42 4 7 23 48 3 30 12  iris[sortindex,][1:2,]  ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 14 4.3 3.0 1.1 0.1 setosa ## 9 4.4 2.9 1.4 0.2 setosa  sortindex \u003c- order(-iris[,1]) # Same as decreasing=TRUE  Sorting multiple columns\niris[order(iris$Sepal.Length, iris$Sepal.Width),][1:2,]  ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 14 4.3 3.0 1.1 0.1 setosa ## 9 4.4 2.9 1.4 0.2 setosa  Operators and Calculations Comparison Operators Comparison operators: ==, !=, \u003c, \u003e, \u003c=, \u003e=\n1==1  ## [1] TRUE  Logical operators for boolean operations: AND: \u0026, OR: |, NOT: !\nx \u003c- 1:10; y \u003c- 10:1 x \u003e y \u0026 x \u003e 5  ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE  Basic Calculations To look up math functions, see Function Index here\nx + y  ## [1] 11 11 11 11 11 11 11 11 11 11  sum(x)  ## [1] 55  mean(x)  ## [1] 5.5  apply(iris[1:6,1:3], 1, mean)  ## 1 2 3 4 5 6 ## 3.333333 3.100000 3.066667 3.066667 3.333333 3.666667  Reading and Writing External Data Import of tabular data Import of a tab-delimited tabular file\nmyDF \u003c- read.delim(\"myData.xls\", sep=\"\\t\")  Import of Google Sheets. The following example imports a sample Google Sheet from here. Detailed instructions for interacting from R with Google Sheets with the required googlesheets4 package are here.\nlibrary(googlesheets4) gs4_deauth() # Easiest method for reading public access sheets mysheet \u003c- read_sheet(\"1U-32UcwZP1k3saKeaH1mbvEAOfZRdNHNkWK2GI1rpPM\", skip=4) myDF \u003c- as.data.frame(mysheet) myDF  Import from Excel sheets works well with readxl. For details see the readxl package manual here. Note: working with tab- or comma-delimited files is more flexible and highly preferred for automated analysis workflows.\nlibrary(\"readxl\") mysheet \u003c- read_excel(targets_path, sheet=\"Sheet1\")  Additional import functions are described in the readr package section here.\nExport of tabular data write.table(myDF, file=\"myfile.xls\", sep=\"\\t\", quote=FALSE, col.names=NA)  Line-wise import myDF \u003c- readLines(\"myData.txt\")  Line-wise export writeLines(month.name, \"myData.txt\")  Export R object mylist \u003c- list(C1=iris[,1], C2=iris[,2]) # Example to export saveRDS(mylist, \"mylist.rds\")  Import R object mylist \u003c- readRDS(\"mylist.rds\")  Copy and paste into R On Windows/Linux systems\nread.delim(\"clipboard\")  On Mac OS X systems\nread.delim(pipe(\"pbpaste\"))  Copy and paste from R On Windows/Linux systems\nwrite.table(iris, \"clipboard\", sep=\"\\t\", col.names=NA, quote=FALSE)  On Mac OS X systems\nzz \u003c- pipe('pbcopy', 'w') write.table(iris, zz, sep=\"\\t\", col.names=NA, quote=FALSE) close(zz)  Homework 3A Homework 3A: Object Subsetting Routines and Import/Export\nUseful R Functions Unique entries Make vector entries unique with unique\nlength(iris$Sepal.Length)  ## [1] 150  length(unique(iris$Sepal.Length))  ## [1] 35  Count occurrences Count occurrences of entries with table\ntable(iris$Species)  ## ## setosa versicolor virginica ## 50 50 50  Aggregate data Compute aggregate statistics with aggregate\naggregate(iris[,1:4], by=list(iris$Species), FUN=mean, na.rm=TRUE)  ## Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 setosa 5.006 3.428 1.462 0.246 ## 2 versicolor 5.936 2.770 4.260 1.326 ## 3 virginica 6.588 2.974 5.552 2.026  Intersect data Compute intersect between two vectors with %in%\nmonth.name %in% c(\"May\", \"July\")  ## [1] FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE  Merge data frames Join two data frames by common field entries with merge (here row names by.x=0). To obtain only the common rows, change all=TRUE to all=FALSE. To merge on specific columns, refer to them by their position numbers or their column names.\nframe1 \u003c- iris[sample(1:length(iris[,1]), 30), ] frame1[1:2,]  ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 147 6.3 2.5 5.0 1.9 virginica ## 49 5.3 3.7 1.5 0.2 setosa  dim(frame1)  ## [1] 30 5  my_result \u003c- merge(frame1, iris, by.x = 0, by.y = 0, all = TRUE) dim(my_result)  ## [1] 150 11  Graphics in R Advantages  Powerful environment for visualizing scientific data Integrated graphics and statistics infrastructure Publication quality graphics Fully programmable Highly reproducible Full LaTeX and Markdown support via knitr and R markdown Vast number of R packages with graphics utilities  Documentation for R Graphics General\n Graphics Task Page - URL R Graph Gallery - URL R Graphical Manual - URL Paul Murrell’s book R (Grid) Graphics - URL  Interactive graphics\n rggobi` (GGobi) - URL iplots - URL Open GL (rgl) - URL  Graphics Environments Viewing and saving graphics in R\n On-screen graphics postscript, pdf, svg jpeg, png, wmf, tiff, …  Four major graphic environments\n Low-level infrastructure   R Base Graphics (low- and high-level) grid: Manual  High-level infrastructure \\begin{itemize}   lattice: Manual, Intro, Book ggplot2: Manual, Intro, Book  Base Graphics: Overview Important high-level plotting functions\n plot: generic x-y plotting barplot: bar plots boxplot: box-and-whisker plot hist: histograms pie: pie charts dotchart: cleveland dot plots image, heatmap, contour, persp: functions to generate image-like plots qqnorm, qqline, qqplot: distribution comparison plots pairs, coplot: display of multivariant data  Help on graphics functions\n ?myfct ?plot ?par  Preferred Object Types  Matrices and data frames Vectors Named vectors  Scatter Plots Basic Scatter Plot Sample data set for subsequent plots\nset.seed(1410) y \u003c- matrix(runif(30), ncol=3, dimnames=list(letters[1:10], LETTERS[1:3]))  Plot data\nplot(y[,1], y[,2])  All pairs pairs(y)  With labels plot(y[,1], y[,2], pch=20, col=\"red\", main=\"Symbols and Labels\") text(y[,1]+0.03, y[,2], rownames(y))  More examples Print instead of symbols the row names\nplot(y[,1], y[,2], type=\"n\", main=\"Plot of Labels\") text(y[,1], y[,2], rownames(y))  Usage of important plotting parameters\ngrid(5, 5, lwd = 2) op \u003c- par(mar=c(8,8,8,8), bg=\"lightblue\") plot(y[,1], y[,2], type=\"p\", col=\"red\", cex.lab=1.2, cex.axis=1.2, cex.main=1.2, cex.sub=1, lwd=4, pch=20, xlab=\"x label\", ylab=\"y label\", main=\"My Main\", sub=\"My Sub\") par(op)  Important arguments\n mar: specifies the margin sizes around the plotting area in order: c(bottom, left, top, right) col: color of symbols pch: type of symbols, samples: example(points) lwd: size of symbols cex.*: control font sizes For details see ?par  Add regression line plot(y[,1], y[,2]) myline \u003c- lm(y[,2]~y[,1]); abline(myline, lwd=2)  summary(myline)  ## ## Call: ## lm(formula = y[, 2] ~ y[, 1]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40357 -0.17912 -0.04299 0.22147 0.46623 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 0.5764 0.2110 2.732 0.0258 * ## y[, 1] -0.3647 0.3959 -0.921 0.3839 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.3095 on 8 degrees of freedom ## Multiple R-squared: 0.09589, Adjusted R-squared: -0.01712 ## F-statistic: 0.8485 on 1 and 8 DF, p-value: 0.3839  Log scale Same plot as above, but on log scale\nplot(y[,1], y[,2], log=\"xy\")  Add a mathematical expression plot(y[,1], y[,2]); text(y[1,1], y[1,2], expression(sum(frac(1,sqrt(x^2*pi)))), cex=1.3)  Homework 3B Homework 3B: Scatter Plots\nLine Plots Single data set plot(y[,1], type=\"l\", lwd=2, col=\"blue\")  Many Data Sets Plots line graph for all columns in data frame y. The split.screen function is used in this example in a for loop to overlay several line graphs in the same plot.\nsplit.screen(c(1,1))  ## [1] 1  plot(y[,1], ylim=c(0,1), xlab=\"Measurement\", ylab=\"Intensity\", type=\"l\", lwd=2, col=1) for(i in 2:length(y[1,])) { screen(1, new=FALSE) plot(y[,i], ylim=c(0,1), type=\"l\", lwd=2, col=i, xaxt=\"n\", yaxt=\"n\", ylab=\"\", xlab=\"\", main=\"\", bty=\"n\") }  close.screen(all=TRUE)  Bar Plots Basics barplot(y[1:4,], ylim=c(0, max(y[1:4,])+0.3), beside=TRUE, legend=letters[1:4]) text(labels=round(as.vector(as.matrix(y[1:4,])),2), x=seq(1.5, 13, by=1) + sort(rep(c(0,1,2), 4)), y=as.vector(as.matrix(y[1:4,]))+0.04)  Error Bars bar \u003c- barplot(m \u003c- rowMeans(y) * 10, ylim=c(0, 10)) stdev \u003c- sd(t(y)) arrows(bar, m, bar, m + stdev, length=0.15, angle = 90)  Histograms hist(y, freq=TRUE, breaks=10)  Density Plots plot(density(y), col=\"red\")  Pie Charts pie(y[,1], col=rainbow(length(y[,1]), start=0.1, end=0.8), clockwise=TRUE) legend(\"topright\", legend=row.names(y), cex=1.3, bty=\"n\", pch=15, pt.cex=1.8, col=rainbow(length(y[,1]), start=0.1, end=0.8), ncol=1)  Color Selection Utilities Default color palette and how to change it\npalette()  ## [1] \"black\" \"#DF536B\" \"#61D04F\" \"#2297E6\" \"#28E2E5\" \"#CD0BBC\" \"#F5C710\" \"gray62\"  palette(rainbow(5, start=0.1, end=0.2)) palette()  ## [1] \"#FF9900\" \"#FFBF00\" \"#FFE600\" \"#F2FF00\" \"#CCFF00\"  palette(\"default\")  The gray function allows to select any type of gray shades by providing values from 0 to 1\ngray(seq(0.1, 1, by= 0.2))  ## [1] \"#1A1A1A\" \"#4D4D4D\" \"#808080\" \"#B3B3B3\" \"#E6E6E6\"  Color gradients with colorpanel function from gplots library`\nlibrary(gplots) colorpanel(5, \"darkblue\", \"yellow\", \"white\")  ## [1] \"#00008B\" \"#808046\" \"#FFFF00\" \"#FFFF80\" \"#FFFFFF\"  Much more on colors in R see Earl Glynn’s color chart here\nSaving Graphics to File After the pdf() command all graphs are redirected to file test.pdf. Works for all common formats similarly: jpeg, png, ps, tiff, …\npdf(\"test.pdf\") plot(1:10, 1:10) dev.off()  Generates Scalable Vector Graphics (SVG) files that can be edited in vector graphics programs, such as InkScape.\nlibrary(\"RSvgDevice\") devSVG(\"test.svg\") plot(1:10, 1:10) dev.off()  Homework 3C Homework 3C: Bar Plots\nAnalysis Routine Overview The following exercise introduces a variety of useful data analysis utilities in R.\nAnalysis Routine: Data Import   Step 1: To get started with this exercise, direct your R session to a dedicated workshop directory and download into this directory the following sample tables. Then import the files into Excel and save them as tab delimited text files.\n MolecularWeight_tair7.xls TargetP_analysis_tair7.xls    Import the tables into R\nImport molecular weight table\nmy_mw \u003c- read.delim(file=\"MolecularWeight_tair7.xls\", header=TRUE, sep=\"\\t\") my_mw[1:2,]  Import subcelluar targeting table\nmy_target \u003c- read.delim(file=\"TargetP_analysis_tair7.xls\", header=TRUE, sep=\"\\t\") my_target[1:2,]  Online import of molecular weight table\nmy_mw \u003c- read.delim(file=\"http://faculty.ucr.edu/~tgirke/Documents/R_BioCond/Samples/MolecularWeight_tair7.xls\", header=TRUE, sep=\"\\t\") my_mw[1:2,]  ## Sequence.id Molecular.Weight.Da. Residues ## 1 AT1G08520.1 83285 760 ## 2 AT1G08530.1 27015 257  Online import of subcelluar targeting table\nmy_target \u003c- read.delim(file=\"http://faculty.ucr.edu/~tgirke/Documents/R_BioCond/Samples/TargetP_analysis_tair7.xls\", header=TRUE, sep=\"\\t\") my_target[1:2,]  ## GeneName Loc cTP mTP SP other ## 1 AT1G08520.1 C 0.822 0.137 0.029 0.039 ## 2 AT1G08530.1 C 0.817 0.058 0.010 0.100  Merging Data Frames  Step 2: Assign uniform gene ID column titles  colnames(my_target)[1] \u003c- \"ID\" colnames(my_mw)[1] \u003c- \"ID\"   Step 3: Merge the two tables based on common ID field  my_mw_target \u003c- merge(my_mw, my_target, by.x=\"ID\", by.y=\"ID\", all.x=TRUE)   Step 4: Shorten one table before the merge and then remove the non-matching rows (NAs) in the merged file  my_mw_target2a \u003c- merge(my_mw, my_target[1:40,], by.x=\"ID\", by.y=\"ID\", all.x=TRUE) # To remove non-matching rows, use the argument setting 'all=FALSE'. my_mw_target2 \u003c- na.omit(my_mw_target2a) # Removes rows containing \"NAs\" (non-matching rows).   Homework 3D: How can the merge function in the previous step be executed so that only the common rows among the two data frames are returned? Prove that both methods - the two step version with na.omit and your method - return identical results. Homework 3E: Replace all NAs in the data frame my_mw_target2a with zeros.  Filtering Data  Step 5: Retrieve all records with a value of greater than 100,000 in ‘MW’ column and ‘C’ value in ‘Loc’ column (targeted to chloroplast).  query \u003c- my_mw_target[my_mw_target[, 2] \u003e 100000 \u0026 my_mw_target[, 4] == \"C\", ] query[1:4, ]  ## ID Molecular.Weight.Da. Residues Loc cTP mTP SP other ## 219 AT1G02730.1 132588 1181 C 0.972 0.038 0.008 0.045 ## 243 AT1G02890.1 136825 1252 C 0.748 0.529 0.011 0.013 ## 281 AT1G03160.1 100732 912 C 0.871 0.235 0.011 0.007 ## 547 AT1G05380.1 126360 1138 C 0.740 0.099 0.016 0.358  dim(query)  ## [1] 170 8   Homework 3F: How many protein entries in the my_mw_target data frame have a MW of greater then 4,000 and less then 5,000. Subset the data frame accordingly and sort it by MW to check that your result is correct.  String Substitutions  Step 6: Use a regular expression in a substitute function to generate a separate ID column that lacks the gene model extensions.  my_mw_target3 \u003c- data.frame(loci=gsub(\"\\\\..*\", \"\", as.character(my_mw_target[,1]), perl = TRUE), my_mw_target) my_mw_target3[1:3,1:8]  ## loci ID Molecular.Weight.Da. Residues Loc cTP mTP SP ## 1 AT1G01010 AT1G01010.1 49426 429 _ 0.10 0.090 0.075 ## 2 AT1G01020 AT1G01020.1 28092 245 * 0.01 0.636 0.158 ## 3 AT1G01020 AT1G01020.2 21711 191 * 0.01 0.636 0.158   Homework 3G: Retrieve those rows in my_mw_target3 where the second column contains the following identifiers: c(\"AT5G52930.1\", \"AT4G18950.1\", \"AT1G15385.1\", \"AT4G36500.1\", \"AT1G67530.1\"). Use the %in% function for this query. As an alternative approach, assign the second column to the row index of the data frame and then perform the same query again using the row index. Explain the difference of the two methods.  Calculations on Data Frames  Step 7: Count the number of duplicates in the loci column with the table function and append the result to the data frame with the cbind function.  mycounts \u003c- table(my_mw_target3[,1])[my_mw_target3[,1]] my_mw_target4 \u003c- cbind(my_mw_target3, Freq=mycounts[as.character(my_mw_target3[,1])])   Step 8: Perform a vectorized devision of columns 3 and 4 (average AA weight per protein)  data.frame(my_mw_target4, avg_AA_WT=(my_mw_target4[,3] / my_mw_target4[,4]))[1:2,]  ## loci ID Molecular.Weight.Da. Residues Loc cTP mTP SP other Freq.Var1 ## 1 AT1G01010 AT1G01010.1 49426 429 _ 0.10 0.090 0.075 0.925 AT1G01010 ## 2 AT1G01020 AT1G01020.1 28092 245 * 0.01 0.636 0.158 0.448 AT1G01020 ## Freq.Freq avg_AA_WT ## 1 1 115.2121 ## 2 2 114.6612   Step 9: Calculate for each row the mean and standard deviation across several columns  mymean \u003c- apply(my_mw_target4[,6:9], 1, mean) mystdev \u003c- apply(my_mw_target4[,6:9], 1, sd, na.rm=TRUE) data.frame(my_mw_target4, mean=mymean, stdev=mystdev)[1:2,5:12]  ## Loc cTP mTP SP other Freq.Var1 Freq.Freq mean ## 1 _ 0.10 0.090 0.075 0.925 AT1G01010 1 0.2975 ## 2 * 0.01 0.636 0.158 0.448 AT1G01020 2 0.3130  Plotting Example  Step 10: Generate scatter plot columns: ‘MW’ and ‘Residues’  plot(my_mw_target4[1:500,3:4], col=\"red\")  Export Results and Run Entire Exercise as Script  Step 11: Write the data frame my_mw_target4 into a tab-delimited text file and inspect it in Excel.  write.table(my_mw_target4, file=\"my_file.xls\", quote=FALSE, sep=\"\\t\", col.names = NA)   Homework 3H: Write all commands from this exercise into an R script named exerciseRbasics.R, or download it from here. Then execute the script with the source function like this: source(\"exerciseRbasics.R\"). This will run all commands of this exercise and generate the corresponding output files in the current working directory. For homework 3H it is not necessary to submit the result files generated by the exerciseRbasics.R script. Stating how the script was executed (e.g. source or Rscript command) will be sufficient.  source(\"exerciseRbasics.R\")  Or run it from the command-line (not from R!) with Rscript like this:\nRscript exerciseRbasics.R  Session Info sessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gplots_3.1.1 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.7 purrr_0.3.4 ## [6] readr_2.1.1 tidyr_1.1.4 tibble_3.1.6 tidyverse_1.3.1 ggplot2_3.3.5 ## [11] limma_3.50.0 BiocStyle_2.22.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.8.3 lubridate_1.8.0 gtools_3.9.2 assertthat_0.2.1 ## [5] digest_0.6.29 utf8_1.2.2 R6_2.5.1 cellranger_1.1.0 ## [9] backports_1.4.0 reprex_2.0.1 evaluate_0.15 highr_0.9 ## [13] httr_1.4.2 blogdown_1.8.2 pillar_1.6.4 rlang_1.0.2 ## [17] readxl_1.3.1 rstudioapi_0.13 jquerylib_0.1.4 rmarkdown_2.13 ## [21] munsell_0.5.0 broom_0.7.10 compiler_4.1.3 modelr_0.1.8 ## [25] xfun_0.30 pkgconfig_2.0.3 htmltools_0.5.2 tidyselect_1.1.1 ## [29] bookdown_0.25 codetools_0.2-18 fansi_0.5.0 crayon_1.4.2 ## [33] tzdb_0.2.0 dbplyr_2.1.1 withr_2.4.3 bitops_1.0-7 ## [37] grid_4.1.3 jsonlite_1.8.0 gtable_0.3.0 lifecycle_1.0.1 ## [41] DBI_1.1.1 magrittr_2.0.2 scales_1.1.1 KernSmooth_2.23-20 ## [45] cli_3.1.0 stringi_1.7.6 fs_1.5.2 xml2_1.3.3 ## [49] bslib_0.3.1 ellipsis_0.3.2 generics_0.1.1 vctrs_0.3.8 ## [53] tools_4.1.3 glue_1.6.2 hms_1.1.1 fastmap_1.1.0 ## [57] yaml_2.3.5 colorspace_2.0-2 BiocManager_1.30.16 caTools_1.18.2 ## [61] rvest_1.0.2 knitr_1.37 haven_2.4.3 sass_0.4.0  References ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/rbasics/rbasics/","tags":"","title":"Introduction to R"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: [ .Rmd ] [ .R ]\n Overview One of the main attractions of using the R (http://cran.at.r-project.org) environment is the ease with which users can write their own programs and custom functions. The R programming syntax is extremely easy to learn, even for users with no previous programming experience. Once the basic R programming control structures are understood, users can use the R language as a powerful environment to perform complex custom analyses of almost any type of data (Gentleman 2008).\nWhy Programming in R?  Powerful statistical environment and programming language Facilitates reproducible research Efficient data structures make programming very easy Ease of implementing custom functions Powerful graphics Access to fast growing number of analysis packages One of the most widely used languages in bioinformatics Is standard for data mining and biostatistical analysis Technical advantages: free, open-source, available for all OSs  R Basics The previous Rbasics tutorial provides a general introduction to the usage of the R environment and its basic command syntax. More details can be found in the R \u0026 BioConductor manual here.\nCode Editors for R Several excellent code editors are available that provide functionalities like R syntax highlighting, auto code indenting and utilities to send code/functions to the R console.\n RStudio: GUI-based IDE for R Vim-R-Tmux: R working environment based on vim and tmux Emacs (ESS add-on package) gedit and Rgedit RKWard Eclipse Tinn-R Notepad++ (NppToR)   Programming in R using RStudio     Programming in R using Vim or Emacs    Finding Help Reference list on R programming (selection)\n Advanced R, by Hadley Wickham R Programming for Bioinformatics, by Robert Gentleman S Programming, by W. N. Venables and B. D. Ripley Programming with Data, by John M. Chambers R Help \u0026 R Coding Conventions, Henrik Bengtsson, Lund University Programming in R (Vincent Zoonekynd) Peter’s R Programming Pages, University of Warwick Rtips, Paul Johnsson, University of Kansas R for Programmers, Norm Matloff, UC Davis High-Performance R, Dirk Eddelbuettel tutorial presented at useR-2008 C/C++ level programming for R, Gopi Goswami  Control Structures Important Operators Comparison operators  == (equal) != (not equal) \u003e (greater than) \u003e= (greater than or equal) \u003c (less than) \u003c= (less than or equal)  Logical operators  \u0026 (and) \u0026\u0026 (and) | (or) || (or) ! (not)  Note: \u0026 and \u0026\u0026 indicate logical AND, while | and || indicate logical OR. The shorter form performs element-wise comparisons of same-length vectors. The longer form evaluates left to right examining only the first element of each vector (can be of different lengths). Evaluation proceeds only until the result is determined. The longer form is preferred for programming control-flow, e.g. via if clauses.\nConditional Executions: if Statements An if statement operates on length-one logical vectors.\nSyntax\nif (TRUE) { statements_1 } else { statements_2 }  In the else component, avoid inserting newlines between } else. For details on how to best and consistently format R code, this style guide is a good start. In addition, the formatR package can be helpful.\nExample\nif (1==0) { print(1) } else { print(2) }  ## [1] 2  Example 2\nif (1==0) { print(1) } else if (1==2) { print(2) } else { print(3) }  ## [1] 3  Conditional Executions: ifelse Statements The ifelse statement operates on vectors.\nSyntax\nifelse(test, true_value, false_value)  Example\nx \u003c- 1:10 ifelse(x\u003c5, sqrt(x), 0)  ## [1] 1.000000 1.414214 1.732051 2.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000  Loops for loop for loops iterate over elements of a looping vector.\nSyntax\nfor(variable in sequence) { statements }  Example\nmydf \u003c- iris myve \u003c- NULL for(i in seq(along=mydf[,1])) { myve \u003c- c(myve, mean(as.numeric(mydf[i,1:3]))) } myve[1:8]  ## [1] 3.333333 3.100000 3.066667 3.066667 3.333333 3.666667 3.133333 3.300000  Note: Inject into objecs is much faster than append approach with c, cbind, etc.\nExample\nmyve \u003c- numeric(length(mydf[,1])) for(i in seq(along=myve)) { myve[i] \u003c- mean(as.numeric(mydf[i,1:3])) } myve[1:8]  ## [1] 3.333333 3.100000 3.066667 3.066667 3.333333 3.666667 3.133333 3.300000  Conditional Stop of Loops The stop function can be used to break out of a loop (or a function) when a condition becomes TRUE. In addition, an error message will be printed.\nExample\nx \u003c- 1:10 z \u003c- NULL for(i in seq(along=x)) { if (x[i] \u003c 5) { z \u003c- c(z, x[i]-1) print(z) } else { stop(\"values need to be \u003c 5\") } }  while loop Iterates as long as a condition is true.\nSyntax\nwhile(condition) { statements }  Example\nz \u003c- 0 while(z\u003c5) { z \u003c- z + 2 print(z) }  ## [1] 2 ## [1] 4 ## [1] 6  The apply Function Family apply Syntax\napply(X, MARGIN, FUN, ARGs)  Arguments\n X: array, matrix or data.frame MARGIN: 1 for rows, 2 for columns FUN: one or more functions ARGs: possible arguments for functions  Example\napply(iris[1:8,1:3], 1, mean)  ## 1 2 3 4 5 6 7 8 ## 3.333333 3.100000 3.066667 3.066667 3.333333 3.666667 3.133333 3.300000  tapply Applies a function to vector components that are defined by a factor.\nSyntax\ntapply(vector, factor, FUN)  Example\niris[1:2,]  ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa  tapply(iris$Sepal.Length, iris$Species, mean)  ## setosa versicolor virginica ## 5.006 5.936 6.588  sapply, lapply and vapply The iterator functions sapply, lapply and vapply apply a function to vectors or lists. The lapply function always returns a list, while sapply returns vector or matrix objects when possible. If not then a list is returned. The vapply function returns a vector or array of type matching the FUN.VALUE. Compared to sapply, vapply is a safer choice with respect to controlling specific output types to avoid exception handling problems.\nExamples\nl \u003c- list(a = 1:10, beta = exp(-3:3), logic = c(TRUE,FALSE,FALSE,TRUE)) lapply(l, mean)  ## $a ## [1] 5.5 ## ## $beta ## [1] 4.535125 ## ## $logic ## [1] 0.5  sapply(l, mean)  ## a beta logic ## 5.500000 4.535125 0.500000  vapply(l, mean, FUN.VALUE=numeric(1))  ## a beta logic ## 5.500000 4.535125 0.500000  Often used in combination with a function definition:\nlapply(names(l), function(x) mean(l[[x]])) sapply(names(l), function(x) mean(l[[x]])) vapply(names(l), function(x) mean(l[[x]]), FUN.VALUE=numeric(1))  Improving Speed Performance of Loops Looping over very large data sets can become slow in R. However, this limitation can be overcome by eliminating certain operations in loops or avoiding loops over the data intensive dimension in an object altogether. The latter can be achieved by performing mainly vector-to-vecor or matrix-to-matrix computations. These vectorized operations run in R often over 100 times faster than the corresponding for() or apply() loops. In addition, one can make use of the existing speed-optimized C-level functions in R, such as rowSums, rowMeans, table, and tabulate. Moreover, one can design custom functions that avoid expensive R loops by using vector- or matrix-based approaches. Alternatively, one can write programs that will perform all time consuming computations on the C-level.\nThe following code samples illustrate the time-performance differences among the different approaches of running iterative operations in R.\n1. for loop with append versus inject approach The following runs a for loop where the result is appended in each iteration with the c() function. The corresponding cbind and rbind for two dimensional data objects would have a similar performance impact as c().\nmyMA \u003c- matrix(rnorm(1000000), 100000, 10, dimnames=list(1:100000, paste(\"C\", 1:10, sep=\"\"))) results \u003c- NULL system.time(for(i in seq(along=myMA[,1])) results \u003c- c(results, mean(myMA[i,]))) user system elapsed 39.156 6.369 45.559  Now the for loop is run with an inject approach for storing the results in each iteration.\nresults \u003c- numeric(length(myMA[,1])) system.time(for(i in seq(along=myMA[,1])) results[i] \u003c- mean(myMA[i,])) user system elapsed 1.550 0.005 1.556  As one can see from the output of system.time, the inject approach is 20-50 times faster.\n2. apply loop versus rowMeans The following performs a row-wise mean calculation on a large matrix first with an apply loop and then with the rowMeans function.\nsystem.time(myMAmean \u003c- apply(myMA, 1, mean)) user system elapsed 1.452 0.005 1.456 system.time(myMAmean \u003c- rowMeans(myMA)) user system elapsed 0.005 0.001 0.006  Based on the results from system.time, the rowMeans approach is over 200 times faster than the apply loop.\n3. apply loop versus vectorized approach In this example row-wise standard deviations are computed with an apply loop and then in a vectorized manner.\nsystem.time(myMAsd \u003c- apply(myMA, 1, sd)) user system elapsed 3.707 0.014 3.721 myMAsd[1:4] 1 2 3 4 0.8505795 1.3419460 1.3768646 1.3005428 system.time(myMAsd \u003c- sqrt((rowSums((myMA-rowMeans(myMA))^2)) / (length(myMA[1,])-1))) user system elapsed 0.020 0.009 0.028 myMAsd[1:4] 1 2 3 4 0.8505795 1.3419460 1.3768646 1.3005428  The vector-based approach in the last step is over 200 times faster than the apply loop.\n4. Example of fast querying routine applied to a large matrix (a) Create a sample matrix The following lfcPvalMA function creates a test matrix containing randomly generated log2 fold changes (LFCs) and p-values (here: pval or FDRs) for variable numbers of samples or test results. In biology this dataset mimics the results of an analysis of differentially expressed genes (DEGs) from several contrasts arranged in a single matrix (or data.frame).\nlfcPvalMA \u003c- function(Nrow=200, Ncol=4, stats_labels=c(\"lfc\", \"pval\")) { set.seed(1410) assign(stats_labels[1], runif(n = Nrow * Ncol, min = -4, max = 4)) assign(stats_labels[2], runif(n = Nrow * Ncol, min = 0, max = 1)) lfc_ma \u003c- matrix(lfc, Nrow, Ncol, dimnames=list(paste(\"g\", 1:Nrow, sep=\"\"), paste(\"t\", 1:Ncol, \"_\", stats_labels[1], sep=\"\"))) pval_ma \u003c- matrix(pval, Nrow, Ncol, dimnames=list(paste(\"g\", 1:Nrow, sep=\"\"), paste(\"t\", 1:Ncol, \"_\", stats_labels[2], sep=\"\"))) statsMA \u003c- cbind(lfc_ma, pval_ma) return(statsMA[, order(colnames(statsMA))]) } degMA \u003c- lfcPvalMA(Nrow=200, Ncol=4, stats_labels=c(\"lfc\", \"pval\")) dim(degMA)  ## [1] 200 8  degMA[1:4,] # Prints first 4 rows of DEG matrix generated as a test data set  ## t1_lfc t1_pval t2_lfc t2_pval t3_lfc t3_pval t4_lfc t4_pval ## g1 -1.8476368 0.39486484 1.879310 0.7785999 0.1769551 0.9904342 0.1747932 0.9536679 ## g2 0.2542926 0.04188993 -1.629778 0.6379570 -1.9280792 0.6106041 -2.3599518 0.1950022 ## g3 3.4703657 0.73881357 2.047794 0.3129176 -3.8891714 0.1508787 3.7811606 0.2560303 ## g4 -2.8548158 0.99201512 -2.710385 0.1772805 1.0920515 0.2826038 3.9313225 0.5519854  (b) Organize results in list To filter the results efficiently, it is usually best to store the two different stats (here lfc and pval) in separate matrices (here two) where each has the same dimensions and row/column ordering. Note, in this case a list is used to store the two matrices.\ndegList \u003c- list(lfc=degMA[ , grepl(\"lfc\", colnames(degMA))], pval=degMA[ , grepl(\"pval\", colnames(degMA))]) names(degList)  ## [1] \"lfc\" \"pval\"  sapply(degList, dim)  ## lfc pval ## [1,] 200 200 ## [2,] 4 4  (c) Combinatorial filter With the above generated data structure of two complementary matrices it is easy to apply combinatorial filtering routines that are both flexible and time-efficient (fast). The following example queries for fold changes of at least 2 (here lfc \u003e= 1 | lfc \u003c= -1) plus p-values of 0.5 or lower. Note, all intermediate and final results are stored in logical matrices. In addition to boolean comparisons, one can apply basic mathematical operations, such as calculating the sum of each cell across many matrices. This returns a numeric matix of integers representing the counts of TRUE values in each position of the considered logical matrices. Subsequently, one can perform summary and filtering routines on these count-based matrices which is convenient when working with large numbers of matrices. All these matrix-to-matrix comparisons are very fast to compute and require zero looping instructions by the user.\nqueryResult \u003c- (degList$lfc \u003c= 1 | degList$lfc \u003c= -1) \u0026 degList$pval \u003c= 0.5 colnames(queryResult) \u003c- gsub(\"_.*\", \"\", colnames(queryResult)) # Adjust column names queryResult[1:4,]  ## t1 t2 t3 t4 ## g1 TRUE FALSE FALSE FALSE ## g2 TRUE FALSE FALSE TRUE ## g3 FALSE FALSE TRUE FALSE ## g4 FALSE TRUE FALSE FALSE  (d) Extract query results  Retrieve row labels (genes) that match the query from the previous step in each column, and store them in a list.  matchingIDlist \u003c- sapply(colnames(queryResult), function(x) names(queryResult[queryResult[ , x] , x]), simplify=FALSE) matchingIDlist  ## $t1 ## [1] \"g1\" \"g2\" \"g5\" \"g6\" \"g11\" \"g16\" \"g18\" \"g19\" \"g21\" \"g23\" \"g24\" \"g31\" \"g36\" ## [14] \"g37\" \"g41\" \"g46\" \"g60\" \"g61\" \"g63\" \"g70\" \"g71\" \"g72\" \"g75\" \"g81\" \"g83\" \"g84\" ## [27] \"g88\" \"g91\" \"g97\" \"g98\" \"g100\" \"g102\" \"g103\" \"g104\" \"g110\" \"g111\" \"g112\" \"g113\" \"g114\" ## [40] \"g120\" \"g121\" \"g123\" \"g124\" \"g126\" \"g130\" \"g134\" \"g135\" \"g139\" \"g140\" \"g145\" \"g147\" \"g153\" ## [53] \"g157\" \"g158\" \"g159\" \"g160\" \"g162\" \"g170\" \"g171\" \"g172\" \"g173\" \"g175\" \"g178\" \"g183\" \"g184\" ## [66] \"g187\" \"g190\" \"g192\" \"g196\" \"g199\" ## ## $t2 ## [1] \"g4\" \"g5\" \"g7\" \"g9\" \"g10\" \"g12\" \"g16\" \"g23\" \"g34\" \"g35\" \"g39\" \"g41\" \"g44\" ## [14] \"g46\" \"g47\" \"g48\" \"g49\" \"g50\" \"g51\" \"g52\" \"g56\" \"g66\" \"g75\" \"g80\" \"g81\" \"g85\" ## [27] \"g88\" \"g89\" \"g90\" \"g94\" \"g99\" \"g102\" \"g112\" \"g115\" \"g116\" \"g118\" \"g119\" \"g120\" \"g129\" ## [40] \"g144\" \"g145\" \"g148\" \"g152\" \"g155\" \"g156\" \"g160\" \"g164\" \"g165\" \"g167\" \"g168\" \"g170\" \"g172\" ## [53] \"g178\" \"g186\" \"g187\" \"g194\" \"g197\" ## ## $t3 ## [1] \"g3\" \"g6\" \"g7\" \"g9\" \"g12\" \"g15\" \"g23\" \"g25\" \"g26\" \"g27\" \"g38\" \"g43\" \"g52\" ## [14] \"g53\" \"g54\" \"g58\" \"g66\" \"g69\" \"g72\" \"g76\" \"g77\" \"g80\" \"g84\" \"g85\" \"g86\" \"g88\" ## [27] \"g89\" \"g90\" \"g91\" \"g99\" \"g100\" \"g107\" \"g110\" \"g122\" \"g124\" \"g125\" \"g129\" \"g134\" \"g139\" ## [40] \"g141\" \"g143\" \"g144\" \"g146\" \"g148\" \"g154\" \"g163\" \"g165\" \"g171\" \"g173\" \"g178\" \"g180\" \"g182\" ## [53] \"g188\" \"g190\" \"g193\" \"g195\" ## ## $t4 ## [1] \"g2\" \"g5\" \"g7\" \"g8\" \"g9\" \"g12\" \"g13\" \"g15\" \"g20\" \"g21\" \"g26\" \"g28\" \"g30\" ## [14] \"g31\" \"g36\" \"g37\" \"g38\" \"g47\" \"g63\" \"g64\" \"g65\" \"g67\" \"g68\" \"g69\" \"g76\" \"g77\" ## [27] \"g80\" \"g85\" \"g90\" \"g98\" \"g99\" \"g101\" \"g105\" \"g106\" \"g120\" \"g123\" \"g125\" \"g126\" \"g129\" ## [40] \"g131\" \"g134\" \"g137\" \"g140\" \"g143\" \"g147\" \"g148\" \"g153\" \"g155\" \"g157\" \"g165\" \"g167\" \"g170\" ## [53] \"g171\" \"g172\" \"g174\" \"g175\" \"g176\" \"g178\" \"g181\" \"g182\" \"g188\" \"g192\" \"g199\"  Return all row labels (genes) that match the above query across a specified number of columns (here 2). Note, the rowSums function is used for this, which performs the row-wise looping internally and extremely fast.  matchingID \u003c- rowSums(queryResult) \u003e 2 queryResult[matchingID, , drop=FALSE]  ## t1 t2 t3 t4 ## g5 TRUE TRUE FALSE TRUE ## g7 FALSE TRUE TRUE TRUE ## g9 FALSE TRUE TRUE TRUE ## g12 FALSE TRUE TRUE TRUE ## g23 TRUE TRUE TRUE FALSE ## g80 FALSE TRUE TRUE TRUE ## g85 FALSE TRUE TRUE TRUE ## g88 TRUE TRUE TRUE FALSE ## g90 FALSE TRUE TRUE TRUE ## g99 FALSE TRUE TRUE TRUE ## g120 TRUE TRUE FALSE TRUE ## g129 FALSE TRUE TRUE TRUE ## g134 TRUE FALSE TRUE TRUE ## g148 FALSE TRUE TRUE TRUE ## g165 FALSE TRUE TRUE TRUE ## g170 TRUE TRUE FALSE TRUE ## g171 TRUE FALSE TRUE TRUE ## g172 TRUE TRUE FALSE TRUE ## g178 TRUE TRUE TRUE TRUE  names(matchingID[matchingID])  ## [1] \"g5\" \"g7\" \"g9\" \"g12\" \"g23\" \"g80\" \"g85\" \"g88\" \"g90\" \"g99\" \"g120\" \"g129\" \"g134\" ## [14] \"g148\" \"g165\" \"g170\" \"g171\" \"g172\" \"g178\"  As demonstrated in the above query examples, by setting up the proper data structures (here two matrices with same dimensions), and utilizing vectorized (matrix-to-matrix) operations along with R’s built-in row* and col* stats function family (e.g. rowSums) one can design with very little code flexible query routines that also run very time-efficient.\nFunctions Function Overview A very useful feature of the R environment is the possibility to expand existing functions and to easily write custom functions. In fact, most of the R software can be viewed as a series of R functions.\nSyntax to define function\nmyfct \u003c- function(arg1, arg2, ...) { function_body }  Syntax to call functions\nmyfct(arg1=..., arg2=...)  The value returned by a function is the value of the function body, which is usually an unassigned final expression, e.g.: return()\nFunction Syntax Rules General\n Functions are defined by  The assignment with the keyword function The declaration of arguments/variables (arg1, arg2, ...) The definition of operations (function_body) that perform computations on the provided arguments. A function name needs to be assigned to call the function.    Naming\n Function names can be almost anything. However, the usage of names of existing functions should be avoided.  Arguments\n It is often useful to provide default values for arguments (e.g.: arg1=1:10). This way they don’t need to be provided in a function call. The argument list can also be left empty (myfct \u003c- function() { fct_body }) if a function is expected to return always the same value(s). The argument ... can be used to allow one function to pass on argument settings to another.  Body\n The actual expressions (commands/operations) are defined in the function body which should be enclosed by braces. The individual commands are separated by semicolons or new lines (preferred).  Usage\n Functions are called by their name followed by parentheses containing possible argument names. Empty parenthesis after the function name will result in an error message when a function requires certain arguments to be provided by the user. The function name alone will print the definition of a function.  Scope\n Variables created inside a function exist only for the life time of a function. Thus, they are not accessible outside of the function. To force variables in functions to exist globally, one can use the double assignment operator: \u003c\u003c-  Examples Define sample function\nmyfct \u003c- function(x1, x2=5) { z1 \u003c- x1 / x1 z2 \u003c- x2 * x2 myvec \u003c- c(z1, z2) return(myvec) }  Function usage\nApply function to values 2 and 5\nmyfct(x1=2, x2=5)  ## [1] 1 25  Run without argument names\nmyfct(2, 5)  ## [1] 1 25  Makes use of default value 5\nmyfct(x1=2)  ## [1] 1 25  Print function definition (often unintended)\nmyfct  ## function(x1, x2=5) { ## z1 \u003c- x1 / x1 ## z2 \u003c- x2 * x2 ## myvec \u003c- c(z1, z2) ## return(myvec) ## } ## \u003cbytecode: 0x56dd086c6988\u003e  Useful Utilities Debugging Utilities Several debugging utilities are available for R. They include:\n traceback browser options(error=recover) options(error=NULL) debug  The Debugging in R page provides an overview of the available resources.\nRegular Expressions R’s regular expression utilities work similar as in other languages. To learn how to use them in R, one can consult the main help page on this topic with ?regexp.\nString matching with grep The grep function can be used for finding patterns in strings, here letter A in vector month.name.\nmonth.name[grep(\"A\", month.name)]  ## [1] \"April\" \"August\"  String substitution with gsub Example for using regular expressions to substitute a pattern by another one using a back reference. Remember: single escapes \\ need to be double escaped \\\\ in R.\ngsub('(i.*a)', 'xxx_\\\\1', \"virginica\", perl = TRUE)  ## [1] \"vxxx_irginica\"  Interpreting a Character String as Expression Some useful examples\nGenerates vector of object names in session\nmyfct \u003c- function(x) x^2 mylist \u003c- ls() n \u003c- which(mylist %in% \"myfct\") mylist[n]  ## [1] \"myfct\"  Executes entry in position n as expression\nget(mylist[n])  ## function(x) x^2  get(mylist[n])(2)  ## [1] 4  Alternative approach\neval(parse(text=mylist[n]))  ## function(x) x^2  Replacement, Split and Paste Functions for Strings Selected examples\nSubstitution with back reference which inserts in this example _ character\nx \u003c- gsub(\"(a)\",\"\\\\1_\", month.name[1], perl=T) x  ## [1] \"Ja_nua_ry\"  Split string on inserted character from above\nstrsplit(x,\"_\")  ## [[1]] ## [1] \"Ja\" \"nua\" \"ry\"  Reverse a character string by splitting first all characters into vector fields\npaste(rev(unlist(strsplit(x, NULL))), collapse=\"\")  ## [1] \"yr_aun_aJ\"  Time, Date and Sleep Selected examples\nReturn CPU (and other) times that an expression used (here ls)\nsystem.time(ls())  ## user system elapsed ## 0 0 0  Return the current system date and time\ndate()  ## [1] \"Thu Apr 14 09:21:47 2022\"  Pause execution of R expressions for a given number of seconds (e.g. in loop)\nSys.sleep(1)  Example Import of Specific File Lines with Regular Expression The following example demonstrates the retrieval of specific lines from an external file with a regular expression. First, an external file is created with the cat function, all lines of this file are imported into a vector with readLines, the specific elements (lines) are then retieved with the grep function, and the resulting lines are split into vector fields with strsplit.\ncat(month.name, file=\"zzz.txt\", sep=\"\\n\") x \u003c- readLines(\"zzz.txt\") x[1:6]  ## [1] \"January\" \"February\" \"March\" \"April\" \"May\" \"June\"  x \u003c- x[c(grep(\"^J\", as.character(x), perl = TRUE))] t(as.data.frame(strsplit(x, \"u\")))  ## [,1] [,2] ## c..Jan....ary.. \"Jan\" \"ary\" ## c..J....ne.. \"J\" \"ne\" ## c..J....ly.. \"J\" \"ly\"  Calling External Software External command-line software can be called with system. The following example calls blastall from R\nsystem(\"blastall -p blastp -i seq.fasta -d uniprot -o seq.blastp\")  Running R Scripts Possibilities for Executing R Scripts R console source(\"my_script.R\")  Command-line Rscript my_script.R # or just ./myscript.R after including shebang line `#!/usr/bin/env Rscript` and making it executable R CMD BATCH my_script.R # Alternative way 1 R --slave \u003c my_script.R # Alternative way 2  Passing arguments from command-line to R Create an R script named test.R with the following content:\nmyarg \u003c- commandArgs() print(iris[1:myarg[6], ])  Then run it from the command-line like this:\nRscript test.R 10  In the given example the number 10 is passed on from the command-line as an argument to the R script which is used to return to STDOUT the first 10 rows of the iris sample data. If several arguments are provided, they will be interpreted as one string and need to be split in R with the strsplit function. A more detailed example can be found here.\nBuilding R Packages This section has been moved to a dedicated tutorial on R package development here.\nProgramming Exercises Exercise 1 for loop Task 1.1: Compute the mean of each row in myMA by applying the mean function in a for loop.\nmyMA \u003c- matrix(rnorm(500), 100, 5, dimnames=list(1:100, paste(\"C\", 1:5, sep=\"\"))) myve_for \u003c- NULL for(i in seq(along=myMA[,1])) { myve_for \u003c- c(myve_for, mean(as.numeric(myMA[i, ]))) } myResult \u003c- cbind(myMA, mean_for=myve_for) myResult[1:4, ]  ## C1 C2 C3 C4 C5 mean_for ## 1 -1.202619 -0.1053735 -1.1074995 0.4269033 -1.3932608 -0.67636998 ## 2 1.174138 -0.8991985 -1.2658862 1.0151751 -0.4075398 -0.07666225 ## 3 1.357860 -1.2370991 0.7402124 -1.0277818 0.4851062 0.06365953 ## 4 1.173512 -1.2724114 0.2270477 -1.5421106 0.7484669 -0.13309912  while loop Task 1.2: Compute the mean of each row in myMA by applying the mean function in a while loop.\nz \u003c- 1 myve_while \u003c- NULL while(z \u003c= length(myMA[,1])) { myve_while \u003c- c(myve_while, mean(as.numeric(myMA[z, ]))) z \u003c- z + 1 } myResult \u003c- cbind(myMA, mean_for=myve_for, mean_while=myve_while) myResult[1:4, -c(1,2)]  ## C3 C4 C5 mean_for mean_while ## 1 -1.1074995 0.4269033 -1.3932608 -0.67636998 -0.67636998 ## 2 -1.2658862 1.0151751 -0.4075398 -0.07666225 -0.07666225 ## 3 0.7402124 -1.0277818 0.4851062 0.06365953 0.06365953 ## 4 0.2270477 -1.5421106 0.7484669 -0.13309912 -0.13309912  Task 1.3: Confirm that the results from both mean calculations are identical\nall(myResult[,6] == myResult[,7])  ## [1] TRUE  apply loop Task 1.4: Compute the mean of each row in myMA by applying the mean function in an apply loop\nmyve_apply \u003c- apply(myMA, 1, mean) myResult \u003c- cbind(myMA, mean_for=myve_for, mean_while=myve_while, mean_apply=myve_apply) myResult[1:4, -c(1,2)]  ## C3 C4 C5 mean_for mean_while mean_apply ## 1 -1.1074995 0.4269033 -1.3932608 -0.67636998 -0.67636998 -0.67636998 ## 2 -1.2658862 1.0151751 -0.4075398 -0.07666225 -0.07666225 -0.07666225 ## 3 0.7402124 -1.0277818 0.4851062 0.06365953 0.06365953 0.06365953 ## 4 0.2270477 -1.5421106 0.7484669 -0.13309912 -0.13309912 -0.13309912  Avoiding loops Task 1.5: When operating on large data sets it is much faster to use the rowMeans function\nmymean \u003c- rowMeans(myMA) myResult \u003c- cbind(myMA, mean_for=myve_for, mean_while=myve_while, mean_apply=myve_apply, mean_int=mymean) myResult[1:4, -c(1,2,3)]  ## C4 C5 mean_for mean_while mean_apply mean_int ## 1 0.4269033 -1.3932608 -0.67636998 -0.67636998 -0.67636998 -0.67636998 ## 2 1.0151751 -0.4075398 -0.07666225 -0.07666225 -0.07666225 -0.07666225 ## 3 -1.0277818 0.4851062 0.06365953 0.06365953 0.06365953 0.06365953 ## 4 -1.5421106 0.7484669 -0.13309912 -0.13309912 -0.13309912 -0.13309912  To find out which other built-in functions for basic calculations exist, type ?rowMeans.\nExercise 2 Custom functions Task 2.1: Use the following code as basis to implement a function that allows the user to compute the mean for any combination of columns in a matrix or data frame. The first argument of this function should specify the input data set, the second the mathematical function to be passed on (e.g. mean, sd, max) and the third one should allow the selection of the columns by providing a grouping vector.\nmyMA \u003c- matrix(rnorm(100000), 10000, 10, dimnames=list(1:10000, paste(\"C\", 1:10, sep=\"\"))) myMA[1:2,]  ## C1 C2 C3 C4 C5 C6 C7 C8 ## 1 0.1750576 -0.2826930 0.9290137 0.0830523803 1.4113331 -1.221849 -0.4663186 0.6170635 ## 2 -0.8056727 0.1885557 0.4891758 -0.0003536156 0.8356054 1.406484 1.0363437 -0.1351562 ## C9 C10 ## 1 0.58305534 0.1716817 ## 2 0.09596605 0.4551591  myList \u003c- tapply(colnames(myMA), c(1,1,1,2,2,2,3,3,4,4), list) names(myList) \u003c- sapply(myList, paste, collapse=\"_\") myMAmean \u003c- sapply(myList, function(x) apply(myMA[,x], 1, mean)) myMAmean[1:4,]  ## C1_C2_C3 C4_C5_C6 C7_C8 C9_C10 ## 1 0.27379276 0.0908453513 0.07537248 0.3773685 ## 2 -0.04264706 0.7472451361 0.45059378 0.2755626 ## 3 0.49411343 -0.4486259202 -0.08519687 1.0929435 ## 4 0.06575562 -0.0007494971 0.07968924 -0.7374505  Exercise 3 Nested loops to generate similarity matrices Task 3.1: Create a sample list populated with character vectors of different lengths\nsetlist \u003c- lapply(11:30, function(x) sample(letters, x, replace=TRUE)) names(setlist) \u003c- paste(\"S\", seq(along=setlist), sep=\"\") setlist[1:6]  ## $S1 ## [1] \"b\" \"s\" \"o\" \"w\" \"w\" \"x\" \"s\" \"z\" \"d\" \"f\" \"b\" ## ## $S2 ## [1] \"e\" \"w\" \"l\" \"y\" \"i\" \"l\" \"m\" \"q\" \"s\" \"r\" \"d\" \"s\" ## ## $S3 ## [1] \"h\" \"i\" \"t\" \"d\" \"f\" \"c\" \"u\" \"r\" \"v\" \"a\" \"n\" \"t\" \"v\" ## ## $S4 ## [1] \"o\" \"w\" \"c\" \"x\" \"c\" \"l\" \"a\" \"z\" \"c\" \"f\" \"n\" \"i\" \"j\" \"u\" ## ## $S5 ## [1] \"a\" \"z\" \"b\" \"t\" \"p\" \"l\" \"u\" \"z\" \"q\" \"x\" \"y\" \"h\" \"l\" \"j\" \"e\" ## ## $S6 ## [1] \"z\" \"v\" \"n\" \"v\" \"b\" \"m\" \"p\" \"p\" \"z\" \"g\" \"l\" \"j\" \"z\" \"z\" \"t\" \"e\"  Task 3.2: Compute the length for all pairwise intersects of the vectors stored in setlist. The intersects can be determined with the %in% function like this: sum(setlist[[1]] %in% setlist[[2]])\nsetlist \u003c- sapply(setlist, unique) olMA \u003c- sapply(names(setlist), function(x) sapply(names(setlist), function(y) sum(setlist[[x]] %in% setlist[[y]]))) olMA[1:12,]  ## S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 ## S1 8 3 2 5 3 2 4 4 4 5 8 5 5 5 6 5 4 6 4 4 ## S2 3 10 3 3 4 3 5 3 3 5 4 6 6 8 7 5 5 8 7 8 ## S3 2 3 11 6 4 3 5 4 4 7 4 8 4 5 8 6 6 6 7 8 ## S4 5 3 6 12 6 4 8 4 4 8 8 7 7 7 8 9 6 8 9 9 ## S5 3 4 4 6 13 7 3 6 6 8 7 8 5 10 8 9 6 10 10 10 ## S6 2 3 3 4 7 11 4 5 5 5 5 7 4 8 7 6 8 8 9 8 ## S7 4 5 5 8 3 4 11 5 6 5 6 6 7 6 9 6 7 7 8 9 ## S8 4 3 4 4 6 5 5 12 7 4 7 6 5 7 10 7 8 8 9 7 ## S9 4 3 4 4 6 5 6 7 11 6 8 6 3 8 10 6 6 8 8 8 ## S10 5 5 7 8 8 5 5 4 6 14 7 9 5 9 9 10 7 11 9 8 ## S11 8 4 4 8 7 5 6 7 8 7 13 7 6 10 10 9 8 9 8 8 ## S12 5 6 8 7 8 7 6 6 6 9 7 15 5 10 10 9 9 11 10 10  Task 3.3 Plot the resulting intersect matrix as heat map. The image or the pheatmap functions can be used for this.\nlibrary(pheatmap); library(\"RColorBrewer\") pheatmap(olMA, color=brewer.pal(9,\"Blues\"), cluster_rows=FALSE, cluster_cols=FALSE, display_numbers=TRUE, number_format=\"%.0f\", fontsize_number=10)  # image(olMA)  Exercise 4 Build your own R package Task 4.1: Save one or more of your functions to a file called script.R and build the package with the package.skeleton function.\npackage.skeleton(name=\"mypackage\", code_files=c(\"script1.R\"))  Task 4.2: Build tarball of the package\nsystem(\"R CMD build mypackage\")  Task 4.3: Install and use package\ninstall.packages(\"mypackage_1.0.tar.gz\", repos=NULL, type=\"source\") library(mypackage) ?myMAcomp # Opens help for function defined by mypackage  Homework 5 See homework section here.\nAdditional Exercises Pattern matching and positional parsing of equences The following sample script patternSearch.R defines functions for importing sequences into R, retrieving reverse and complement of nucleotide sequences, pattern searching, positional parsing and exporting search results in HTML format. Sourcing the script will return usage instructions of its functions.\nsource(\"https://raw.githubusercontent.com/tgirke/GEN242/main/content/en/tutorials/rprogramming/scripts/patternSearch.R\")  Identify over-represented strings in sequence sets Example functions for finding over-represented words in sets of DNA, RNA or protein sequences are defined in this script: wordFinder.R. Sourcing the script will return usage instructions of its functions.\nsource(\"https://raw.githubusercontent.com/tgirke/GEN242/main/content/en/tutorials/rprogramming/scripts/wordFinder.R\")  Object-Oriented Programming (OOP) R supports several systems for object-oriented programming (OOP). This includes an older S3 system, and the more recently introduced R6 and S4 systems. The latter is the most formal version that supports multiple inheritance, multiple dispatch and introspection. Many of these features are not available in the older S3 system. In general, the OOP approach taken by R is to separate the class specifications from the specifications of generic functions (function-centric system). The following introduction is restricted to the S4 system since it is nowadays the preferred OOP method for package development in Bioconductor. More information about OOP in R can be found in the following introductions:\n Vincent Zoonekynd’s introduction to S3 Classes Christophe Genolini’s S4 Intro Advanced Bioconductor Courses Programming with R by John Chambers R Programming for Bioinformatics by Robert Gentleman Advanced R online book by Hadley Wichham  Define S4 Classes 1. Define S4 Classes with setClass() and new() y \u003c- matrix(1:10, 2, 5) # Sample data set setClass(Class=\"myclass\", representation=representation(a=\"ANY\"), prototype=prototype(a=y[1:2,]), # Defines default value (optional) validity=function(object) { # Can be defined in a separate step using setValidity if(class(object@a)[1]!=\"matrix\") { return(paste(\"expected matrix, but obtained\", class(object@a))) } else { return(TRUE) } } )  The setClass function defines classes. Its most important arguments are\n Class: the name of the class representation: the slots that the new class should have and/or other classes that this class extends. prototype: an object providing default data for the slots. contains: the classes that this class extends. validity, access, version: control arguments included for compatibility with S-Plus. where: the environment to use to store or remove the definition as meta data.  2. Create new class instance The function new creates an instance of a class (here myclass).\nmyobj \u003c- new(\"myclass\", a=y) myobj  ## An object of class \"myclass\" ## Slot \"a\": ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10  If evaluated the following would return an error due to wrong input type (data.frame instead of matrix).\nnew(\"myclass\", a=iris) # Returns error due to wrong input  The arguments of new are:\n Class: the name of the class ...: data to include in the new object with arguments according to slots in class definition  3. Initialization method A more generic way of creating class instances is to define an initialization method (more details below).\nsetMethod(\"initialize\", \"myclass\", function(.Object, a) { .Object@a \u003c- a/a .Object }) new(\"myclass\", a = y)  ## An object of class \"myclass\" ## Slot \"a\": ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1 ## [2,] 1 1 1 1 1  4. Usage and helper functions The ‘@’ operator extracts the contents of a slot. Its usage should be limited to internal functions.\nmyobj@a  ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10  Create a new S4 object from an old one.\ninitialize(.Object=myobj, a=as.matrix(cars[1:2,]))  ## An object of class \"myclass\" ## Slot \"a\": ## speed dist ## 1 1 1 ## 2 1 1  If evaluated the removeClass function removes an object from the current session. This does not apply to associated methods.\nremoveClass(\"myclass\")  5. Inheritance Inheritance allows to define new classes that inherit all properties (e.g. data slots, methods) from their existing parent classes. The contains argument used below allows to extend existing classes. This propagates all slots of parent classes.\nsetClass(\"myclass1\", representation(a = \"character\", b = \"character\")) setClass(\"myclass2\", representation(c = \"numeric\", d = \"numeric\")) setClass(\"myclass3\", contains=c(\"myclass1\", \"myclass2\")) new(\"myclass3\", a=letters[1:4], b=letters[1:4], c=1:4, d=4:1)  ## An object of class \"myclass3\" ## Slot \"a\": ## [1] \"a\" \"b\" \"c\" \"d\" ## ## Slot \"b\": ## [1] \"a\" \"b\" \"c\" \"d\" ## ## Slot \"c\": ## [1] 1 2 3 4 ## ## Slot \"d\": ## [1] 4 3 2 1  getClass(\"myclass1\")  ## Class \"myclass1\" [in \".GlobalEnv\"] ## ## Slots: ## ## Name: a b ## Class: character character ## ## Known Subclasses: \"myclass3\"  getClass(\"myclass2\")  ## Class \"myclass2\" [in \".GlobalEnv\"] ## ## Slots: ## ## Name: c d ## Class: numeric numeric ## ## Known Subclasses: \"myclass3\"  getClass(\"myclass3\")  ## Class \"myclass3\" [in \".GlobalEnv\"] ## ## Slots: ## ## Name: a b c d ## Class: character character numeric numeric ## ## Extends: \"myclass1\", \"myclass2\"  6. Coerce objects to another class The following defines a coerce method. After this the standard as(..., \"...\") syntax can be used to coerce the new class to another one.\nsetAs(from=\"myclass\", to=\"character\", def=function(from) as.character(as.matrix(from@a))) as(myobj, \"character\")  ## [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \"10\"  7. Virtual classes Virtual classes are constructs for which no instances will be or can be created. They are used to link together classes which may have distinct representations (e.g. cannot inherit from each other) but for which one wants to provide similar functionality. Often it is desired to create a virtual class and to then have several other classes extend it. Virtual classes can be defined by leaving out the representation argument or including the class VIRTUAL as illustrated here:\nsetClass(\"myVclass\") setClass(\"myVclass\", representation(a = \"character\", \"VIRTUAL\"))  8. Introspection of classes Useful functions to introspect classes include:\n getClass(\"myclass\") getSlots(\"myclass\") slotNames(\"myclass\") extends(\"myclass2\")  Assign Generics and Methods Generics and methods can be assigned with the methods setGeneric() and setMethod().\n1. Accessor functions This avoids the usage of the @ operator.\nsetGeneric(name=\"acc\", def=function(x) standardGeneric(\"acc\"))  ## [1] \"acc\"  setMethod(f=\"acc\", signature=\"myclass\", definition=function(x) { return(x@a) }) acc(myobj)  ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10  2. Replacement methods  Using custom accessor function with acc \u003c- syntax.  setGeneric(name=\"acc\u003c-\", def=function(x, value) standardGeneric(\"acc\u003c-\"))  ## [1] \"acc\u003c-\"  setReplaceMethod(f=\"acc\", signature=\"myclass\", definition=function(x, value) { x@a \u003c- value return(x) }) ## After this the following replace operations with 'acc' work on new object class acc(myobj)[1,1] \u003c- 999 # Replaces first value colnames(acc(myobj)) \u003c- letters[1:5] # Assigns new column names rownames(acc(myobj)) \u003c- letters[1:2] # Assigns new row names myobj  ## An object of class \"myclass\" ## Slot \"a\": ## a b c d e ## a 999 3 5 7 9 ## b 2 4 6 8 10  Replacement method using [ operator, here [...] \u003c- syntax.  setReplaceMethod(f=\"[\", signature=\"myclass\", definition=function(x, i, j, value) { x@a[i,j] \u003c- value return(x) }) myobj[1,2] \u003c- 999 myobj  ## An object of class \"myclass\" ## Slot \"a\": ## a b c d e ## a 999 999 5 7 9 ## b 2 4 6 8 10  3. Behavior of bracket operator The behavior of the bracket [ subsetting operator can be defined as follows.\nsetMethod(f=\"[\", signature=\"myclass\", definition=function(x, i, j, ..., drop) { x@a \u003c- x@a[i,j] return(x) }) myobj[1:2, 1:3] # Standard subsetting works now on new class  ## An object of class \"myclass\" ## Slot \"a\": ## a b c ## a 999 999 5 ## b 2 4 6  4. Print behavior A convient summary printing behavior for a new class should always be defined.\nsetMethod(f=\"show\", signature=\"myclass\", definition=function(object) { cat(\"An instance of \", \"\\\"\", class(object), \"\\\"\", \" with \", length(acc(object)[,1]), \" elements\", \"\\n\", sep=\"\") if(length(acc(object)[,1])\u003e=5) { print(as.data.frame(rbind(acc(object)[1:2,], ...=rep(\"...\", length(acc(object)[1,])), acc(object)[(length(acc(object)[,1])-1):length(acc(object)[,1]),]))) } else { print(acc(object)) } }) myobj # Prints object with custom method  ## An instance of \"myclass\" with 2 elements ## a b c d e ## a 999 999 5 7 9 ## b 2 4 6 8 10  5. Define custom methods The following gives an example for defining a data specific method, here randomizing row order of matrix stored in new S4 class.\nsetGeneric(name=\"randomize\", def=function(x) standardGeneric(\"randomize\"))  ## [1] \"randomize\"  setMethod(f=\"randomize\", signature=\"myclass\", definition=function(x) { acc(x)[sample(1:length(acc(x)[,1]), length(acc(x)[,1])), ] }) randomize(myobj)  ## a b c d e ## b 2 4 6 8 10 ## a 999 999 5 7 9  6. Plotting method Define a graphical plotting method for new class and allow users to access it with R’s generic plot function.\nsetMethod(f=\"plot\", signature=\"myclass\", definition=function(x, ...) { barplot(as.matrix(acc(x)), ...) }) plot(myobj)  7. Utilities to inspect methods Important inspection methods for classes include:\n showMethods(class=\"myclass\") findMethods(\"randomize\") getMethod(\"randomize\", signature=\"myclass\") existsMethod(\"randomize\", signature=\"myclass\")  Session Info sessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] RColorBrewer_1.1-2 pheatmap_1.0.12 BiocStyle_2.22.0 ## ## loaded via a namespace (and not attached): ## [1] knitr_1.37 magrittr_2.0.2 munsell_0.5.0 colorspace_2.0-2 ## [5] R6_2.5.1 rlang_1.0.2 fastmap_1.1.0 highr_0.9 ## [9] stringr_1.4.0 tools_4.1.3 grid_4.1.3 gtable_0.3.0 ## [13] xfun_0.30 cli_3.1.0 jquerylib_0.1.4 htmltools_0.5.2 ## [17] yaml_2.3.5 digest_0.6.29 lifecycle_1.0.1 bookdown_0.24 ## [21] BiocManager_1.30.16 codetools_0.2-18 sass_0.4.0 evaluate_0.15 ## [25] rmarkdown_2.13 blogdown_1.8.2 stringi_1.7.6 compiler_4.1.3 ## [29] bslib_0.3.1 scales_1.1.1 jsonlite_1.8.0  References Gentleman, Robert. 2008. R Programming for Bioinformatics (Chapman \u0026 Hall/CRC Computer Science \u0026 Data Analysis). 1 edition. Chapman; Hall/CRC. http://www.amazon.com/Programming-Bioinformatics-Chapman-Computer-Analysis/dp/1420063677.\n  ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/rprogramming/rprogramming/","tags":"","title":"Programming in R"},{"body":"Overview  R provides a large number of packages for parallel evaluations on multi-core, multi-socket and multi-node systems. The latter are usually referred to as computer clusters. MPI is also supported For an overview of parallelization packages available for R see here One of the most comprehensive parallel computing environments for R is batchtools. Older versions of this package were released under the name BatchJobs (Bischl et al. 2015). batchtools supports both multi-core and multi-node computations with and without schedulers. By making use of cluster template files, most schedulers and queueing systems are supported (e.g. Torque, Sun Grid Engine, Slurm). The BiocParallel package (see here) provides similar functionalities as batchtools, but is tailored to use Bioconductor objects.  Reminder: Traditional Job Submission for R This topic is covered in more detail in other tutorials. The following only provides a very brief overview of this submission method.\n1. Create Slurm submission script, here called script_name.sh with:\n#!/bin/bash -l #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1G #SBATCH --time=1-00:15:00 # 1 day and 15 minutes #SBATCH --mail-user=useremail@address.com #SBATCH --mail-type=ALL #SBATCH --job-name=\"some_test\" #SBATCH -p short # Choose queue/partition from: intel, batch, highmem, gpu, short Rscript my_script.R  2. Submit R script called my_script.R by above Slurm script with:\nsbatch script_name.sh  Parallel Evaluations on Clusters with batchtools  The following introduces the usage of batchtools for a computer cluster using SLURM as scheduler (workload manager). SLURM is the scheduler used by the HPCC at UCR. Similar instructions are provided in HPCC’s manual section covering batchtools here To simplify the evaluation of the R code on the following slides, the corresponding text version is available for download from here.  Hands-on Demo of batchtools Set up working directory for SLURM First login to your cluster account, open R and execute the following lines. This will create a test directory (here mytestdir), redirect R into this directory and then download the required files:\n slurm.tmpl .batchtools.conf.R  dir.create(\"mytestdir\") setwd(\"mytestdir\") download.file(\"https://bit.ly/3Oh9dRO\", \"slurm.tmpl\") download.file(\"https://bit.ly/3KPBwou\", \".batchtools.conf.R\")  Load package and define some custom function The following code defines a test function (here myFct) that will be run on the cluster for demonstration purposes.\nThe test function (myFct) subsets the iris data frame by rows, and appends the host name and R version of each node where the function was executed. The R version to be used on each node can be specified in the slurm.tmpl file (under module load).\nlibrary('RenvModule') module('load','slurm') # Loads slurm among other modules library(batchtools) myFct \u003c- function(x) { Sys.sleep(10) # to see job in queue, pause for 10 sec result \u003c- cbind(iris[x, 1:4,], Node=system(\"hostname\", intern=TRUE), Rversion=paste(R.Version()[6:7], collapse=\".\")) return(result) }  Submit jobs from R to cluster The following creates a batchtools registry, defines the number of jobs and resource requests, and then submits the jobs to the cluster via SLURM.\nreg \u003c- makeRegistry(file.dir=\"myregdir\", conf.file=\".batchtools.conf.R\") Njobs \u003c- 1:4 # Define number of jobs (here 4) ids \u003c- batchMap(fun=myFct, x=Njobs) done \u003c- submitJobs(ids, reg=reg, resources=list(partition=\"short\", walltime=120, ntasks=1, ncpus=1, memory=1024)) waitForJobs() # Wait until jobs are completed  Summarize job status After the jobs are completed one can inspect their status as follows.\ngetStatus() # Summarize job status showLog(Njobs[1]) # killJobs(Njobs) # # Possible from within R or outside with scancel  Access/assemble results The results are stored as .rds files in the registry directory (here myregdir). One can access them manually via readRDS or use various convenience utilities provided by the batchtools package.\nreadRDS(\"myregdir/results/1.rds\") # reads from rds file first result chunk loadResult(1) lapply(Njobs, loadResult) reduceResults(rbind) # Assemble result chunks in single data.frame do.call(\"rbind\", lapply(Njobs, loadResult))  Remove registry directory from file system By default existing registries will not be overwritten. If required one can explicitly clean and delete them with the following functions.\nclearRegistry() # Clear registry in R session removeRegistry(wait=0, reg=reg) # Delete registry directory # unlink(\"myregdir\", recursive=TRUE) # Same as previous line  Load registry into R Loading a registry can be useful when accessing the results at a later state or after moving them to a local system.\nfrom_file \u003c- loadRegistry(\"myregdir\", conf.file=\".batchtools.conf.R\") reduceResults(rbind)  Conclusions Advantages of batchtools  many parallelization methods multiple cores, and across both multiple CPU sockets and nodes most schedulers supported takes full advantage of a cluster robust job management by organizing results in registry file-based database simplifies submission, monitoring and restart of jobs well supported and maintained package  Session Info sessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.24 digest_0.6.29 R6_2.5.1 jsonlite_1.8.0 ## [5] magrittr_2.0.2 evaluate_0.15 blogdown_1.8.2 stringi_1.7.6 ## [9] rlang_1.0.2 cli_3.1.0 jquerylib_0.1.4 bslib_0.3.1 ## [13] rmarkdown_2.13 tools_4.1.3 stringr_1.4.0 xfun_0.30 ## [17] yaml_2.3.5 fastmap_1.1.0 compiler_4.1.3 htmltools_0.5.2 ## [21] knitr_1.37 sass_0.4.0  References Bischl, Bernd, Michel Lang, Olaf Mersmann, Jörg Rahnenführer, and Claus Weihs. 2015. “BatchJobs and BatchExperiments: Abstraction Mechanisms for Using R in Batch Environments.” Journal of Statistical Software. http://www.jstatsoft.org/v64/i11/.\n  ","categories":"","description":"","excerpt":"Overview  R provides a large number of packages for parallel …","ref":"/tutorials/rparallel/rparallel/","tags":"","title":"Parallel Evaluations in R"},{"body":"Internal  This page provides links to password protected resources that are only accessible to the instructor and/or students enrolled in this class.\n  GEN242 on Canvas/eLearn GitHub and GitHub Classroom for homework assignments and course projects Piazza for course communication  ","categories":"","description":"","excerpt":"Internal  This page provides links to password protected resources …","ref":"/about/internal/internal_resources/","tags":"","title":"Internal Resources"},{"body":"   This page provides instructions how to create new deployment instances of this teaching site, and how to configure and customize it. It uses the Docsy theme of the Hugo framework for building content driven websites.\n Quick start  Visit this template page Click on the Use this Template button. Choose a Repository Name Click on the Create repository from template button.  Usage locally  Go to your new repository on GitHub Click on the Code button. Copy the URL git@github.com:\u003cusername\u003e/\u003crepository_name\u003e.git Open terminal and run the following commands. Note: every time the repos is cloned from GitHub it is important to include the --recurse-submodules argument. This assures that required submodules will be cloned as well.  git clone --recurse-submodules --depth 1 git@github.com:\u003cusername\u003e/\u003crepository_name\u003e.git cd \u003crepository_name\u003e   Run the website locally  hugo server   Run the website locally with blogdown  blogdown::serve_site()  Prerequisites and Installation Install nodejs Download nodejs binary for 64 bit linux from here. Next, install it according the following instructions. Note, the version in all commands needs to match the downloaded one (here v16.14.2).\n Unzip the binary archive to any directory, where you wish to install nodejs. The following uses /usr/local/lib/nodejs.  VERSION=v16.14.2 DISTRO=linux-x64 sudo mkdir -p /usr/local/lib/nodejs sudo tar -xJvf node-$VERSION-$DISTRO.tar.xz -C /usr/local/lib/nodejs  Set the required environment variables by adding the following lines to your ~/.profile file.  # Nodejs VERSION=v16.14.2 DISTRO=linux-x64 export PATH=/usr/local/lib/nodejs/node-$VERSION-$DISTRO/bin:$PATH  Refresh ~/.profile and test versions  . ~/.profile node -v npm -v  Make executables available to root when using sudo npm install.  sudo ln -s -f /usr/local/lib/nodejs/node-v16.14.2-linux-x64/bin/node /usr/local/bin/node sudo ln -s -f /usr/local/lib/nodejs/node-v16.14.2-linux-x64/bin/npm /usr/local/bin/npm sudo ln -s -f /usr/local/lib/nodejs/node-v16.14.2-linux-x64/bin/npx /usr/local/bin/npx  Install blogdown and Hugo blogdown installed.packages(\"rstudio/blogdown\") # If anything wrong try develop version remotes::install_github(\"rstudio/blogdown\")  Hugo You need a recent extended version (we recommend version 0.79.0 or later) of Hugo to do local builds and previews of sites that use Docsy.\nIt is recommended to install Hugo from R for working with blogdown\nblogdown::install_hugo(extended = TRUE)  or from command-line from here\nwget https://github.com/gohugoio/hugo/releases/download/v0.94.2/hugo_extended_0.94.2_Linux-64bit.deb sudo dpkg -i hugo_extended_0.94.2_Linux-64bit.deb hugo version  For Windows and macOS please see instructions here.\nInstall PostCSS To build or update your site’s CSS resources, you also need PostCSS to create the final assets. If you need to install it, you must have a recent version of NodeJS installed on your machine so you can use npm, the Node package manager. By default npm installs tools under the directory where you run npm install:\nsudo npm install -D autoprefixer sudo npm install -D postcss-cli # Starting in version 8 of postcss-cli, you must also separately install postcss: sudo npm install -D postcss # go to your website directory cd \u003crepository_name\u003e npm audit fix  Run the website locally with blogdown  Open R in console or Rstudio  This repo contains an .Rprofile file that will automatically serve the site for you R starting directory is this newly cloned repository. Otherwise, change working directory to the repository and run:\nblogdown::serve_site()  You should see a website is opened in your local browser or Rstudio viewer.\nRun the website locally from terminal cd YOUR_NEW_REPO_PATH hugo server  Customize Color settings The background color of the top menu bar can be changed under assets/scss/_variables_project.scss. One custom setting in this file is that the original $primary: #30638E; has been changed to $primary: #28498f;.\n","categories":"","description":"","excerpt":"   This page provides instructions how to create new deployment …","ref":"/about/internal/install/","tags":"","title":"Deployment and Maintenance of this Site"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: [ .Rmd ] [ .R ]\n Overview Sequence Analysis in R and Bioconductor R Base\n Some basic string handling utilities. Wide spectrum of numeric data analysis tools.  Bioconductor\nBioconductor packages provide much more sophisticated string handling utilities for sequence analysis (Lawrence et al. 2013; Huber et al. 2015).\n Biostrings: general sequence analysis environment ShortRead: pipeline for short read data IRanges: low-level infrastructure for range data GenomicRanges: high-level infrastructure for range data GenomicFeatures: managing transcript centric annotations GenomicAlignments: handling short genomic alignments Rsamtools: interface to samtools, bcftools and tabix BSgenome: genome annotation data biomaRt: interface to BioMart annotations rtracklayer: Annotation imports, interface to online genome browsers HelloRanges: Bedtools semantics in Bioc’s Ranges infrastructure  Package Requirements Several Bioconductor packages are required for this tutorial. To install them, execute the following lines in the R console. Please also make sure that you have a recent R version installed on your system. R versions 4.0.x or higher are recommended.\nPlease do not run this install on the HPCC unless you want to reinstall some of these packages in your own user account.\nsource(\"https://bioconductor.org/biocLite.R\") if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(c(\"Biostrings\", \"GenomicRanges\", \"rtracklayer\", \"systemPipeR\", \"seqLogo\", \"ShortRead\"))  Strings in R Base Basic String Matching and Parsing String matching Generate sample sequence data set\nmyseq \u003c- c(\"ATGCAGACATAGTG\", \"ATGAACATAGATCC\", \"GTACAGATCAC\")  String searching with regular expression support\nmyseq[grep(\"ATG\", myseq)]  ## [1] \"ATGCAGACATAGTG\" \"ATGAACATAGATCC\"  Searches myseq for first match of pattern “AT”\npos1 \u003c- regexpr(\"AT\", myseq) as.numeric(pos1); attributes(pos1)$match.length # Returns position information of matches  ## [1] 1 1 7 ## [1] 2 2 2  Searches myseq for all matches of pattern “AT”\npos2 \u003c- gregexpr(\"AT\", myseq) as.numeric(pos2[[1]]); attributes(pos2[[1]])$match.length # Returns positions of matches in first sequence  ## [1] 1 9 ## [1] 2 2  String substitution with regular expression support\ngsub(\"^ATG\", \"atg\", myseq)  ## [1] \"atgCAGACATAGTG\" \"atgAACATAGATCC\" \"GTACAGATCAC\"  Positional parsing nchar(myseq) # Computes length of strings  ## [1] 14 14 11  substring(myseq[1], c(1,3), c(2,5)) # Positional parsing of several fragments from one string  ## [1] \"AT\" \"GCA\"  substring(myseq, c(1,4,7), c(2,6,10)) # Positional parsing of many strings  ## [1] \"AT\" \"AAC\" \"ATCA\"  Random Sequence Generation Random DNA sequences of any length rand \u003c- sapply(1:100, function(x) paste(sample(c(\"A\",\"T\",\"G\",\"C\"), sample(10:20), replace=TRUE), collapse=\"\")) rand[1:3]  ## [1] \"GCACACCATGTT\" \"CCAAGGGGTCACGCAAGTAA\" \"GAATACGGAAAAG\"  Count identical sequences table(c(rand[1:4], rand[1]))  ## ## CCAAGGGGTCACGCAAGTAA GAATACGGAAAAG GCACACCATGTT TAATCATCGAAGTT ## 1 1 2 1  Extract reads from reference Note: this requires the Biostrings package.\nlibrary(Biostrings) ref \u003c- DNAString(paste(sample(c(\"A\",\"T\",\"G\",\"C\"), 100000, replace=T), collapse=\"\")) randstart \u003c- sample(1:(length(ref)-15), 1000) randreads \u003c- Views(ref, randstart, width=15) rand_set \u003c- DNAStringSet(randreads) unlist(rand_set)  ## 15000-letter DNAString object ## seq: AATCGCGGCACTTAGCCAGCTTGTCTTATCGGGTATTTTGAATCTT...ACTGTCGCCAATGGGTCCAACGGATTCTACGCCACGACTCGTAACT  Sequences in Bioconductor Important Data Objects of Biostrings XString for single sequence  DNAString: for DNA RNAString: for RNA AAString: for amino acid BString: for any string  XStringSet for many sequences  `DNAStringSet``: for DNA RNAStringSet: for RNA AAStringSet: for amino acid BStringSet: for any string  QualityScaleXStringSet for sequences with quality data  QualityScaledDNAStringSet: for DNA QualityScaledRNAStringSet: for RNA QualityScaledAAStringSet: for amino acid QualityScaledBStringSet: for any string  Sequence Import and Export Download the following sequences to your current working directory and then import them into R: https://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Bacteria/Halobacterium_sp_uid217/AE004437.ffn\ndir.create(\"data\", showWarnings = FALSE) # system(\"wget https://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Bacteria/Halobacterium_sp_uid217/AE004437.ffn\") download.file(\"https://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Bacteria/Halobacterium_sp_uid217/AE004437.ffn\", \"data/AE004437.ffn\")  Import FASTA file with readDNAStringSet\nmyseq \u003c- readDNAStringSet(\"data/AE004437.ffn\") myseq[1:3]  ## DNAStringSet object of length 3: ## width seq names ## [1] 1206 ATGACTCGGCGGTCTCGTGTCGGTGCCGGCCTC...GTCGTCGTTGTTCGACGCTGGCGGAACCCATGA gi|12057215|gb|AE... ## [2] 666 ATGAGCATCATCGAACTCGAAGGCGTGGTCAAA...GTCAACCTCGTCGATGGGGTGTTACACACGTGA gi|12057215|gb|AE... ## [3] 1110 ATGGCGTGGCGGAACCTCGGGCGGAACCGCGTG...AACGATCCGCCCGTCGAGGCGCTCGGCGAATGA gi|12057215|gb|AE...  Subset sequences with regular expression on sequence name field\nsub \u003c- myseq[grep(\"99.*\", names(myseq))] length(sub)  ## [1] 170  Export subsetted sequences to FASTA file\nwriteXStringSet(sub, file=\"./data/AE004437sub.ffn\", width=80)  Now inspect exported sequence file AE004437sub.ffn in a text editor\nWorking with XString Containers The XString stores the different types of biosequences in dedicated containers\nlibrary(Biostrings) d \u003c- DNAString(\"GCATAT-TAC\") d  ## 10-letter DNAString object ## seq: GCATAT-TAC  d[1:4]  ## 4-letter DNAString object ## seq: GCAT  RNA sequences\nr \u003c- RNAString(\"GCAUAU-UAC\") r \u003c- RNAString(d) # Converts d to RNAString object r  ## 10-letter RNAString object ## seq: GCAUAU-UAC  Protein sequences\np \u003c- AAString(\"HCWYHH\") p  ## 6-letter AAString object ## seq: HCWYHH  Any type of character strings\nb \u003c- BString(\"I store any set of characters. Other XString objects store only the IUPAC characters.\") b  ## 85-letter BString object ## seq: I store any set of characters. Other XString objects store only the IUPAC characters.  Working with XStringSet Containers XStringSet containers allow to store many biosequences in one object\ndset \u003c- DNAStringSet(c(\"GCATATTAC\", \"AATCGATCC\", \"GCATATTAC\")) names(dset) \u003c- c(\"seq1\", \"seq2\", \"seq3\") # Assigns names dset[1:2]  ## DNAStringSet object of length 2: ## width seq names ## [1] 9 GCATATTAC seq1 ## [2] 9 AATCGATCC seq2  Important utilities for XStringSet containers\nwidth(dset) # Returns the length of each sequences  ## [1] 9 9 9  d \u003c- dset[[1]] # The [[ subsetting operator returns a single entry as XString object dset2 \u003c- c(dset, dset) # Appends/concatenates two XStringSet objects dsetchar \u003c- as.character(dset) # Converts XStringSet to named vector dsetone \u003c- unlist(dset) # Collapses many sequences to a single one stored in a DNAString container  Sequence subsetting by positions:\nDNAStringSet(dset, start=c(1,2,3), end=c(4,8,5))  ## DNAStringSet object of length 3: ## width seq names ## [1] 4 GCAT seq1 ## [2] 7 ATCGATC seq2 ## [3] 3 ATA seq3  Multiple Alignment Class The XMultipleAlignment class stores the different types of multiple sequence alignments:\norigMAlign \u003c- readDNAMultipleAlignment(filepath = system.file(\"extdata\", \"msx2_mRNA.aln\", package = \"Biostrings\"), format = \"clustal\") origMAlign  ## DNAMultipleAlignment with 8 rows and 2343 columns ## aln names ## [1] -----TCCCGTCTCCGCAGCAAAAAAGTTTGAGTCG...TTGTCCAAACTCACAATTAAAAAAAAAAAAAAAAA gi|84452153|ref|N... ## [2] ------------------------------------...----------------------------------- gi|208431713|ref|... ## [3] ------------------------------------...----------------------------------- gi|118601823|ref|... ## [4] ----------------------AAAAGTTGGAGTCT...----------------------------------- gi|114326503|ref|... ## [5] ------------------------------------...----------------------------------- gi|119220589|ref|... ## [6] ------------------------------------...----------------------------------- gi|148540149|ref|... ## [7] --------------CGGCTCCGCAGCGCCTCACTCG...----------------------------------- gi|45383056|ref|N... ## [8] GGGGGAGACTTCAGAAGTTGTTGTCCTCTCCGCTGA...----------------------------------- gi|213515133|ref|...  Basic Sequence Manipulations Reverse and Complement randset \u003c- DNAStringSet(rand) complement(randset[1:2])  ## DNAStringSet object of length 2: ## width seq ## [1] 12 CGTGTGGTACAA ## [2] 20 GGTTCCCCAGTGCGTTCATT  reverse(randset[1:2])  ## DNAStringSet object of length 2: ## width seq ## [1] 12 TTGTACCACACG ## [2] 20 AATGAACGCACTGGGGAACC  reverseComplement(randset[1:2])  ## DNAStringSet object of length 2: ## width seq ## [1] 12 AACATGGTGTGC ## [2] 20 TTACTTGCGTGACCCCTTGG  Translate DNA into Protein translate(randset[1:2])  ## Warning in .Call2(\"DNAStringSet_translate\", x, skip_code, dna_codes[codon_alphabet], : in 'x[[2]]': ## last 2 bases were ignored ## AAStringSet object of length 2: ## width seq ## [1] 4 AHHV ## [2] 6 PRGHAS  Pattern Matching Pattern matching with mismatches Find pattern matches in reference\nmyseq1 \u003c- readDNAStringSet(\"./data/AE004437.ffn\") mypos \u003c- matchPattern(\"ATGGTG\", myseq1[[1]], max.mismatch=1)  Count only the corresponding matches\ncountPattern(\"ATGGCT\", myseq1[[1]], max.mismatch=1)  ## [1] 3  Count matches in many sequences\nvcountPattern(\"ATGGCT\", myseq1, max.mismatch=1)[1:20]  ## [1] 3 0 5 4 1 2 2 1 4 3 0 0 1 2 0 1 4 0 0 1  Results shown in DNAStringSet object\ntmp \u003c- c(DNAStringSet(\"ATGGTG\"), DNAStringSet(mypos))  Return a consensus matrix for query and hits\nconsensusMatrix(tmp)[1:4,]  ## [,1] [,2] [,3] [,4] [,5] [,6] ## A 3 0 0 0 0 0 ## C 1 1 0 0 0 0 ## G 0 0 4 4 1 4 ## T 0 3 0 0 3 0  Find all pattern matches in reference\nmyvpos \u003c- vmatchPattern(\"ATGGCT\", myseq1, max.mismatch=1) myvpos # The results are stored as MIndex object.  ## MIndex object of length 2058 ## $`gi|12057215|gb|AE004437.1|:248-1453 Halobacterium sp. NRC-1, complete genome` ## IRanges object with 3 ranges and 0 metadata columns: ## start end width ## \u003cinteger\u003e \u003cinteger\u003e \u003cinteger\u003e ## [1] 1 6 6 ## [2] 383 388 6 ## [3] 928 933 6 ## ## $`gi|12057215|gb|AE004437.1|:1450-2115 Halobacterium sp. NRC-1, complete genome` ## IRanges object with 0 ranges and 0 metadata columns: ## start end width ## \u003cinteger\u003e \u003cinteger\u003e \u003cinteger\u003e ## ## $`gi|12057215|gb|AE004437.1|:2145-3254 Halobacterium sp. NRC-1, complete genome` ## IRanges object with 5 ranges and 0 metadata columns: ## start end width ## \u003cinteger\u003e \u003cinteger\u003e \u003cinteger\u003e ## [1] 1 6 6 ## [2] 94 99 6 ## [3] 221 226 6 ## [4] 535 540 6 ## [5] 601 606 6 ## ## ... ## \u003c2055 more elements\u003e  Views(myseq1[[1]], start(myvpos[[1]]), end(myvpos[[1]])) # Retrieves the result for single entry  ## Views on a 1206-letter DNAString subject ## subject: ATGACTCGGCGGTCTCGTGTCGGTGCCGGCCTCGCAGCCATTGT...TTGCGATCGTCGTCGTCGTTGTTCGACGCTGGCGGAACCCATGA ## views: ## start end width ## [1] 1 6 6 [ATGACT] ## [2] 383 388 6 [ATGGCA] ## [3] 928 933 6 [ATGACT]  Return all matches\nsapply(names(myseq1), function(x) as.character(Views(myseq1[[x]], start(myvpos[[x]]), end(myvpos[[x]]))))[1:4]  Pattern matching with regular expression support myseq \u003c- DNAStringSet(c(\"ATGCAGACATAGTG\", \"ATGAACATAGATCC\", \"GTACAGATCAC\")) myseq[grep(\"^ATG\", myseq, perl=TRUE)] # String searching with regular expression support  ## DNAStringSet object of length 2: ## width seq ## [1] 14 ATGCAGACATAGTG ## [2] 14 ATGAACATAGATCC  pos1 \u003c- regexpr(\"AT\", myseq) # Searches 'myseq' for first match of pattern \"AT\" as.numeric(pos1); attributes(pos1)$match.length # Returns position information of matches  ## [1] 1 1 7 ## [1] 2 2 2  pos2 \u003c- gregexpr(\"AT\", myseq) # Searches 'myseq' for all matches of pattern \"AT\" as.numeric(pos2[[1]]); attributes(pos2[[1]])$match.length # Match positions in first sequence  ## [1] 1 9 ## [1] 2 2  DNAStringSet(gsub(\"^ATG\", \"NNN\", myseq)) # String substitution with regular expression support  ## DNAStringSet object of length 3: ## width seq ## [1] 14 NNNCAGACATAGTG ## [2] 14 NNNAACATAGATCC ## [3] 11 GTACAGATCAC  PWM Viewing and Searching Plot with seqLogo library(seqLogo) pwm \u003c- PWM(DNAStringSet(c(\"GCT\", \"GGT\", \"GCA\"))) pwm  ## [,1] [,2] [,3] ## A 0.0000000 0.0000000 0.2312611 ## C 0.0000000 0.3157205 0.0000000 ## G 0.3685591 0.2312611 0.0000000 ## T 0.0000000 0.0000000 0.3157205  seqLogo(t(t(pwm) * 1/colSums(pwm)))  Plot with ggseqlogo The ggseqlogo package (manual) provides many customization options for plotting sequence logos. It also supports various alphabets including sequence logos for amino acid sequences.\nlibrary(ggplot2); library(ggseqlogo) pwm \u003c- PWM(DNAStringSet(c(\"GCT\", \"GGT\", \"GCA\"))) ggseqlogo(pwm)  ## Warning: `guides(\u003cscale\u003e = FALSE)` is deprecated. Please use `guides(\u003cscale\u003e = \"none\")` instead.  Search sequence for PWM matches with score better than min.score\nchr \u003c- DNAString(\"AAAGCTAAAGGTAAAGCAAAA\") matchPWM(pwm, chr, min.score=0.9)  ## Views on a 21-letter DNAString subject ## subject: AAAGCTAAAGGTAAAGCAAAA ## views: ## start end width ## [1] 4 6 3 [GCT] ## [2] 10 12 3 [GGT] ## [3] 16 18 3 [GCA]  NGS Sequences Sequence and Quality Data: FASTQ Format Four lines per sequence:\n ID Sequence ID Base call qualities (Phred scores) as ASCII characters  The following gives an example of 3 Illumina reads in a FASTQ file. The numbers at the beginning of each line are not part of the FASTQ format. They have been added solely for illustration purposes.\n1. @SRR038845.3 HWI-EAS038:6:1:0:1938 length=36 2. CAACGAGTTCACACCTTGGCCGACAGGCCCGGGTAA 3. +SRR038845.3 HWI-EAS038:6:1:0:1938 length=36 4. BA@7\u003eB=\u003e:\u003e\u003e7@7@\u003e\u003e9=BAA?;\u003e52;\u003e:9=8.=A 1. @SRR038845.41 HWI-EAS038:6:1:0:1474 length=36 2. CCAATGATTTTTTTCCGTGTTTCAGAATACGGTTAA 3. +SRR038845.41 HWI-EAS038:6:1:0:1474 length=36 4. BCCBA@BB@BBBBAB@B9B@=BABA@A:@693:@B= 1. @SRR038845.53 HWI-EAS038:6:1:1:360 length=36 2. GTTCAAAAAGAACTAAATTGTGTCAATAGAAAACTC 3. +SRR038845.53 HWI-EAS038:6:1:1:360 length=36 4. BBCBBBBBB@@BAB?BBBBCBC\u003eBBBAA8\u003eBBBAA@  Sequence and Quality Data: QualityScaleXStringSet Phred quality scores are integers from 0-50 that are stored as ASCII characters after adding 33. The basic R functions rawToChar and charToRaw can be used to interconvert among their representations.\nPhred score interconversion\nphred \u003c- 1:9 phreda \u003c- paste(sapply(as.raw((phred)+33), rawToChar), collapse=\"\") phreda  ## [1] \"\\\"#$%\u0026'()*\"  as.integer(charToRaw(phreda))-33  ## [1] 1 2 3 4 5 6 7 8 9  Construct QualityScaledDNAStringSet from scratch\ndset \u003c- DNAStringSet(sapply(1:100, function(x) paste(sample(c(\"A\",\"T\",\"G\",\"C\"), 20, replace=T), collapse=\"\"))) # Creates random sample sequence. myqlist \u003c- lapply(1:100, function(x) sample(1:40, 20, replace=T)) # Creates random Phred score list. myqual \u003c- sapply(myqlist, function(x) toString(PhredQuality(x))) # Converts integer scores into ASCII characters. myqual \u003c- PhredQuality(myqual) # Converts to a PhredQuality object. dsetq1 \u003c- QualityScaledDNAStringSet(dset, myqual) # Combines DNAStringSet and quality data in QualityScaledDNAStringSet object. dsetq1[1:2]  ## A QualityScaledDNAStringSet instance containing: ## ## DNAStringSet object of length 2: ## width seq ## [1] 20 TAAAGGGCCCTACGCCTTGC ## [2] 20 TGCCTACACACAGTTAGTAC ## ## PhredQuality object of length 2: ## width seq ## [1] 20 DE4\u003c,-D;7:E94G724C1E ## [2] 20 H#)I\"/+2)3\u003e/I@?9'8/)  Processing FASTQ Files with ShortRead The following explains the basic usage of ShortReadQ objects. To make the sample code work, download and unzip this file to your current working directory. The following code performs the download for you.\nlibrary(ShortRead) download.file(\"http://cluster.hpcc.ucr.edu/~tgirke/HTML_Presentations/Manuals/testdata/samplefastq/data.zip\", \"data.zip\") unzip(\"data.zip\")  Important utilities for accessing FASTQ files\nfastq \u003c- list.files(\"data\", \"*.fastq$\"); fastq \u003c- paste(\"data/\", fastq, sep=\"\") names(fastq) \u003c- paste(\"flowcell6_lane\", 1:length(fastq), sep=\"_\") (fq \u003c- readFastq(fastq[1])) # Imports first FASTQ file  ## class: ShortReadQ ## length: 1000 reads; width: 36 cycles  countLines(dirPath=\"./data\", pattern=\".fastq$\")/4 # Counts numbers of reads in FASTQ files  ## SRR038845.fastq SRR038846.fastq SRR038848.fastq SRR038850.fastq ## 1000 1000 1000 1000  id(fq)[1] # Returns ID field  ## BStringSet object of length 1: ## width seq ## [1] 43 SRR038845.3 HWI-EAS038:6:1:0:1938 length=36  sread(fq)[1] # Returns sequence  ## DNAStringSet object of length 1: ## width seq ## [1] 36 CAACGAGTTCACACCTTGGCCGACAGGCCCGGGTAA  quality(fq)[1] # Returns Phred scores  ## class: FastqQuality ## quality: ## BStringSet object of length 1: ## width seq ## [1] 36 BA@7\u003eB=\u003e:\u003e\u003e7@7@\u003e\u003e9=BAA?;\u003e52;\u003e:9=8.=A  as(quality(fq), \"matrix\")[1:4,1:12] # Coerces Phred scores to numeric matrix  ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 33 32 31 22 29 33 28 29 25 29 29 22 ## [2,] 33 34 34 33 32 31 33 33 31 33 33 33 ## [3,] 33 33 34 33 33 33 33 33 33 31 31 33 ## [4,] 33 33 33 33 31 33 28 31 28 32 33 33  ShortReadQ(sread=sread(fq), quality=quality(fq), id=id(fq)) # Constructs a ShortReadQ from components  ## class: ShortReadQ ## length: 1000 reads; width: 36 cycles  FASTQ Quality Reports Using systemPipeR The following seeFastq and seeFastqPlot functions generate and plot a series of useful quality statistics for a set of FASTQ files.\nlibrary(systemPipeR) fqlist \u003c- seeFastq(fastq=fastq, batchsize=800, klength=8) # For real data set batchsize to at least 10^5 seeFastqPlot(fqlist)  Handles many samples in one PDF file. For more details see here\nUsing ShortRead The ShortRead package contains several FASTQ quality reporting functions.\nsp \u003c- SolexaPath(system.file('extdata', package='ShortRead')) fl \u003c- file.path(analysisPath(sp), \"s_1_sequence.txt\") fls \u003c- c(fl, fl) coll \u003c- QACollate(QAFastqSource(fls), QAReadQuality(), QAAdapterContamination(), QANucleotideUse(), QAQualityUse(), QASequenceUse(), QAFrequentSequence(n=10), QANucleotideByCycle(), QAQualityByCycle()) x \u003c- qa2(coll, verbose=TRUE) res \u003c- report(x) if(interactive()) browseURL(res)  Filtering and Trimming FASTQ Files with ShortRead Adaptor trimming fqtrim \u003c- trimLRPatterns(Rpattern=\"GCCCGGGTAA\", subject=fq) sread(fq)[1:2] # Before trimming  ## DNAStringSet object of length 2: ## width seq ## [1] 36 CAACGAGTTCACACCTTGGCCGACAGGCCCGGGTAA ## [2] 36 CCAATGATTTTTTTCCGTGTTTCAGAATACGGTTAA  sread(fqtrim)[1:2] # After trimming  ## DNAStringSet object of length 2: ## width seq ## [1] 26 CAACGAGTTCACACCTTGGCCGACAG ## [2] 36 CCAATGATTTTTTTCCGTGTTTCAGAATACGGTTAA  Read counting and duplicate removal tables(fq)$distribution # Counts read occurences  ## nOccurrences nReads ## 1 1 948 ## 2 2 26  sum(srduplicated(fq)) # Identifies duplicated reads  ## [1] 26  fq[!srduplicated(fq)]  ## class: ShortReadQ ## length: 974 reads; width: 36 cycles  Trimming low quality tails cutoff \u003c- 30 cutoff \u003c- rawToChar(as.raw(cutoff+33)) sread(trimTails(fq, k=2, a=cutoff, successive=FALSE))[1:2]  ## DNAStringSet object of length 2: ## width seq ## [1] 4 CAAC ## [2] 20 CCAATGATTTTTTTCCGTGT  Removal of reads with Phred scores below a threshold value cutoff \u003c- 30 qcount \u003c- rowSums(as(quality(fq), \"matrix\") \u003c= 20) fq[qcount == 0] # Number of reads where all Phred scores \u003e= 20  ## class: ShortReadQ ## length: 349 reads; width: 36 cycles  Removal of reads with x Ns and/or low complexity segments filter1 \u003c- nFilter(threshold=1) # Keeps only reads without Ns filter2 \u003c- polynFilter(threshold=20, nuc=c(\"A\",\"T\",\"G\",\"C\")) # Removes reads with nucleotide bias, \u003e=20 of any base filter \u003c- compose(filter1, filter2) fq[filter(fq)]  ## class: ShortReadQ ## length: 989 reads; width: 36 cycles  Memory Efficient FASTQ Processing Streaming through FASTQ files with FastqStreamer and random sampling reads with FastqSampler\nfq \u003c- yield(FastqStreamer(fastq[1], 50)) # Imports first 50 reads fq \u003c- yield(FastqSampler(fastq[1], 50)) # Random samples 50 reads  Streaming through a FASTQ file while applying filtering/trimming functions and writing the results to a new file here SRR038845.fastq_sub in data directory.\nf \u003c- FastqStreamer(fastq[1], 50) while(length(fq \u003c- yield(f))) { fqsub \u003c- fq[grepl(\"^TT\", sread(fq))] writeFastq(fqsub, paste(fastq[1], \"sub\", sep=\"_\"), mode=\"a\", compress=FALSE) } close(f)  Range Operations Important Data Objects for Range Operations  IRanges: stores range data only (IRanges library) GRanges: stores ranges and annotations (GenomicRanges library) GRangesList: list version of GRanges container (GenomicRanges library)  Range Data Are Stored in IRanges and GRanges Containers Construct GRanges Object library(GenomicRanges); library(rtracklayer) gr \u003c- GRanges(seqnames = Rle(c(\"chr1\", \"chr2\", \"chr1\", \"chr3\"), c(1, 3, 2, 4)), ranges = IRanges(1:10, end = 7:16, names = head(letters, 10)), strand = Rle(strand(c(\"-\", \"+\", \"*\", \"+\", \"-\")), c(1, 2, 2, 3, 2)), score = 1:10, GC = seq(1, 0, length = 10)) # Example of creating a GRanges object with its constructor function.  Import GFF into GRanges Object gff \u003c- import.gff(\"http://cluster.hpcc.ucr.edu/~tgirke/Documents/R_BioCond/Samples/gff3.gff\") # Imports a simplified GFF3 genome annotation file. seqlengths(gff) \u003c- end(ranges(gff[which(values(gff)[,\"type\"]==\"chromosome\"),])) names(gff) \u003c- 1:length(gff) # Assigns names to corresponding slot gff[1:4,]  ## GRanges object with 4 ranges and 10 metadata columns: ## seqnames ranges strand | source type score phase ID ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e \u003ccharacter\u003e ## 1 Chr1 1-30427671 + | TAIR10 chromosome NA \u003cNA\u003e Chr1 ## 2 Chr1 3631-5899 + | TAIR10 gene NA \u003cNA\u003e AT1G01010 ## 3 Chr1 3631-5899 + | TAIR10 mRNA NA \u003cNA\u003e AT1G01010.1 ## 4 Chr1 3760-5630 + | TAIR10 protein NA \u003cNA\u003e AT1G01010.1-Protein ## Name Note Parent Index Derives_from ## \u003ccharacter\u003e \u003cCharacterList\u003e \u003cCharacterList\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 1 Chr1 \u003cNA\u003e \u003cNA\u003e ## 2 AT1G01010 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## 3 AT1G01010.1 AT1G01010 1 \u003cNA\u003e ## 4 AT1G01010.1 \u003cNA\u003e AT1G01010.1 ## ------- ## seqinfo: 7 sequences from an unspecified genome  seqinfo(gff)  ## Seqinfo object with 7 sequences from an unspecified genome: ## seqnames seqlengths isCircular genome ## Chr1 30427671 NA \u003cNA\u003e ## Chr2 19698289 NA \u003cNA\u003e ## Chr3 23459830 NA \u003cNA\u003e ## Chr4 18585056 NA \u003cNA\u003e ## Chr5 26975502 NA \u003cNA\u003e ## ChrC 154478 NA \u003cNA\u003e ## ChrM 366924 NA \u003cNA\u003e  Coerce GRanges object to data.frame as.data.frame(gff)[1:4, 1:7]  ## seqnames start end width strand source type ## 1 Chr1 1 30427671 30427671 + TAIR10 chromosome ## 2 Chr1 3631 5899 2269 + TAIR10 gene ## 3 Chr1 3631 5899 2269 + TAIR10 mRNA ## 4 Chr1 3760 5630 1871 + TAIR10 protein  Utilities for Range Containers Accessor and subsetting methods for GRanges objects Subsetting and replacement\ngff[1:4]  ## GRanges object with 4 ranges and 10 metadata columns: ## seqnames ranges strand | source type score phase ID ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e \u003ccharacter\u003e ## 1 Chr1 1-30427671 + | TAIR10 chromosome NA \u003cNA\u003e Chr1 ## 2 Chr1 3631-5899 + | TAIR10 gene NA \u003cNA\u003e AT1G01010 ## 3 Chr1 3631-5899 + | TAIR10 mRNA NA \u003cNA\u003e AT1G01010.1 ## 4 Chr1 3760-5630 + | TAIR10 protein NA \u003cNA\u003e AT1G01010.1-Protein ## Name Note Parent Index Derives_from ## \u003ccharacter\u003e \u003cCharacterList\u003e \u003cCharacterList\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 1 Chr1 \u003cNA\u003e \u003cNA\u003e ## 2 AT1G01010 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## 3 AT1G01010.1 AT1G01010 1 \u003cNA\u003e ## 4 AT1G01010.1 \u003cNA\u003e AT1G01010.1 ## ------- ## seqinfo: 7 sequences from an unspecified genome  gff[1:4, c(\"type\", \"ID\")]  ## GRanges object with 4 ranges and 2 metadata columns: ## seqnames ranges strand | type ID ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003ccharacter\u003e ## 1 Chr1 1-30427671 + | chromosome Chr1 ## 2 Chr1 3631-5899 + | gene AT1G01010 ## 3 Chr1 3631-5899 + | mRNA AT1G01010.1 ## 4 Chr1 3760-5630 + | protein AT1G01010.1-Protein ## ------- ## seqinfo: 7 sequences from an unspecified genome  gff[2] \u003c- gff[3]  GRanges objects can be concatenated with the c function\nc(gff[1:2], gff[401:402])  ## GRanges object with 4 ranges and 10 metadata columns: ## seqnames ranges strand | source type score phase ID ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e \u003ccharacter\u003e ## 1 Chr1 1-30427671 + | TAIR10 chromosome NA \u003cNA\u003e Chr1 ## 2 Chr1 3631-5899 + | TAIR10 mRNA NA \u003cNA\u003e AT1G01010.1 ## 401 Chr5 5516-5769 - | TAIR10 protein NA \u003cNA\u003e AT5G01015.2-Protein ## 402 Chr5 5770-5801 - | TAIR10 five_prime_UTR NA \u003cNA\u003e \u003cNA\u003e ## Name Note Parent Index Derives_from ## \u003ccharacter\u003e \u003cCharacterList\u003e \u003cCharacterList\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 1 Chr1 \u003cNA\u003e \u003cNA\u003e ## 2 AT1G01010.1 AT1G01010 1 \u003cNA\u003e ## 401 AT5G01015.2 \u003cNA\u003e AT5G01015.2 ## 402 \u003cNA\u003e AT5G01015.2 \u003cNA\u003e \u003cNA\u003e ## ------- ## seqinfo: 7 sequences from an unspecified genome  Acessor functions\nseqnames(gff)  ## factor-Rle of length 449 with 7 runs ## Lengths: 72 22 38 118 172 13 14 ## Values : Chr1 Chr2 Chr3 Chr4 Chr5 ChrC ChrM ## Levels(7): Chr1 Chr2 Chr3 Chr4 Chr5 ChrC ChrM  ranges(gff)  ## IRanges object with 449 ranges and 0 metadata columns: ## start end width ## \u003cinteger\u003e \u003cinteger\u003e \u003cinteger\u003e ## 1 1 30427671 30427671 ## 2 3631 5899 2269 ## 3 3631 5899 2269 ## 4 3760 5630 1871 ## 5 3631 3913 283 ## ... ... ... ... ## 445 11918 12241 324 ## 446 11918 12241 324 ## 447 11918 12241 324 ## 448 11918 12241 324 ## 449 11918 12241 324  strand(gff)  ## factor-Rle of length 449 with 13 runs ## Lengths: 18 54 28 21 12 117 1 171 1 12 1 8 5 ## Values : + - + - + - + - + - + - + ## Levels(3): + - *  seqlengths(gff)  ## Chr1 Chr2 Chr3 Chr4 Chr5 ChrC ChrM ## 30427671 19698289 23459830 18585056 26975502 154478 366924  start(gff[1:4])  ## [1] 1 3631 3631 3760  end(gff[1:4])  ## [1] 30427671 5899 5899 5630  width(gff[1:4])  ## [1] 30427671 2269 2269 1871  Accessing metadata component\nvalues(gff) # or elementMetadata(gff)  ## DataFrame with 449 rows and 10 columns ## source type score phase ID Name Note ## \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e \u003ccharacter\u003e \u003ccharacter\u003e \u003cCharacterList\u003e ## 1 TAIR10 chromosome NA NA Chr1 Chr1 ## 2 TAIR10 mRNA NA NA AT1G01010.1 AT1G01010.1 ## 3 TAIR10 mRNA NA NA AT1G01010.1 AT1G01010.1 ## 4 TAIR10 protein NA NA AT1G01010.1-Protein AT1G01010.1 ## 5 TAIR10 exon NA NA NA NA ## ... ... ... ... ... ... ... ... ## 445 TAIR10 gene NA NA ATMG00030 ATMG00030 protein_coding_gene ## 446 TAIR10 mRNA NA NA ATMG00030.1 ATMG00030.1 ## 447 TAIR10 protein NA NA ATMG00030.1-Protein ATMG00030.1 ## 448 TAIR10 exon NA NA NA NA ## 449 TAIR10 CDS NA 0 NA NA ## Parent Index Derives_from ## \u003cCharacterList\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 1 NA NA ## 2 AT1G01010 1 NA ## 3 AT1G01010 1 NA ## 4 NA AT1G01010.1 ## 5 AT1G01010.1 NA NA ## ... ... ... ... ## 445 NA NA ## 446 ATMG00030 1 NA ## 447 NA ATMG00030.1 ## 448 ATMG00030.1 NA NA ## 449 ATMG00030.1,ATMG00030.1-Protein NA NA  values(gff)[, \"type\"][1:20]  ## [1] chromosome mRNA mRNA protein exon five_prime_UTR ## [7] CDS exon CDS exon CDS exon ## [13] CDS exon CDS exon CDS three_prime_UTR ## [19] gene mRNA ## Levels: chromosome gene mRNA protein exon five_prime_UTR CDS three_prime_UTR rRNA tRNA  gff[values(gff)[ ,\"type\"] == \"gene\"]  ## GRanges object with 21 ranges and 10 metadata columns: ## seqnames ranges strand | source type score phase ID Name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 19 Chr1 5928-8737 - | TAIR10 gene NA \u003cNA\u003e AT1G01020 AT1G01020 ## 64 Chr1 11649-13714 - | TAIR10 gene NA \u003cNA\u003e AT1G01030 AT1G01030 ## 74 Chr2 1025-2810 + | TAIR10 gene NA \u003cNA\u003e AT2G01008 AT2G01008 ## 84 Chr2 3706-5513 + | TAIR10 gene NA \u003cNA\u003e AT2G01010 AT2G01010 ## 87 Chr2 5782-5945 + | TAIR10 gene NA \u003cNA\u003e AT2G01020 AT2G01020 ## ... ... ... ... . ... ... ... ... ... ... ## 427 ChrC 383-1444 - | TAIR10 gene NA \u003cNA\u003e ATCG00020 ATCG00020 ## 432 ChrC 1717-4347 - | TAIR10 gene NA \u003cNA\u003e ATCG00030 ATCG00030 ## 437 ChrM 273-734 - | TAIR10 gene NA \u003cNA\u003e ATMG00010 ATMG00010 ## 442 ChrM 8848-11415 - | TAIR10 gene NA \u003cNA\u003e ATMG00020 ATMG00020 ## 445 ChrM 11918-12241 + | TAIR10 gene NA \u003cNA\u003e ATMG00030 ATMG00030 ## Note Parent Index Derives_from ## \u003cCharacterList\u003e \u003cCharacterList\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 19 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## 64 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## 74 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## 84 rRNA \u003cNA\u003e \u003cNA\u003e ## 87 rRNA \u003cNA\u003e \u003cNA\u003e ## ... ... ... ... ... ## 427 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## 432 tRNA \u003cNA\u003e \u003cNA\u003e ## 437 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## 442 rRNA \u003cNA\u003e \u003cNA\u003e ## 445 protein_coding_gene \u003cNA\u003e \u003cNA\u003e ## ------- ## seqinfo: 7 sequences from an unspecified genome  Useful utilities for GRanges objects Remove chromosome ranges\ngff \u003c- gff[values(gff)$type != \"chromosome\"]  Erase the strand information\nstrand(gff) \u003c- \"*\"  Collapses overlapping ranges to continuous ranges.\nreduce(gff)  ## GRanges object with 22 ranges and 0 metadata columns: ## seqnames ranges strand ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e ## [1] Chr1 3631-5899 * ## [2] Chr1 5928-8737 * ## [3] Chr1 11649-13714 * ## [4] Chr2 1025-2810 * ## [5] Chr2 3706-5513 * ## ... ... ... ... ## [18] ChrC 383-1444 * ## [19] ChrC 1717-4347 * ## [20] ChrM 273-734 * ## [21] ChrM 8848-11415 * ## [22] ChrM 11918-12241 * ## ------- ## seqinfo: 7 sequences from an unspecified genome  Return uncovered regions\ngaps(gff)  ## GRanges object with 43 ranges and 0 metadata columns: ## seqnames ranges strand ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e ## [1] Chr1 1-30427671 + ## [2] Chr1 1-30427671 - ## [3] Chr1 1-3630 * ## [4] Chr1 5900-5927 * ## [5] Chr1 8738-11648 * ## ... ... ... ... ## [39] ChrM 1-366924 - ## [40] ChrM 1-272 * ## [41] ChrM 735-8847 * ## [42] ChrM 11416-11917 * ## [43] ChrM 12242-366924 * ## ------- ## seqinfo: 7 sequences from an unspecified genome  More intuitive way to get uncovered regions\nsetdiff(as(seqinfo(gff), \"GRanges\"), gff)  ## GRanges object with 29 ranges and 0 metadata columns: ## seqnames ranges strand ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e ## [1] Chr1 1-3630 * ## [2] Chr1 5900-5927 * ## [3] Chr1 8738-11648 * ## [4] Chr1 13715-30427671 * ## [5] Chr2 1-1024 * ## ... ... ... ... ## [25] ChrC 4348-154478 * ## [26] ChrM 1-272 * ## [27] ChrM 735-8847 * ## [28] ChrM 11416-11917 * ## [29] ChrM 12242-366924 * ## ------- ## seqinfo: 7 sequences from an unspecified genome  Return disjoint ranges\ndisjoin(gff)  ## GRanges object with 211 ranges and 0 metadata columns: ## seqnames ranges strand ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e ## [1] Chr1 3631-3759 * ## [2] Chr1 3760-3913 * ## [3] Chr1 3914-3995 * ## [4] Chr1 3996-4276 * ## [5] Chr1 4277-4485 * ## ... ... ... ... ## [207] ChrC 1752-4310 * ## [208] ChrC 4311-4347 * ## [209] ChrM 273-734 * ## [210] ChrM 8848-11415 * ## [211] ChrM 11918-12241 * ## ------- ## seqinfo: 7 sequences from an unspecified genome  Returns coverage of ranges\ncoverage(gff)  ## RleList of length 7 ## $Chr1 ## integer-Rle of length 30427671 with 45 runs ## Lengths: 3630 129 154 82 281 ... 233 161 380 30413957 ## Values : 0 4 5 3 5 ... 4 2 4 0 ## ## $Chr2 ## integer-Rle of length 19698289 with 14 runs ## Lengths: 1024 248 185 53 362 ... 164 625 102 19691617 ## Values : 0 5 3 5 3 ... 3 0 5 0 ## ## $Chr3 ## integer-Rle of length 23459830 with 29 runs ## Lengths: 1652 145 139 111 95 ... 155 148 156 23453781 ## Values : 0 4 5 3 5 ... 3 5 4 0 ## ## $Chr4 ## integer-Rle of length 18585056 with 72 runs ## Lengths: 1179 357 1358 128 872 ... 212 114 74 18571697 ## Values : 0 5 0 5 3 ... 3 5 4 0 ## ## $Chr5 ## integer-Rle of length 26975502 with 64 runs ## Lengths: 1222 28 28 109 72 ... 76 55 174 26967058 ## Values : 0 4 7 13 16 ... 3 5 4 0 ## ## ... ## \u003c2 more elements\u003e  Return the index pairings for overlapping ranges\nfindOverlaps(gff, gff[1:4])  ## Hits object with 55 hits and 0 metadata columns: ## queryHits subjectHits ## \u003cinteger\u003e \u003cinteger\u003e ## [1] 1 1 ## [2] 1 2 ## [3] 1 4 ## [4] 1 3 ## [5] 2 1 ## ... ... ... ## [51] 16 1 ## [52] 16 2 ## [53] 16 3 ## [54] 17 1 ## [55] 17 2 ## ------- ## queryLength: 442 / subjectLength: 4  Counts overlapping ranges\ncountOverlaps(gff, gff[1:4])[1:40]  ## 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## 4 4 4 4 3 4 3 3 3 3 3 3 3 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 35 36 37 38 39 40 41 ## 0 0 0 0 0 0 0  Return only overlapping ranges\nsubsetByOverlaps(gff, gff[1:4])  ## GRanges object with 17 ranges and 10 metadata columns: ## seqnames ranges strand | source type score phase ID ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e \u003ccharacter\u003e ## 2 Chr1 3631-5899 * | TAIR10 mRNA NA \u003cNA\u003e AT1G01010.1 ## 3 Chr1 3631-5899 * | TAIR10 mRNA NA \u003cNA\u003e AT1G01010.1 ## 4 Chr1 3760-5630 * | TAIR10 protein NA \u003cNA\u003e AT1G01010.1-Protein ## 5 Chr1 3631-3913 * | TAIR10 exon NA \u003cNA\u003e \u003cNA\u003e ## 6 Chr1 3631-3759 * | TAIR10 five_prime_UTR NA \u003cNA\u003e \u003cNA\u003e ## .. ... ... ... . ... ... ... ... ... ## 14 Chr1 5174-5326 * | TAIR10 exon NA \u003cNA\u003e \u003cNA\u003e ## 15 Chr1 5174-5326 * | TAIR10 CDS NA 0 \u003cNA\u003e ## 16 Chr1 5439-5899 * | TAIR10 exon NA \u003cNA\u003e \u003cNA\u003e ## 17 Chr1 5439-5630 * | TAIR10 CDS NA 0 \u003cNA\u003e ## 18 Chr1 5631-5899 * | TAIR10 three_prime_UTR NA \u003cNA\u003e \u003cNA\u003e ## Name Note Parent Index Derives_from ## \u003ccharacter\u003e \u003cCharacterList\u003e \u003cCharacterList\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 2 AT1G01010.1 AT1G01010 1 \u003cNA\u003e ## 3 AT1G01010.1 AT1G01010 1 \u003cNA\u003e ## 4 AT1G01010.1 \u003cNA\u003e AT1G01010.1 ## 5 \u003cNA\u003e AT1G01010.1 \u003cNA\u003e \u003cNA\u003e ## 6 \u003cNA\u003e AT1G01010.1 \u003cNA\u003e \u003cNA\u003e ## .. ... ... ... ... ... ## 14 \u003cNA\u003e AT1G01010.1 \u003cNA\u003e \u003cNA\u003e ## 15 \u003cNA\u003e AT1G01010.1,AT1G01010.1-Protein \u003cNA\u003e \u003cNA\u003e ## 16 \u003cNA\u003e AT1G01010.1 \u003cNA\u003e \u003cNA\u003e ## 17 \u003cNA\u003e AT1G01010.1,AT1G01010.1-Protein \u003cNA\u003e \u003cNA\u003e ## 18 \u003cNA\u003e AT1G01010.1 \u003cNA\u003e \u003cNA\u003e ## ------- ## seqinfo: 7 sequences from an unspecified genome  GRangesList Objects sp \u003c- split(gff, seq(along=gff)) # Stores every range in separate component of a GRangesList object split(gff, seqnames(gff)) # Stores ranges of each chromosome in separate component.  ## GRangesList object of length 7: ## $Chr1 ## GRanges object with 71 ranges and 10 metadata columns: ## seqnames ranges strand | source type score phase ID ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e \u003ccharacter\u003e ## 2 Chr1 3631-5899 * | TAIR10 mRNA NA \u003cNA\u003e AT1G01010.1 ## 3 Chr1 3631-5899 * | TAIR10 mRNA NA \u003cNA\u003e AT1G01010.1 ## 4 Chr1 3760-5630 * | TAIR10 protein NA \u003cNA\u003e AT1G01010.1-Protein ## 5 Chr1 3631-3913 * | TAIR10 exon NA \u003cNA\u003e \u003cNA\u003e ## 6 Chr1 3631-3759 * | TAIR10 five_prime_UTR NA \u003cNA\u003e \u003cNA\u003e ## .. ... ... ... . ... ... ... ... ... ## 68 Chr1 13335-13714 * | TAIR10 exon NA \u003cNA\u003e \u003cNA\u003e ## 69 Chr1 12941-13173 * | TAIR10 five_prime_UTR NA \u003cNA\u003e \u003cNA\u003e ## 70 Chr1 11864-12940 * | TAIR10 CDS NA 0 \u003cNA\u003e ## 71 Chr1 11649-11863 * | TAIR10 three_prime_UTR NA \u003cNA\u003e \u003cNA\u003e ## 72 Chr1 11649-13173 * | TAIR10 exon NA \u003cNA\u003e \u003cNA\u003e ## Name Note Parent Index Derives_from ## \u003ccharacter\u003e \u003cCharacterList\u003e \u003cCharacterList\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## 2 AT1G01010.1 AT1G01010 1 \u003cNA\u003e ## 3 AT1G01010.1 AT1G01010 1 \u003cNA\u003e ## 4 AT1G01010.1 \u003cNA\u003e AT1G01010.1 ## 5 \u003cNA\u003e AT1G01010.1 \u003cNA\u003e \u003cNA\u003e ## 6 \u003cNA\u003e AT1G01010.1 \u003cNA\u003e \u003cNA\u003e ## .. ... ... ... ... ... ## 68 \u003cNA\u003e AT1G01030.1 \u003cNA\u003e \u003cNA\u003e ## 69 \u003cNA\u003e AT1G01030.1 \u003cNA\u003e \u003cNA\u003e ## 70 \u003cNA\u003e AT1G01030.1,AT1G01030.1-Protein \u003cNA\u003e \u003cNA\u003e ## 71 \u003cNA\u003e AT1G01030.1 \u003cNA\u003e \u003cNA\u003e ## 72 \u003cNA\u003e AT1G01030.1 \u003cNA\u003e \u003cNA\u003e ## ------- ## seqinfo: 7 sequences from an unspecified genome ## ## ... ## \u003c6 more elements\u003e  unlist(sp) # Returns data as GRanges object  ## GRanges object with 442 ranges and 10 metadata columns: ## seqnames ranges strand | source type score phase ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e \u003cfactor\u003e \u003cnumeric\u003e \u003cinteger\u003e ## 1.2 Chr1 3631-5899 * | TAIR10 mRNA NA \u003cNA\u003e ## 2.3 Chr1 3631-5899 * | TAIR10 mRNA NA \u003cNA\u003e ## 3.4 Chr1 3760-5630 * | TAIR10 protein NA \u003cNA\u003e ## 4.5 Chr1 3631-3913 * | TAIR10 exon NA \u003cNA\u003e ## 5.6 Chr1 3631-3759 * | TAIR10 five_prime_UTR NA \u003cNA\u003e ## ... ... ... ... . ... ... ... ... ## 438.445 ChrM 11918-12241 * | TAIR10 gene NA \u003cNA\u003e ## 439.446 ChrM 11918-12241 * | TAIR10 mRNA NA \u003cNA\u003e ## 440.447 ChrM 11918-12241 * | TAIR10 protein NA \u003cNA\u003e ## 441.448 ChrM 11918-12241 * | TAIR10 exon NA \u003cNA\u003e ## 442.449 ChrM 11918-12241 * | TAIR10 CDS NA 0 ## ID Name Note Parent ## \u003ccharacter\u003e \u003ccharacter\u003e \u003cCharacterList\u003e \u003cCharacterList\u003e ## 1.2 AT1G01010.1 AT1G01010.1 AT1G01010 ## 2.3 AT1G01010.1 AT1G01010.1 AT1G01010 ## 3.4 AT1G01010.1-Protein AT1G01010.1 ## 4.5 \u003cNA\u003e \u003cNA\u003e AT1G01010.1 ## 5.6 \u003cNA\u003e \u003cNA\u003e AT1G01010.1 ## ... ... ... ... ... ## 438.445 ATMG00030 ATMG00030 protein_coding_gene ## 439.446 ATMG00030.1 ATMG00030.1 ATMG00030 ## 440.447 ATMG00030.1-Protein ATMG00030.1 ## 441.448 \u003cNA\u003e \u003cNA\u003e ATMG00030.1 ## 442.449 \u003cNA\u003e \u003cNA\u003e ATMG00030.1,ATMG00030.1-Protein ## Index Derives_from ## \u003ccharacter\u003e \u003ccharacter\u003e ## 1.2 1 \u003cNA\u003e ## 2.3 1 \u003cNA\u003e ## 3.4 \u003cNA\u003e AT1G01010.1 ## 4.5 \u003cNA\u003e \u003cNA\u003e ## 5.6 \u003cNA\u003e \u003cNA\u003e ## ... ... ... ## 438.445 \u003cNA\u003e \u003cNA\u003e ## 439.446 1 \u003cNA\u003e ## 440.447 \u003cNA\u003e ATMG00030.1 ## 441.448 \u003cNA\u003e \u003cNA\u003e ## 442.449 \u003cNA\u003e \u003cNA\u003e ## ------- ## seqinfo: 7 sequences from an unspecified genome  sp[1:4, \"type\"] # Subsetting of GRangesList objects is similar to GRanges objects.  ## GRangesList object of length 4: ## $`1` ## GRanges object with 1 range and 1 metadata column: ## seqnames ranges strand | type ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e ## 2 Chr1 3631-5899 * | mRNA ## ------- ## seqinfo: 7 sequences from an unspecified genome ## ## $`2` ## GRanges object with 1 range and 1 metadata column: ## seqnames ranges strand | type ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e ## 3 Chr1 3631-5899 * | mRNA ## ------- ## seqinfo: 7 sequences from an unspecified genome ## ## $`3` ## GRanges object with 1 range and 1 metadata column: ## seqnames ranges strand | type ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e ## 4 Chr1 3760-5630 * | protein ## ------- ## seqinfo: 7 sequences from an unspecified genome ## ## $`4` ## GRanges object with 1 range and 1 metadata column: ## seqnames ranges strand | type ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cfactor\u003e ## 5 Chr1 3631-3913 * | exon ## ------- ## seqinfo: 7 sequences from an unspecified genome  lapply(sp[1:4], length) # Looping over GRangesList objects similar to lists  ## $`1` ## [1] 1 ## ## $`2` ## [1] 1 ## ## $`3` ## [1] 1 ## ## $`4` ## [1] 1  Transcript Ranges Storing annotation ranges in TranscriptDb databases makes many operations more robust and convenient.\nlibrary(GenomicFeatures) download.file(\"http://cluster.hpcc.ucr.edu/~tgirke/Documents/R_BioCond/Samples/gff3.gff\", \"data/gff3.gff\") txdb \u003c- makeTxDbFromGFF(file=\"data/gff3.gff\", format=\"gff\", dataSource=\"TAIR\", organism=\"Arabidopsis thaliana\")  ## Warning in .extract_exons_from_GRanges(cds_IDX, gr, mcols0, tx_IDX, feature = \"cds\", : 163 CDS couldn't be linked to a transcript so were dropped (showing only the first 6): ## seqid start end strand ID Name Parent Parent_type ## 1 Chr1 3760 3913 + \u003cNA\u003e \u003cNA\u003e AT1G01010.1-Protein \u003cNA\u003e ## 2 Chr1 3996 4276 + \u003cNA\u003e \u003cNA\u003e AT1G01010.1-Protein \u003cNA\u003e ## 3 Chr1 4486 4605 + \u003cNA\u003e \u003cNA\u003e AT1G01010.1-Protein \u003cNA\u003e ## 4 Chr1 4706 5095 + \u003cNA\u003e \u003cNA\u003e AT1G01010.1-Protein \u003cNA\u003e ## 5 Chr1 5174 5326 + \u003cNA\u003e \u003cNA\u003e AT1G01010.1-Protein \u003cNA\u003e ## 6 Chr1 5439 5630 + \u003cNA\u003e \u003cNA\u003e AT1G01010.1-Protein \u003cNA\u003e  saveDb(txdb, file=\"./data/TAIR10.sqlite\")  ## TxDb object: ## # Db type: TxDb ## # Supporting package: GenomicFeatures ## # Data source: TAIR ## # Organism: Arabidopsis thaliana ## # Taxonomy ID: 3702 ## # miRBase build ID: NA ## # Genome: NA ## # Nb of transcripts: 28 ## # Db created by: GenomicFeatures package from Bioconductor ## # Creation time: 2022-04-20 18:50:11 -0700 (Wed, 20 Apr 2022) ## # GenomicFeatures version at creation time: 1.46.1 ## # RSQLite version at creation time: 2.2.9 ## # DBSCHEMAVERSION: 1.2  txdb \u003c- loadDb(\"./data/TAIR10.sqlite\") transcripts(txdb)  ## GRanges object with 28 ranges and 2 metadata columns: ## seqnames ranges strand | tx_id tx_name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cinteger\u003e \u003ccharacter\u003e ## [1] Chr1 3631-5899 + | 1 AT1G01010.1 ## [2] Chr1 5928-8737 - | 2 AT1G01020.1 ## [3] Chr1 6790-8737 - | 3 AT1G01020.2 ## [4] Chr1 11649-13714 - | 4 AT1G01030.1 ## [5] Chr2 1025-2810 + | 5 AT2G01008.1 ## ... ... ... ... . ... ... ## [24] ChrC 383-1444 - | 24 ATCG00020.1 ## [25] ChrC 1717-4347 - | 25 ATCG00030.1 ## [26] ChrM 11918-12241 + | 26 ATMG00030.1 ## [27] ChrM 273-734 - | 27 ATMG00010.1 ## [28] ChrM 8848-11415 - | 28 ATMG00020.1 ## ------- ## seqinfo: 7 sequences (2 circular) from an unspecified genome; no seqlengths  transcriptsBy(txdb, by = \"gene\")  ## GRangesList object of length 22: ## $AT1G01010 ## GRanges object with 1 range and 2 metadata columns: ## seqnames ranges strand | tx_id tx_name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cinteger\u003e \u003ccharacter\u003e ## [1] Chr1 3631-5899 + | 1 AT1G01010.1 ## ------- ## seqinfo: 7 sequences (2 circular) from an unspecified genome; no seqlengths ## ## $AT1G01020 ## GRanges object with 2 ranges and 2 metadata columns: ## seqnames ranges strand | tx_id tx_name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cinteger\u003e \u003ccharacter\u003e ## [1] Chr1 5928-8737 - | 2 AT1G01020.1 ## [2] Chr1 6790-8737 - | 3 AT1G01020.2 ## ------- ## seqinfo: 7 sequences (2 circular) from an unspecified genome; no seqlengths ## ## $AT1G01030 ## GRanges object with 1 range and 2 metadata columns: ## seqnames ranges strand | tx_id tx_name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cinteger\u003e \u003ccharacter\u003e ## [1] Chr1 11649-13714 - | 4 AT1G01030.1 ## ------- ## seqinfo: 7 sequences (2 circular) from an unspecified genome; no seqlengths ## ## ... ## \u003c19 more elements\u003e  exonsBy(txdb, by = \"gene\")  ## GRangesList object of length 22: ## $AT1G01010 ## GRanges object with 6 ranges and 2 metadata columns: ## seqnames ranges strand | exon_id exon_name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cinteger\u003e \u003ccharacter\u003e ## [1] Chr1 3631-3913 + | 1 \u003cNA\u003e ## [2] Chr1 3996-4276 + | 2 \u003cNA\u003e ## [3] Chr1 4486-4605 + | 3 \u003cNA\u003e ## [4] Chr1 4706-5095 + | 4 \u003cNA\u003e ## [5] Chr1 5174-5326 + | 5 \u003cNA\u003e ## [6] Chr1 5439-5899 + | 6 \u003cNA\u003e ## ------- ## seqinfo: 7 sequences (2 circular) from an unspecified genome; no seqlengths ## ## $AT1G01020 ## GRanges object with 12 ranges and 2 metadata columns: ## seqnames ranges strand | exon_id exon_name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cinteger\u003e \u003ccharacter\u003e ## [1] Chr1 5928-6263 - | 7 \u003cNA\u003e ## [2] Chr1 6437-7069 - | 8 \u003cNA\u003e ## [3] Chr1 6790-7069 - | 9 \u003cNA\u003e ## [4] Chr1 7157-7232 - | 10 \u003cNA\u003e ## [5] Chr1 7157-7450 - | 11 \u003cNA\u003e ## ... ... ... ... . ... ... ## [8] Chr1 7762-7835 - | 14 \u003cNA\u003e ## [9] Chr1 7942-7987 - | 15 \u003cNA\u003e ## [10] Chr1 8236-8325 - | 16 \u003cNA\u003e ## [11] Chr1 8417-8464 - | 17 \u003cNA\u003e ## [12] Chr1 8571-8737 - | 18 \u003cNA\u003e ## ------- ## seqinfo: 7 sequences (2 circular) from an unspecified genome; no seqlengths ## ## $AT1G01030 ## GRanges object with 2 ranges and 2 metadata columns: ## seqnames ranges strand | exon_id exon_name ## \u003cRle\u003e \u003cIRanges\u003e \u003cRle\u003e | \u003cinteger\u003e \u003ccharacter\u003e ## [1] Chr1 11649-13173 - | 19 \u003cNA\u003e ## [2] Chr1 13335-13714 - | 20 \u003cNA\u003e ## ------- ## seqinfo: 7 sequences (2 circular) from an unspecified genome; no seqlengths ## ## ... ## \u003c19 more elements\u003e  txdb from BioMart Alternative sources for creating txdb databases are BioMart, Bioc annotation packages, UCSC, etc. The following shows how to create a txdb from BioMart.\nlibrary(GenomicFeatures); library(\"biomaRt\") txdb \u003c- makeTxDbFromBiomart(biomart = \"plants_mart\", dataset = \"athaliana_eg_gene\", host=\"plants.ensembl.org\")  The following steps are useful to find out what is availble in BioMart.\nlistMarts() # Lists BioMart databases listMarts(host=\"plants.ensembl.org\") mymart \u003c- useMart(\"plants_mart\", host=\"plants.ensembl.org\") # Select one, here plants_mart listDatasets(mymart) # List datasets available in the selected BioMart database mymart \u003c- useMart(\"plants_mart\", dataset=\"athaliana_eg_gene\", host=\"plants.ensembl.org\") listAttributes(mymart) # List available features getBM(attributes=c(\"ensembl_gene_id\", \"description\"), mart=mymart)[1:4,]  Efficient Sequence Parsing getSeq The following parses all annotation ranges provided by a GRanges object (e.g. gff) from a genome sequence stored in a local file.\ngff \u003c- gff[values(gff)$type != \"chromosome\"] # Remove chromosome ranges rand \u003c- DNAStringSet(sapply(unique(as.character(seqnames(gff))), function(x) paste(sample(c(\"A\",\"T\",\"G\",\"C\"), 200000, replace=T), collapse=\"\"))) writeXStringSet(DNAStringSet(rand), \"./data/test\") getSeq(FaFile(\"./data/test\"), gff)  ## DNAStringSet object of length 442: ## width seq names ## [1] 2269 GCCAGGTAGAATTCCAAAAATTGAAAGGCGTT...TGATCTGCCAGCGCCATTCGCTACGTCCGAAC Chr1 ## [2] 2269 GCCAGGTAGAATTCCAAAAATTGAAAGGCGTT...TGATCTGCCAGCGCCATTCGCTACGTCCGAAC Chr1 ## [3] 1871 CCTCCAAGTCTTAAAGCCTGCCTGCGACATTC...AAACCGCGAAATCTTATCGACCATTGTGTCTC Chr1 ## [4] 283 GCCAGGTAGAATTCCAAAAATTGAAAGGCGTT...GCGGTCACTTTGCTAGGGCTGGTAGCCATGCA Chr1 ## [5] 129 GCCAGGTAGAATTCCAAAAATTGAAAGGCGTT...TATCACACCTCTCTCTACTCGAATGACACGGC Chr1 ## ... ... ... ## [438] 324 CTGGGTTGGAATGGGGCAAATACTTTCAAGGA...CAGGTGCAAATTGGCATCGCCGCACGGACGTT ChrM ## [439] 324 CTGGGTTGGAATGGGGCAAATACTTTCAAGGA...CAGGTGCAAATTGGCATCGCCGCACGGACGTT ChrM ## [440] 324 CTGGGTTGGAATGGGGCAAATACTTTCAAGGA...CAGGTGCAAATTGGCATCGCCGCACGGACGTT ChrM ## [441] 324 CTGGGTTGGAATGGGGCAAATACTTTCAAGGA...CAGGTGCAAATTGGCATCGCCGCACGGACGTT ChrM ## [442] 324 CTGGGTTGGAATGGGGCAAATACTTTCAAGGA...CAGGTGCAAATTGGCATCGCCGCACGGACGTT ChrM  extractTranscriptSeqs Sequences composed of several ranges, such as transcripts (or CDSs) with several exons, can be parsed with extractTranscriptSeqs. Note: the following expects the genome sequence in a file with path data/test and a valid txdb defining the ranges for that genome.\nlibrary(GenomicFeatures); library(Biostrings); library(Rsamtools) txdb \u003c- loadDb(\"./data/TAIR10.sqlite\") indexFa(\"data/test\") # Creates index for genome fasta  ## [1] \"data/test.fai\"  txseq \u003c- extractTranscriptSeqs(FaFile(\"data/test\"), txdb, use.names=TRUE) txseq  ## DNAStringSet object of length 28: ## width seq names ## [1] 1688 GCCAGGTAGAATTCCAAAAATTGAAAGGCGTTT...TGATCTGCCAGCGCCATTCGCTACGTCCGAAC AT1G01010.1 ## [2] 1623 CGCTTGTCCGTTCAAGAAAAGAATCTATTCCCC...CCACGCGTAGATTAGTTGGGGACGTAAGGACC AT1G01020.1 ## [3] 1085 CGCTTGTCCGTTCAAGAAAAGAATCTATTCCCC...GAGCAGGTTCGTTTTTAGAGACTCTATACCGT AT1G01020.2 ## [4] 1905 GTCCAGCCTACCCTATCGATACCTGTCACCATT...CAAAATCCGGCAGCATTTATCATGATCACGTA AT1G01030.1 ## [5] 1239 CGCGGGAGAAACATTCGTCGAGGAATGATGCTG...ATGGTTTGCTCGACGAAGCTTATTCTGTCCGC AT2G01008.1 ## ... ... ... ## [24] 1062 GCCGTCCACAATGACGGTGTGGATCTGAGAACG...TGGGTATCAGCCTACCGGCTGTAGGGAAATAG ATCG00020.1 ## [25] 72 CCCGAGACTAGCCGCACCACGTGAGCTTTGGTG...AAACATCCTGTGCACGTTTTTTTGCCGGCGAT ATCG00030.1 ## [26] 324 CTGGGTTGGAATGGGGCAAATACTTTCAAGGAG...CAGGTGCAAATTGGCATCGCCGCACGGACGTT ATMG00030.1 ## [27] 462 GGATCTGACGCCACCAGAACCACAACTCCTGGA...GTGCTATGATGGCGTATGAGCGGGGGGTCGTA ATMG00010.1 ## [28] 2568 CCGCTGATAGAGGTCAAATTACCAACAATAGTG...TGATAAATGGGATGCTTTAAGGACTCGTCGGC ATMG00020.1  Homework 6 See here.\nSession Info sessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid stats4 stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] GenomicFeatures_1.46.1 AnnotationDbi_1.56.2 rtracklayer_1.54.0 ## [4] systemPipeR_2.0.4 ShortRead_1.52.0 GenomicAlignments_1.30.0 ## [7] SummarizedExperiment_1.24.0 Biobase_2.54.0 MatrixGenerics_1.6.0 ## [10] matrixStats_0.61.0 Rsamtools_2.10.0 GenomicRanges_1.46.1 ## [13] BiocParallel_1.28.2 ggseqlogo_0.1 ggplot2_3.3.5 ## [16] seqLogo_1.60.0 Biostrings_2.62.0 GenomeInfoDb_1.30.0 ## [19] XVector_0.34.0 IRanges_2.28.0 S4Vectors_0.32.3 ## [22] BiocGenerics_0.40.0 BiocStyle_2.22.0 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-7 bit64_4.0.5 filelock_1.0.2 progress_1.2.2 ## [5] RColorBrewer_1.1-2 httr_1.4.2 tools_4.1.3 bslib_0.3.1 ## [9] utf8_1.2.2 R6_2.5.1 DBI_1.1.1 colorspace_2.0-2 ## [13] withr_2.4.3 prettyunits_1.1.1 tidyselect_1.1.1 curl_4.3.2 ## [17] bit_4.0.4 compiler_4.1.3 cli_3.1.0 xml2_1.3.3 ## [21] DelayedArray_0.20.0 labeling_0.4.2 bookdown_0.24 sass_0.4.0 ## [25] scales_1.1.1 rappdirs_0.3.3 stringr_1.4.0 digest_0.6.29 ## [29] rmarkdown_2.13 jpeg_0.1-9 pkgconfig_2.0.3 htmltools_0.5.2 ## [33] highr_0.9 dbplyr_2.1.1 fastmap_1.1.0 htmlwidgets_1.5.4 ## [37] rlang_1.0.2 RSQLite_2.2.9 farver_2.1.0 jquerylib_0.1.4 ## [41] BiocIO_1.4.0 generics_0.1.1 hwriter_1.3.2 jsonlite_1.8.0 ## [45] dplyr_1.0.7 RCurl_1.98-1.5 magrittr_2.0.2 GenomeInfoDbData_1.2.7 ## [49] Matrix_1.4-0 Rcpp_1.0.8.2 munsell_0.5.0 fansi_0.5.0 ## [53] lifecycle_1.0.1 stringi_1.7.6 yaml_2.3.5 zlibbioc_1.40.0 ## [57] BiocFileCache_2.2.0 blob_1.2.2 parallel_4.1.3 crayon_1.4.2 ## [61] lattice_0.20-45 hms_1.1.1 KEGGREST_1.34.0 knitr_1.37 ## [65] pillar_1.6.4 rjson_0.2.20 codetools_0.2-18 biomaRt_2.50.1 ## [69] XML_3.99-0.8 glue_1.6.2 evaluate_0.15 blogdown_1.8.2 ## [73] latticeExtra_0.6-29 BiocManager_1.30.16 png_0.1-7 vctrs_0.3.8 ## [77] gtable_0.3.0 purrr_0.3.4 assertthat_0.2.1 cachem_1.0.6 ## [81] xfun_0.30 restfulr_0.0.13 viridisLite_0.4.0 tibble_3.1.6 ## [85] memoise_2.0.1 ellipsis_0.3.2  References Huber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nat. Methods 12 (2): 115–21. https://doi.org/10.1038/nmeth.3252.\n Lawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Comput. Biol. 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n  ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/rsequences/rsequences/","tags":"","title":"NGS Analysis Basics"},{"body":"                    pre code { white-space: pre !important; overflow-x: scroll !important; word-break: keep-all !important; word-wrap: initial !important; }  document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code download: [ .Rmd ] [ .R ]\n Introduction systemPipeR provides flexible utilities for designing, building and running automated end-to-end analysis workflows for a wide range of research applications, including next-generation sequencing (NGS) experiments, such as RNA-Seq, ChIP-Seq, VAR-Seq and Ribo-Seq (H Backman and Girke 2016). Important features include a uniform workflow interface across different data analysis applications, automated report generation, and support for running both R and command-line software, such as NGS aligners or peak/variant callers, on local computers or compute clusters (Figure 1). The latter supports interactive job submissions and batch submissions to queuing systems of clusters.\nsystemPipeR has been designed to improve the reproducibility of large-scale data analysis projects while substantially reducing the time it takes to analyze complex omics data sets. It provides a uniform workflow interface and management system that allows the users to run selected workflow steps, as well as customize and design entirely new workflows. Additionally, the package take advantage of central community S4 classes of the Bioconductor ecosystem, and enhances them with command-line software support.\nThe main motivation and advantages of using systemPipeR for complex data analysis tasks are:\n Design of complex workflows involving multiple R/Bioconductor packages Common workflow interface for different applications User-friendly access to widely used Bioconductor utilities Support of command-line software from within R Reduced complexity of using compute clusters from R Accelerated runtime of workflows via parallelization on computer systems with multiple CPU cores and/or multiple nodes Improved reproducibility by automating the generation of analysis reports    Figure 1: Relevant features in systemPipeR. Workflow design concepts are illustrated under (A). Examples of systemPipeR's visualization functionalities are given under (B). \nA central concept for designing workflows within the systemPipeR environment is the use of workflow management containers. Workflow management containers facilitate the design and execution of complex data analysis steps. For its command-line interface systemPipeR adopts the widely used Common Workflow Language (CWL) (Amstutz et al. 2016). The interface to CWL is established by systemPipeR's workflow control class called SYSargsList (see Figure 2). This design offers many advantages such as: (i) options to run workflows either entirely from within R, from various command-line wrappers (e.g., cwl-runner) or from other languages (e.g., Bash or Python). Apart from providing support for both command-line and R/Bioconductor software, the package provides utilities for containerization, parallel evaluations on computer clusters and automated generation of interactive analysis reports.\n  Figure 2: Overview of systemPipeR workflows management instances. (A) A typical analysis workflow requires multiple software tools (red), as well the description of the input (green) and output files, including analysis reports (purple). (B) systemPipeR provides multiple utilities to design and build a workflow, allowing multi-instance, integration of R code and command-line software, a simple and efficient annotation system, that allows automatic control of the input and output data, and multiple resources to manage the entire workflow. (C) Options are provided to execute single or multiple workflow steps, while enabling scalability, checkpoints, and generation of technical and scientific reports.\nAn important feature of systemPipeR's CWL interface is that it provides two options to run command-line tools and workflows based on CWL. First, one can run CWL in its native way via an R-based wrapper utility for cwl-runner or cwl-tools (CWL-based approach). Second, one can run workflows using CWL’s command-line and workflow instructions from within R (R-based approach). In the latter case the same CWL workflow definition files (e.g. *.cwl and *.yml) are used but rendered and executed entirely with R functions defined by systemPipeR, and thus use CWL mainly as a command-line and workflow definition format rather than execution software to run workflows. In this regard systemPipeR also provides several convenience functions that are useful for designing and debugging workflows, such as a command-line rendering function to retrieve the exact command-line strings for each data set and processing step prior to running a command-line.\nWorkflow Management with SYSargsList The SYSargsList S4 class is a list-like container that stores the paths to all input and output files along with the corresponding parameters used in each analysis step (see Figure 3). SYSargsList instances are constructed from an optional targets files, and two CWL parameter files including *.cwl and *.yml (for details, see below). When running preconfigured NGS workflows, the only input the user needs to provide is the initial targets file containing the paths to the input files (e.g., FASTQ) and experiment design information, such as sample labels and biological replicates. Subsequent targets instances are created automatically, based on the connectivity establish between each workflow step. SYSargsList containers store all information required for one or multiple steps. This establishes central control for running, monitoring and debugging complex workflows from start to finish.\n  Figure 3: Workflow steps with input/output file operations are controlled by the SYSargsList container. Each of its components (SYSargs2) are constructed from an optional targets and two param files. Alternatively, LineWise instances containing pure R code can be used.\nGetting Started Installation The R software for running systemPipeR can be downloaded from CRAN. The systemPipeR environment can be installed from the R console using the BiocManager::install command. The associated data package systemPipeRdata can be installed the same way. The latter is a helper package for generating systemPipeR workflow environments with a single command containing all parameter files and sample data required to quickly test and run workflows.\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"systemPipeR\") BiocManager::install(\"systemPipeRdata\")  To use command-line software, the corresponding tool and dependencies need to be installed on a user’s system. See details.\nLoading package and documentation library(\"systemPipeR\") # Loads the package library(help = \"systemPipeR\") # Lists package info vignette(\"systemPipeR\") # Opens vignette  Load sample data and workflow templates The mini sample FASTQ files used by this introduction as well as the associated workflow reporting vignettes can be loaded via the systemPipeRdata package as shown below. The chosen data set SRP010938 contains 18 paired-end (PE) read sets from Arabidposis thaliana (Howard et al. 2013). To minimize processing time during testing, each FASTQ file has been subsetted to 90,000-100,000 randomly sampled PE reads that map to the first 100,000 nucleotides of each chromosome of the A. thalina genome. The corresponding reference genome sequence (FASTA) and its GFF annotation files (provided in the same download) have been truncated accordingly. This way the entire test sample data set requires less than 200MB disk storage space. A PE read set has been chosen for this test data set for flexibility, because it can be used for testing both types of analysis routines requiring either SE (single-end) reads or PE reads.\nThe following generates a fully populated systemPipeR workflow environment (here for RNA-Seq) in the current working directory of an R session. The systemPipeRdata package provides preconfigured workflow templates for RNA-Seq, ChIP-Seq, VAR-Seq, and Ribo-Seq. Additional, templates are available on the project web site here.\nsystemPipeRdata::genWorkenvir(workflow = \"rnaseq\") setwd(\"rnaseq\")  Project structure The working environment of the sample data loaded in the previous step contains the following pre-configured directory structure (Figure 4). Directory names are indicated in green. Users can change this structure as needed, but need to adjust the code in their workflows accordingly.\n workflow/ (e.g. rnaseq/)  This is the root directory of the R session running the workflow. Run script ( *.Rmd) and sample annotation (targets.txt) files are located here. Note, this directory can have any name (e.g. rnaseq, varseq). Changing its name does not require any modifications in the run script(s). Important subdirectories:  param/  param/cwl/: This subdirectory stores all the CWL parameter files. To organize workflows, each can have its own subdirectory, where all CWL param and input.yml files need to be in the same subdirectory.   data/   FASTQ files FASTA file of reference (e.g. reference genome) Annotation files etc.   results/  Analysis results are usually written to this directory, including: alignment, variant and peak files (BAM, VCF, BED); tabular result files; and image/plot files. Note, the user has the option to organize results files for a given sample and analysis step in a separate subdirectory.          Figure 4: systemPipeR’s preconfigured directory structure.\nThe following parameter files are included in each workflow template:\n targets.txt: initial one provided by user; downstream targets_*.txt files are generated automatically *.param/cwl: defines parameter for input/output file operations, e.g.:  hisat2/hisat2-mapping-se.cwl hisat2/hisat2-mapping-se.yml   Configuration files for computer cluster environments (skip on single machines):  .batchtools.conf.R: defines the type of scheduler for batchtools pointing to template file of cluster, and located in user’s home directory *.tmpl: specifies parameters of scheduler used by a system, e.g. Torque, SGE, Slurm, etc.    Structure of targets file The targets file defines all input files (e.g. FASTQ, BAM, BCF) and sample comparisons of an analysis workflow. The following shows the format of a sample targets file included in the package. It also can be viewed and downloaded from systemPipeR’s GitHub repository here. In a target file with a single type of input files, here FASTQ files of single-end (SE) reads, the first three columns are mandatory including their column names, while it is four mandatory columns for FASTQ files of PE reads. All subsequent columns are optional and any number of additional columns can be added as needed. The columns in targets files are expected to be tab separated (TSV format). The SampleName column contains usually short labels for referencing samples (here FASTQ files) across many workflow steps (e.g. plots and column titles). Importantly, the labels used in the SampleName column need to be unique, while technical or biological replicates are indicated by duplicated values under the Factor column. For readability and transparency, it is useful to use here a short, consistent and informative syntax for naming samples and replicates. To avoid problems with other packages or external software, it is recommended to use the basic naming rules for R objects and their components as outlined here. This is important since the values used under the SampleName and Factor columns are intended to be used as labels for naming columns or plotting features in downstream analysis steps.\nUsers should note here, the usage of targets files is optional when using systemPipeR’s new CWL interface. They can be replaced by a standard YAML input file used by CWL. Since for organizing experimental variables targets files are extremely useful and user-friendly. Thus, we encourage users to keep using them.\nStructure of targets file for single-end (SE) samples library(systemPipeR) targetspath \u003c- system.file(\"extdata\", \"targets.txt\", package = \"systemPipeR\") showDF(read.delim(targetspath, comment.char = \"#\"))   {\"x\":{\"filter\":\"none\",\"vertical\":false,\"extensions\":[\"FixedColumns\",\"Scroller\"],\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\"],[\"./data/SRR446027_1.fastq.gz\",\"./data/SRR446028_1.fastq.gz\",\"./data/SRR446029_1.fastq.gz\",\"./data/SRR446030_1.fastq.gz\",\"./data/SRR446031_1.fastq.gz\",\"./data/SRR446032_1.fastq.gz\",\"./data/SRR446033_1.fastq.gz\",\"./data/SRR446034_1.fastq.gz\",\"./data/SRR446035_1.fastq.gz\",\"./data/SRR446036_1.fastq.gz\",\"./data/SRR446037_1.fastq.gz\",\"./data/SRR446038_1.fastq.gz\",\"./data/SRR446039_1.fastq.gz\",\"./data/SRR446040_1.fastq.gz\",\"./data/SRR446041_1.fastq.gz\",\"./data/SRR446042_1.fastq.gz\",\"./data/SRR446043_1.fastq.gz\",\"./data/SRR446044_1.fastq.gz\"],[\"M1A\",\"M1B\",\"A1A\",\"A1B\",\"V1A\",\"V1B\",\"M6A\",\"M6B\",\"A6A\",\"A6B\",\"V6A\",\"V6B\",\"M12A\",\"M12B\",\"A12A\",\"A12B\",\"V12A\",\"V12B\"],[\"M1\",\"M1\",\"A1\",\"A1\",\"V1\",\"V1\",\"M6\",\"M6\",\"A6\",\"A6\",\"V6\",\"V6\",\"M12\",\"M12\",\"A12\",\"A12\",\"V12\",\"V12\"],[\"Mock.1h.A\",\"Mock.1h.B\",\"Avr.1h.A\",\"Avr.1h.B\",\"Vir.1h.A\",\"Vir.1h.B\",\"Mock.6h.A\",\"Mock.6h.B\",\"Avr.6h.A\",\"Avr.6h.B\",\"Vir.6h.A\",\"Vir.6h.B\",\"Mock.12h.A\",\"Mock.12h.B\",\"Avr.12h.A\",\"Avr.12h.B\",\"Vir.12h.A\",\"Vir.12h.B\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\"]],\"container\":\"\\n \\n \\n  \\n FileName\\n SampleName\\n Factor\\n SampleLong\\n Experiment\\n Date\\n \\n \\n\",\"options\":{\"scrollX\":true,\"fixedColumns\":true,\"deferRender\":true,\"scrollY\":200,\"scroller\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":5},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} To work with custom data, users need to generate a targets file containing the paths to their own FASTQ files and then provide under targetspath the path to the corresponding targets file.\nStructure of targets file for paired-end (PE) samples For paired-end (PE) samples, the structure of the targets file is similar, where users need to provide two FASTQ path columns: FileName1 and FileName2 with the paths to the PE FASTQ files.\ntargetspath \u003c- system.file(\"extdata\", \"targetsPE.txt\", package = \"systemPipeR\") showDF(read.delim(targetspath, comment.char = \"#\"))   {\"x\":{\"filter\":\"none\",\"vertical\":false,\"extensions\":[\"FixedColumns\",\"Scroller\"],\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\"],[\"./data/SRR446027_1.fastq.gz\",\"./data/SRR446028_1.fastq.gz\",\"./data/SRR446029_1.fastq.gz\",\"./data/SRR446030_1.fastq.gz\",\"./data/SRR446031_1.fastq.gz\",\"./data/SRR446032_1.fastq.gz\",\"./data/SRR446033_1.fastq.gz\",\"./data/SRR446034_1.fastq.gz\",\"./data/SRR446035_1.fastq.gz\",\"./data/SRR446036_1.fastq.gz\",\"./data/SRR446037_1.fastq.gz\",\"./data/SRR446038_1.fastq.gz\",\"./data/SRR446039_1.fastq.gz\",\"./data/SRR446040_1.fastq.gz\",\"./data/SRR446041_1.fastq.gz\",\"./data/SRR446042_1.fastq.gz\",\"./data/SRR446043_1.fastq.gz\",\"./data/SRR446044_1.fastq.gz\"],[\"./data/SRR446027_2.fastq.gz\",\"./data/SRR446028_2.fastq.gz\",\"./data/SRR446029_2.fastq.gz\",\"./data/SRR446030_2.fastq.gz\",\"./data/SRR446031_2.fastq.gz\",\"./data/SRR446032_2.fastq.gz\",\"./data/SRR446033_2.fastq.gz\",\"./data/SRR446034_2.fastq.gz\",\"./data/SRR446035_2.fastq.gz\",\"./data/SRR446036_2.fastq.gz\",\"./data/SRR446037_2.fastq.gz\",\"./data/SRR446038_2.fastq.gz\",\"./data/SRR446039_2.fastq.gz\",\"./data/SRR446040_2.fastq.gz\",\"./data/SRR446041_2.fastq.gz\",\"./data/SRR446042_2.fastq.gz\",\"./data/SRR446043_2.fastq.gz\",\"./data/SRR446044_2.fastq.gz\"],[\"M1A\",\"M1B\",\"A1A\",\"A1B\",\"V1A\",\"V1B\",\"M6A\",\"M6B\",\"A6A\",\"A6B\",\"V6A\",\"V6B\",\"M12A\",\"M12B\",\"A12A\",\"A12B\",\"V12A\",\"V12B\"],[\"M1\",\"M1\",\"A1\",\"A1\",\"V1\",\"V1\",\"M6\",\"M6\",\"A6\",\"A6\",\"V6\",\"V6\",\"M12\",\"M12\",\"A12\",\"A12\",\"V12\",\"V12\"],[\"Mock.1h.A\",\"Mock.1h.B\",\"Avr.1h.A\",\"Avr.1h.B\",\"Vir.1h.A\",\"Vir.1h.B\",\"Mock.6h.A\",\"Mock.6h.B\",\"Avr.6h.A\",\"Avr.6h.B\",\"Vir.6h.A\",\"Vir.6h.B\",\"Mock.12h.A\",\"Mock.12h.B\",\"Avr.12h.A\",\"Avr.12h.B\",\"Vir.12h.A\",\"Vir.12h.B\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\"]],\"container\":\"\\n \\n \\n  \\n FileName1\\n FileName2\\n SampleName\\n Factor\\n SampleLong\\n Experiment\\n Date\\n \\n \\n\",\"options\":{\"scrollX\":true,\"fixedColumns\":true,\"deferRender\":true,\"scrollY\":200,\"scroller\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":6},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Sample comparisons Sample comparisons are defined in the header lines of the targets file starting with ‘# \u003cCMP\u003e’.\nreadLines(targetspath)[1:4]  ## [1] \"# Project ID: Arabidopsis - Pseudomonas alternative splicing study (SRA: SRP010938; PMID: 24098335)\" ## [2] \"# The following line(s) allow to specify the contrasts needed for comparative analyses, such as DEG identification. All possible comparisons can be specified with 'CMPset: ALL'.\" ## [3] \"# \u003cCMP\u003e CMPset1: M1-A1, M1-V1, A1-V1, M6-A6, M6-V6, A6-V6, M12-A12, M12-V12, A12-V12\" ## [4] \"# \u003cCMP\u003e CMPset2: ALL\"  The function readComp imports the comparison information and stores it in a list. Alternatively, readComp can obtain the comparison information from the corresponding SYSargsList object (see below). Note, these header lines are optional. They are mainly useful for controlling comparative analyses according to certain biological expectations, such as identifying differentially expressed genes in RNA-Seq experiments based on simple pair-wise comparisons.\nreadComp(file = targetspath, format = \"vector\", delim = \"-\")  ## $CMPset1 ## [1] \"M1-A1\" \"M1-V1\" \"A1-V1\" \"M6-A6\" \"M6-V6\" \"A6-V6\" \"M12-A12\" ## [8] \"M12-V12\" \"A12-V12\" ## ## $CMPset2 ## [1] \"M1-A1\" \"M1-V1\" \"M1-M6\" \"M1-A6\" \"M1-V6\" \"M1-M12\" \"M1-A12\" ## [8] \"M1-V12\" \"A1-V1\" \"A1-M6\" \"A1-A6\" \"A1-V6\" \"A1-M12\" \"A1-A12\" ## [15] \"A1-V12\" \"V1-M6\" \"V1-A6\" \"V1-V6\" \"V1-M12\" \"V1-A12\" \"V1-V12\" ## [22] \"M6-A6\" \"M6-V6\" \"M6-M12\" \"M6-A12\" \"M6-V12\" \"A6-V6\" \"A6-M12\" ## [29] \"A6-A12\" \"A6-V12\" \"V6-M12\" \"V6-A12\" \"V6-V12\" \"M12-A12\" \"M12-V12\" ## [36] \"A12-V12\"  Project initialization To create a workflow within systemPipeR, we can start by defining an empty container and checking the directory structure:\nsal \u003c- SPRproject()  ## Creating directory '/home/tgirke/tmp/GEN242/content/en/tutorials/systempiper/rnaseq/.SPRproject' ## Creating file '/home/tgirke/tmp/GEN242/content/en/tutorials/systempiper/rnaseq/.SPRproject/SYSargsList.yml'  Internally, SPRproject function will create a hidden directory called .SPRproject, by default. This directory will store all log files generated during a workflow run.\nInitially, the object sal is a empty container containing only the basic project information. The project information can be accessed with the projectInfo method.\nsal  ## Instance of 'SYSargsList': ## No workflow steps added  projectInfo(sal)  ## $project ## [1] \"/home/tgirke/tmp/GEN242/content/en/tutorials/systempiper/rnaseq\" ## ## $data ## [1] \"data\" ## ## $param ## [1] \"param\" ## ## $results ## [1] \"results\" ## ## $logsDir ## [1] \".SPRproject\" ## ## $sysargslist ## [1] \".SPRproject/SYSargsList.yml\"  Also, the length function will return how many steps a workflow contains. Since no steps have been added yet it returns zero.\nlength(sal)  ## [1] 0  Building workflows systemPipeR workflows can be populated with a single command from an R Markdown file or stepwise in interactive mode. This section introduces first how to build a workflow stepwise in interactive mode, and then how to build a workflow with a single command from an R Markdown file.\nStepwise workflow construction New workflows are constructed, or existing ones modified, by connecting each step via the appendStep method. Each step in a SYSargsList instance contains the instructions needed for processing a set of input files with a specific command-line and the paths to the exptected outfiles. For constructing R code-based workflow steps the constructor function Linewise is used.\nThe following demonstrates how to create a command-line workflow step here using as example the short read aligner software HISAT2 (Kim, Langmead, and Salzberg 2015).\nThe constructor function renders the proper command-line strings for each sample and software tool, appending a new step in the SYSargsList object defined in the previous step. For this, the SYSargsList constructor function uses in this example data from three input files:\n CWL command-line specification file (wf_file argument) Input variables (input_file argument) Targets file (targets argument)  In CWL files with the extension .cwl define the parameters of a chosen command-line step or an entire workflow, while files with the extension .yml define the input variables of command-line steps. Note, input variables provided by a targets or targetsPE file can be passed on via the inputvars argument to the .yml file and from there to the SYSargsList object.\nappendStep(sal) \u003c- SYSargsList(step_name = \"hisat2_mapping\", dir = TRUE, targets = \"targetsPE.txt\", wf_file = \"workflow-hisat2/workflow_hisat2-pe.cwl\", input_file = \"workflow-hisat2/workflow_hisat2-pe.yml\", dir_path = \"param/cwl\", inputvars = c(FileName1 = \"_FASTQ_PATH1_\", FileName2 = \"_FASTQ_PATH2_\", SampleName = \"_SampleName_\"))  An overview of the workflow can be printed as follows.\nsal  ## Instance of 'SYSargsList': ## WF Steps: ## 1. hisat2_mapping --\u003e Status: Pending ## Total Files: 72 | Existing: 0 | Missing: 72 ## 1.1. hisat2 ## cmdlist: 18 | Pending: 18 ## 1.2. samtools-view ## cmdlist: 18 | Pending: 18 ## 1.3. samtools-sort ## cmdlist: 18 | Pending: 18 ## 1.4. samtools-index ## cmdlist: 18 | Pending: 18 ##  In the above the workflow status for the new step is Pending. This means the workflow object has been constructed but not executed yet.\nSeveral accessor methods are available to explore the SYSargsList object. Of particular interest is the cmdlist() method. It constructs the system commands for running command-line software as specified by a given .cwl file combined with the paths to the input samples (e.g. FASTQ files), here provided by a targets file. The example below shows the cmdlist() output for running HISAT2 on the first PE read sample. Evaluating the output of cmdlist() can be very helpful for designing and debugging .cwl files of new command-line software or changing the parameter settings of existing ones. The rendered command-line instance for each input sample can be returned as follows.\ncmdlist(sal, step = \"hisat2_mapping\", targets = 1)  ## $hisat2_mapping ## $hisat2_mapping$M1A ## $hisat2_mapping$M1A$hisat2 ## [1] \"hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 --min-intronlen 30 --max-intronlen 3000 -1 ./data/SRR446027_1.fastq.gz -2 ./data/SRR446027_2.fastq.gz --threads 4\" ## ## $hisat2_mapping$M1A$`samtools-view` ## [1] \"samtools view -bS -o ./results/M1A.bam ./results/M1A.sam \" ## ## $hisat2_mapping$M1A$`samtools-sort` ## [1] \"samtools sort -o ./results/M1A.sorted.bam ./results/M1A.bam -@ 4\" ## ## $hisat2_mapping$M1A$`samtools-index` ## [1] \"samtools index -b results/M1A.sorted.bam results/M1A.sorted.bam.bai ./results/M1A.sorted.bam \"  The outfiles components of SYSargsList define the expected output files for each step in the workflow; some of which are the input for the next workflow step (see Figure 3).\noutfiles(sal)  ## $hisat2_mapping ## DataFrame with 18 rows and 4 columns ## hisat2_sam samtools_bam samtools_sort_bam ## \u003ccharacter\u003e \u003ccharacter\u003e \u003ccharacter\u003e ## M1A ./results/M1A.sam ./results/M1A.bam ./results/M1A.sorted.. ## M1B ./results/M1B.sam ./results/M1B.bam ./results/M1B.sorted.. ## A1A ./results/A1A.sam ./results/A1A.bam ./results/A1A.sorted.. ## A1B ./results/A1B.sam ./results/A1B.bam ./results/A1B.sorted.. ## V1A ./results/V1A.sam ./results/V1A.bam ./results/V1A.sorted.. ## ... ... ... ... ## M12B ./results/M12B.sam ./results/M12B.bam ./results/M12B.sorte.. ## A12A ./results/A12A.sam ./results/A12A.bam ./results/A12A.sorte.. ## A12B ./results/A12B.sam ./results/A12B.bam ./results/A12B.sorte.. ## V12A ./results/V12A.sam ./results/V12A.bam ./results/V12A.sorte.. ## V12B ./results/V12B.sam ./results/V12B.bam ./results/V12B.sorte.. ## samtools_index ## \u003ccharacter\u003e ## M1A ./results/M1A.sorted.. ## M1B ./results/M1B.sorted.. ## A1A ./results/A1A.sorted.. ## A1B ./results/A1B.sorted.. ## V1A ./results/V1A.sorted.. ## ... ... ## M12B ./results/M12B.sorte.. ## A12A ./results/A12A.sorte.. ## A12B ./results/A12B.sorte.. ## V12A ./results/V12A.sorte.. ## V12B ./results/V12B.sorte..  In an ‘R-centric’ rather than a ‘CWL-centric’ workflow design the connectivity among workflow steps is established by creating the downstream targets instances automatically (see Figure 3). Each step uses the outfiles from the previous step as input, thereby establishing connectivity among each step in the workflow. By chaining several SYSargsList steps together one can construct complex workflows involving many sample-level input/output file operations with any combination of command-line or R-based software. Also, systemPipeR provides features to automatically build these connections. This provides additional security to make sure that all samples will be processed in a reproducible manner.\nAlternatively, a CWL-centric workflow design can be used that defines all or most workflow steps with CWL workflow and parameter files. Due to time and space restrictions, the CWL-centric approach is not covered by this tutorial.\nThe following R code step generates a data.frame containing important read alignment statistics such as the total number of reads in the FASTQ files, the number of total alignments, as well as the number of primary alignments in the corresponding BAM files. The constructor function LineWise requires the step_name and the R code assigned to the code argument. The R code should be enclosed by braces ({}).\nappendStep(sal) \u003c- LineWise(code = { fqpaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"targetsWF\", column = \"FileName1\") bampaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") read_statsDF \u003c- alignStats(args = bampaths, fqpaths = fqpaths, pairEnd = TRUE) write.table(read_statsDF, \"results/alignStats.xls\", row.names = FALSE, quote = FALSE, sep = \"\\t\") }, step_name = \"align_stats\", dependency = \"hisat2_mapping\")  To inspect the R code of this step, the codeLine method can be used.\ncodeLine(sal, \"align_stats\")  ## align_stats ## fqpaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"targetsWF\", column = \"FileName1\") ## bampaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") ## read_statsDF \u003c- alignStats(args = bampaths, fqpaths = fqpaths, pairEnd = TRUE) ## write.table(read_statsDF, \"results/alignStats.xls\", row.names = FALSE, quote = FALSE, sep = \"\\t\")  The getColumn method allows to extract certain information of a workflow instance, here output files that can be accessed from within R as demonstrated below.\ngetColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\")  ## M1A M1B ## \"./results/M1A.sorted.bam\" \"./results/M1B.sorted.bam\" ## A1A A1B ## \"./results/A1A.sorted.bam\" \"./results/A1B.sorted.bam\" ## V1A V1B ## \"./results/V1A.sorted.bam\" \"./results/V1B.sorted.bam\" ## M6A M6B ## \"./results/M6A.sorted.bam\" \"./results/M6B.sorted.bam\" ## A6A A6B ## \"./results/A6A.sorted.bam\" \"./results/A6B.sorted.bam\" ## V6A V6B ## \"./results/V6A.sorted.bam\" \"./results/V6B.sorted.bam\" ## M12A M12B ## \"./results/M12A.sorted.bam\" \"./results/M12B.sorted.bam\" ## A12A A12B ## \"./results/A12A.sorted.bam\" \"./results/A12B.sorted.bam\" ## V12A V12B ## \"./results/V12A.sorted.bam\" \"./results/V12B.sorted.bam\"  Loading workflows from an R Markdown The importWF function allows to load an entire workflow from an R Markdown (Rmd) file into an SYSargsList object that has been intialized with SPRproject() as introduced above. Next, one can run the workflow from start to finish with a single function call using runWF. Below the latter has been commented out to first introduce additional details prior to executing the workflow with runWF, which will be discussed in the following section.\nsal \u003c- SPRproject()  ## Creating directory '/home/tgirke/tmp/GEN242/content/en/tutorials/systempiper/rnaseq/.SPRproject' ## Creating file '/home/tgirke/tmp/GEN242/content/en/tutorials/systempiper/rnaseq/.SPRproject/SYSargsList.yml'  sal \u003c- importWF(sal, file_path = \"systemPipeRNAseq.Rmd\")  ## Reading Rmd file ## ## ---- Actions ---- ## Ignore none-R chunks at line: 25 ## Checking chunk eval values ## Checking chunk SPR option ## Ignore non-SPR chunks: 34, 43, 61, 96, 121, 188, 205, 298, 658, 686, 704, 712, 723 ## Parse chunk code ## Now importing step 'load_SPR' ## Now importing step 'preprocessing' ## Now importing step 'trimming' ## Now importing step 'fastq_report' ## Now importing step 'hisat2_index' ## Now importing step 'hisat2_mapping' ## Now importing step 'align_stats' ## Now importing step 'bam_IGV' ## Now importing step 'create_db' ## Now importing step 'read_counting' ## Now importing step 'sample_tree' ## Now importing step 'run_edger' ## Now importing step 'custom_annot' ## Now importing step 'filter_degs' ## Now importing step 'venn_diagram' ## Now importing step 'get_go_annot' ## Now importing step 'go_enrich' ## Now importing step 'go_plot' ## Now importing step 'heatmap' ## Now importing step 'sessionInfo'  # sal \u003c- runWF(sal)  Several accessor functions are available to inspect the workflow.\nstepsWF(sal) dependency(sal) codeLine(sal) targetsWF(sal)  Third-party software tools Examples of preconfigured CWL templates for third-party software tools are provided in the following table. In addition, the tutorial entitled Automate Creation of CWL Instructions explains how to create these CWL templates for essentially any command-line tool simply by providing the command-line string for a new software as input.\n  Tool Name\n Description\n Step\n     bwa\n BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome.  Alignment\n   Bowtie2\n Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences.\n Alignment\n   FASTX-Toolkit\n FASTX-Toolkit is a collection of command line tools for Short-Reads FASTA/FASTQ files preprocessing.\n Read Preprocessing\n   TransRate\n Transrate is software for de-novo transcriptome assembly quality analysis.\n Quality\n   Gsnap\n GSNAP is a genomic short-read nucleotide alignment program.\n Alignment\n   Samtools\n Samtools is a suite of programs for interacting with high-throughput sequencing data.\n Post-processing\n   Trimmomatic\n Trimmomatic is a flexible read trimming tool for Illumina NGS data.\n Read Preprocessing\n   Rsubread\n Rsubread is a Bioconductor software package that provides high-performance alignment and read counting functions for RNA-seq reads.\n Alignment\n   Picard\n Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.\n Manipulating HTS data\n   Busco\n BUSCO assesses genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs.\n Quality\n   Hisat2\n HISAT2 is a fast and sensitive alignment program for mapping NGS reads (both DNA and RNA) to reference genomes.\n Alignment\n   Tophat2\n TopHat is a fast splice junction mapper for RNA-Seq reads.\n Alignment\n   GATK\n Variant Discovery in High-Throughput Sequencing Data.\n Variant Discovery\n   STAR\n STAR is an ultrafast universal RNA-seq aligner.\n Alignment\n   Trim_galore\n Trim Galore is a wrapper around Cutadapt and FastQC to consistently apply adapter and quality trimming to FastQ files.\n Read Preprocessing\n   TransDecoder\n TransDecoder identifies candidate coding regions within transcript sequences.\n Find Coding Regions\n   Trinity\n Trinity assembles transcript sequences from Illumina RNA-Seq data.\n denovo Transcriptome Assembly\n   Trinotate\n Trinotate is a comprehensive annotation suite designed for automatic functional annotation of transcriptomes.\n Transcriptome Functional Annotation\n   MACS2\n MACS2 identifies transcription factor binding sites in ChIP-seq data.\n Peak calling\n   Kallisto\n kallisto is a program for quantifying abundances of transcripts from RNA-Seq data.\n Read counting\n   BCFtools\n BCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF.\n Variant Discovery\n   Bismark\n Bismark is a program to map bisulfite treated sequencing reads to a genome of interest and perform methylation calls in a single step.\n Bisulfite mapping\n   Fastqc\n FastQC is a quality control tool for high throughput sequence data.\n Quality\n   Blast\n BLAST finds regions of similarity between biological sequences.\n Blast\n     Importantly, command-line software needs to be installed on a user’s system\nand available in a user’s PATH. To check whether this is the case, one can use the\ntryCL function.\ntryCL(command = \"grep\")  How to run workflows? Setup and requirements To execute the code in this tutorial, one needs the following software installed.\n R (version \u003e=4.1.2) systemPipeR package (version \u003e=2.0.8) Hisat2 (version \u003e= 2.1.0)  To build custom workflows with any additional command-line software, one needs the respective software installed and available in the user PATH. To test this, one can use the tryCL function.\ntryCL(command = \"hisat2\") ## 'All set up, proceed!'  Run from R For running a workflow, the runWF function can be used. It executes all workflow steps stored in a SYSargsList container (below named sal). This assumes the sal object has been initialized and populated as outlined above using SPRproject() and importWF(sal, file_path=\"...\"), respectively.\nsal \u003c- runWF(sal)  This essential function allows the user to choose one or multiple steps to be executed using its steps argument. However, it is necessary to maintain valid dependencies (also see workflow graph). If a selected step depends on a previous step(s), the output of which may not be available yet, then the execution will fail.\nsal \u003c- runWF(sal, steps = c(1, 3))  The runWF function allows to force the execution of chosen workflow steps, even if the status of a given step is 'Success' and the expected outfiles exist already. This is useful for updating certain steps when needed. Another option is to force runWF to ignore all warnings and errors. This can be achieved by assigning FALSE to the arguments warning.stop and error.stop, respectively.\nsal \u003c- runWF(sal, force = TRUE, warning.stop = FALSE, error.stop = FALSE)  While SYSargsList (sal) objects are autosaved when working with workflows, it can be sometimes safer to explicity save the object before closing R.\nsal \u003c- write_SYSargsList(sal)  Resume and reset workflows To restart workflows, set resume=TRUE.\nsal \u003c- SPRproject(resume = TRUE)  To delete all steps in a workflow including the content saved under the .SPRproject directory, use overwrite=TRUE. This option should be used with caution, since it effectively deletes most previous workflow data.\nsal \u003c- SPRproject(overwrite = TRUE)  For additional options and details, users want to consult the help documentation for: ?SPRproject, ?runWF and ?'SYSargsList-class'.\nParallelization on clusters The computation of time-consuming steps can be greatly accelerated by processing many files in parallel using several compute nodes of a cluster, where a scheduling system is used for load balancing.\nThe following resources list provides to a SYSargsList (sal) object the number of independent parallel cluster processes defined under the Njobs element in the list. The given example will run 18 processes in parallel using for each 4 CPU cores, thus utilizing a total of 72 CPU cores. Note, runWF can be used with most queueing systems as it is based on utilities defined by the batchtools package, which supports the use of template files (*.tmpl) for defining the run parameters of different schedulers. To run the following code, one needs to have both a conffile (see .batchtools.conf.R samples here) and a template file (see *.tmpl samples here) for the queueing system available on a system. The following example uses the sample conffile and template files for the Slurm scheduler provided by this package.\nThe resources list can be appended when a workflow step is generated. Alternatively, one can append these resource specifications to any step of a pre-generated SYSarsList with the addResources function.\nresources \u003c- list(conffile=\".batchtools.conf.R\", template=\"batchtools.slurm.tmpl\", Njobs=18, walltime=120, # in min ntasks=1, ncpus=4, memory=1024, # in Mb partition = \"short\" ) sal \u003c- addResources(sal, c(\"hisat2_mapping\"), resources = resources) sal \u003c- runWF(sal)  The above example will submit via runWF(sal) the hisat2_mapping step to a partition called short on a computer cluster. Users need to adjust this and other parameters defined in the resources list to their environment.\nRun from Command-Line (without cluster) Create an R script containing the following (or similar) minimum content.\n#!/usr/bin/env Rscript library(systemPipeR) sal \u003c- SPRproject() sal \u003c- importWF(sal, file_path = \"systemPipeRNAseq.Rmd\") # adjust name of Rmd file if different sal \u003c- runWF(sal) # runs entire workflow sal \u003c- renderReport(sal) # after workflow has completed render Rmd to HTML report  Assuming the script is named wf_run_script.R it can be executed from the command-line (not R console!) as follows. In addition, one can make the script executable to run it like any other script.\nRscript wf_run_script.R  This will run systemPipeR workflows on a single machine. In this case a limited amount of parallelization is possible if the corresponding code chunks in the workflow take advantage of multi-core parallelization instructions provided by BiocParallel, batchtools or related packages. However, this type of parallelization is usually limited to the number of cores available on a single CPU. A much more scalable approach is the use of computer clusters as described above and in the next section.\nSubmit workflow from command-line to cluster In addition to running workflows within interactive R sessions or submitting them from the command-line on a single system (see above), one can execute systemPipeR workflows from the command-line to an HPC cluster by including the relevant workflow run instructions in an R script and then submitting it via a submission script of a workload manager system to a computer cluster. The following gives an example for the Slurm workload manager. To understand the details, it is important to inspect the content of the two files (here .R and .sh). Additional details about resource specification under Slurm are given below.\n R script: wf_run_script.R Slurm submission script: wf_run_script.sh  As a test, users can generate in their user account of a cluster a workflow environment populated with the toy data as outlined here. After this, it is best to create within the workflow directory a subdirectory, e.g. called cl_sbatch_run, and then save the above two files to this subdirectory. Next, the parameters in both files need to be adjusted to match the type of workflow and the required computing resources. This includes the name of the Rmd file and scheduler resource settings such as: partition, Njobs, walltime, memory, etc. After all relevant settings have been set correctly, one can execute the workflow with sbatch from within the cl_sbatch_run directory as follows (note this is a command-line call outside of R):\nsbatch wf_run_script.sh  After the submission to the cluster, one usually should check its status and progress with squeue -u \u003cusername\u003e (under Slurm) as well as by monitoring the content of the slurm-\u003cjobid\u003e.out file generated by the scheduler in the same directory. This file contains most of the STDOUT and STDERROR generated by a cluster job. Once everything is working on the toy data, users can run the workflow on real data the same way.\nVisualize workflow systemPipeR workflows instances can be visualized with the plotWF function. The resulting plot includes the following information.\n Workflow topology graph (dependency graphs between different steps) Workflow step status, e.g. Success, Error, Pending, Warnings Sample status and statistics Workflow timing: run duration time  If no argument is provided, the basic plot will automatically detect width, height, layout, plot method, branches, etc.\nplotWF(sal)   {\"x\":{\"dot\":\"digraph {\\n node[fontsize=20];\\n subgraph {\\n load_SPR - hisat2_index - hisat2_mapping - create_db - read_counting - run_edger - custom_annot - filter_degs - get_go_annot - go_enrich - heatmap - sessionInfo\\n }\\n load_SPR - preprocessing\\n preprocessing - hisat2_mapping\\n hisat2_mapping - align_stats\\n hisat2_mapping - bam_IGV\\n read_counting - sample_tree\\n filter_degs - venn_diagram\\n go_enrich - go_plot\\n load_SPR - trimming\\n preprocessing - fastq_report\\n load_SPR[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=load_SPR\n0/0/0/1; 0s tooltip=\\\"step load_SPR: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n preprocessing[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=preprocessing\n0/0/0/36; 0s , shape=\\\"box\\\" tooltip=\\\"step preprocessing: 0 samples passed; 0 samples have warnings; 0 samples have errors; 36 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n trimming[style=\\\"solid, rounded\\\" label=trimming\n0/0/0/72; 0s , shape=\\\"box\\\" tooltip=\\\"step trimming: 0 samples passed; 0 samples have warnings; 0 samples have errors; 72 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n fastq_report[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=fastq_report\n0/0/0/1; 0s tooltip=\\\"step fastq_report: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n hisat2_index[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=hisat2_index\n0/0/0/8; 0s , shape=\\\"box\\\" tooltip=\\\"step hisat2_index: 0 samples passed; 0 samples have warnings; 0 samples have errors; 8 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n hisat2_mapping[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=hisat2_mapping\n0/0/0/72; 0s , shape=\\\"box\\\" tooltip=\\\"step hisat2_mapping: 0 samples passed; 0 samples have warnings; 0 samples have errors; 72 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n align_stats[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=align_stats\n0/0/0/1; 0s tooltip=\\\"step align_stats: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n bam_IGV[style=\\\"solid, \\\"label=bam_IGV\n0/0/0/1; 0s tooltip=\\\"step bam_IGV: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n create_db[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=create_db\n0/0/0/1; 0s tooltip=\\\"step create_db: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n read_counting[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=read_counting\n0/0/0/1; 0s tooltip=\\\"step read_counting: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n sample_tree[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=sample_tree\n0/0/0/1; 0s tooltip=\\\"step sample_tree: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n run_edger[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=run_edger\n0/0/0/1; 0s tooltip=\\\"step run_edger: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n custom_annot[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=custom_annot\n0/0/0/1; 0s tooltip=\\\"step custom_annot: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n filter_degs[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=filter_degs\n0/0/0/1; 0s tooltip=\\\"step filter_degs: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n venn_diagram[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=venn_diagram\n0/0/0/1; 0s tooltip=\\\"step venn_diagram: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n get_go_annot[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=get_go_annot\n0/0/0/1; 0s tooltip=\\\"step get_go_annot: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n go_enrich[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=go_enrich\n0/0/0/1; 0s tooltip=\\\"step go_enrich: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n go_plot[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=go_plot\n0/0/0/1; 0s tooltip=\\\"step go_plot: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n heatmap[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=heatmap\n0/0/0/1; 0s tooltip=\\\"step heatmap: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n sessionInfo[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=sessionInfo\n0/0/0/1; 0s tooltip=\\\"step sessionInfo: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-24 16:55:24; End time: 2022-04-24 16:55:24; Duration: 00:00:00\\\"]\\n subgraph cluster_legend {\\n rankdir=TB;\\n color=\\\"#eeeeee\\\";\\n style=filled;\\n ranksep =1;\\n label=\\\"Legends\\\";\\n fontsize = 30;\\n node [style=filled, fontsize=10];\\n legend_img- step_state[color=\\\"#eeeeee\\\"];\\n\\n legend_img[shape=none, image=\\\"plotwf_legend-src.png\\\", label = \\\" \\\", height=1, width=3, style=\\\"\\\"];\\n\\n step_state[style=\\\"filled\\\", shape=\\\"box\\\" color=white, label =\\n Step Colors\\n Pending steps; Successful steps; Failed steps\\n Targets Files / Code Chunk 0 (pass)  | 0 (warning)  | 0 (error)  | 0 (total); Duration\\n ];\\n\\n }\\n\\n}\\n\",\"plotid\":\"sprwf-65123748\",\"responsive\":true,\"width\":null,\"height\":null,\"plot_method\":\"renderSVGElement\",\"rmd\":true,\"msg\":\"\",\"plot_ctr\":true,\"pan_zoom\":false,\"legend_uri\":\"data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!-- Do not edit this file with editors other than diagrams.net -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="496px" height="278px" viewBox="-0.5 -0.5 496 278" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2021-11-24T20:39:44.923Z&quot; agent=&quot;5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&quot; version=&quot;15.8.3&quot; etag=&quot;T_dExnL7SLX2buX8PaX3&quot; type=&quot;google&quot;&gt;&lt;diagram id=&quot;Vw9VViGsx-sL_NjKe7JP&quot;&gt;7VrbcuI4EP0aHpOyJd94DARmXmZ3K2zVPitYGFVkyWPLgczXr2TL+CID3sGQTRUkldgtqS2d02p1t5nAebz/lqJk+4OHmE6AFe4n8HkCgA0AmKhfK/zQEsvxS0mUklDLasGK/MJVRy3NSYizVkfBORUkaQvXnDG8Fi0ZSlO+a3fbcNp+aoIibAhWa0RN6T8kFNtSOrWsWv4dk2hbPdmrWmJUddaCbItCvmuI4GIC5ynnoryK93NMFXoVLuW45ZHWw8RSzMSQAZqJd0RzvTY9L/FRLTZKeZ5M4Ez+YyFW4yx5d5i4utFKcCrwvo8A9Fops8wJ2odlS4PBPMYi/ZBdKkWBHqJtBUB9v6uBB45byrZN0KGnCddkRwfdNR7yQkPSDw88D08blt2WCLxK0Fq17qT5S9lWxFL/sy0vN4TSOac8LcbC0MVB6Cg0RcrfcNXCOJPDZxFFWdYHd/aGxXp7DvsmxqAfY42pA4dB6oyAqHNdRJfLRfC8OIboVVEMpgNR9C9H0b0uiovlEiznt0XxLGzwcti8sWHjTOjTKTBR3LjqR8oRJRGTMoo34hioSlVjbPmRci4fToQysWBEhH3rSg7TNxDOOJWNXZjl9EUbywqktVwGlijM1CKJPHOfdENMwlANn6U4I7/0kaIwSThhopizO5u4z0pXLnhW8mIfBbxBVoODJp3ydoliQhX6f5NYRhzA+gPv5N8XHiM2lI7TBxzw/Ue3fcQB05PAHsKqw/QSvgKDrxBlW3wn7Dhh0HUNwgKDMOdKhE0Nwn4gJuPNWM3+q5Fm27fbZm7w6Hdoc81Qss8xjkFblbk0eJvzOMkFvpN2Yqs5xlbr4cy7Fmd2T7jg0eIIT+QSm6x5P3OVshW4PJT4PskOQbKv2+RVpP7Lp8hEFVe65DRKdWWrYQ9V9/8UxIV4g3Iqbre9vKDLlO8ZTLn2Y09kB0YIiG0zkX2576wTfAV2hy9o9WSB19pZZl4tvaFcXPhACbu7xFMuEXZdIrRsg7jgWsSZ6Xuq6jssoiZrdSplX+K8PiHAA0N8Gej1Ze4IIA/I7jELn1QhszbHBpadSl1hz1WdEh4wwqFR5DyLUHP9PSZWyVJMkSDvbfV9kOgn/KX2ZPMwmT6CDgPdqknG83SN9UDQKHB2dEFgG3FfV5dAaYSFoavg6bD4YdQNqDBcRJ3OzsrO/3ceZabUAd57DKxp/XF/j1PHMlLmM5pHZNiscMgELESC601+P7eOBIhdpwod06leLZQ36xx/JoJwhuidtFMnoTWAtD4PMgppZq1jlPxLwTs0+bro7Q4MPfx6quB7i+pi+/UZBGaU7/i9kcwIDAKz6jEKg4zLllvRuCw+n0ujGZLemEizFPKSM0ZYNFHqSxJe0yGE2pZktKCyQ+oKZ5n0yF/IH+s5dt4E2dbNzMIOgnYYBL3KAs74ZziGUZg1l8oUQvJ+kS2c9RKvaP0WFbv6YV0yo/SJFLGsgm+mfMTA5x2M+Yg7OYiLhekx+GdO0q9WeP98m/XbNguBGVH0VeFHsViz7rQSOBnPh82Lr2zcrWG4NUxh24MFZjGr71Xab1iDvK2/V1VmfvXX0+DiXw==&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g><rect x="4" y="90" width="490" height="92" fill="#d5e8d4" stroke="none" pointer-events="none"/><rect x="4" y="182" width="490" height="94" fill="#ffe8de" stroke="none" pointer-events="none"/><rect x="4" y="4" width="490" height="86" fill="#eff2fc" stroke="none" pointer-events="none"/><rect x="4" y="4" width="140" height="272" fill-opacity="0.8" fill="#f5f5f5" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 11px; margin-left: 115px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">solid</div></div></div></foreignObject><text x="115" y="13" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">solid</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 10px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">dashed</div></div></div></foreignObject><text x="198" y="12" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">dashed</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 32px; margin-left: 116px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Management</div></div></div></foreignObject><text x="116" y="35" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Management</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 32px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Compute</div></div></div></foreignObject><text x="198" y="35" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Compute</text></switch></g><ellipse cx="232.5" cy="123" rx="51.5" ry="27" fill="rgba(255, 255, 255, 1)" stroke="rgba(0, 0, 0, 1)" stroke-width="2" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 50px; height: 1px; padding-top: 62px; margin-left: 92px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">ellipse</span></div></div></div></foreignObject><text x="116" y="65" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">ellipse</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 85px; margin-left: 114px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">R</div></div></div></foreignObject><text x="114" y="88" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">R</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 83px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Command-line</div></div></div></foreignObject><text x="198" y="86" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Command-line</text></switch></g><rect x="349" y="96" width="105" height="50" rx="7.5" ry="7.5" fill="rgba(255, 255, 255, 1)" stroke="rgba(0, 0, 0, 1)" stroke-width="2" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 51px; height: 1px; padding-top: 61px; margin-left: 176px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;">rectangle</div></div></div></foreignObject><text x="201" y="63" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">rectangle</text></switch></g><path d="M 182.5 38 L 287.5 38" fill="none" stroke="rgba(0, 0, 0, 1)" stroke-width="6" stroke-miterlimit="10" pointer-events="none"/><path d="M 354 37.62 L 459 37.62" fill="none" stroke="rgba(0, 0, 0, 1)" stroke-width="6" stroke-miterlimit="10" stroke-dasharray="18 18" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 128px; margin-left: 115px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Mandatory</div></div></div></foreignObject><text x="115" y="131" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Mandatory</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 128px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Optional</div></div></div></foreignObject><text x="198" y="131" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Optional</text></switch></g><rect x="184" y="190" width="95" height="40" fill="#d3d6eb" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 46px; height: 1px; padding-top: 105px; margin-left: 93px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">fill</span></div></div></div></foreignObject><text x="116" y="109" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">fill</text></switch></g><rect x="349" y="190" width="95" height="40" fill="#ffffff" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 46px; height: 1px; padding-top: 105px; margin-left: 176px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">no fill</span></div></div></div></foreignObject><text x="198" y="109" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">no fill</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 24px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;">Running <br style="font-size: 10px" />Session</div></div></div></foreignObject><text x="35" y="27" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">Running...</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 113px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;"><div style="font-size: 10px"><span style="background-color: transparent ; font-size: 10px">Running</span></div>Requirement</div></div></div></foreignObject><text x="35" y="116" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">RunningRequire...</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 68px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;">Step <br style="font-size: 10px" />Class</div></div></div></foreignObject><text x="35" y="71" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">Step...</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Viewer does not support full SVG 1.1</text></a></switch></svg>\"},\"evals\":[],\"jsHooks\":[]} Technical reports systemPipeR compiles all workflow execution logs in one central location, making it easy to check any standard output (stdout) or standard error (stderr) for any command-line tool used in a workflow. Also, the information is appended to the workflow plot making it easy to click on respective steps.\nsal \u003c- renderLogs(sal)  Scientific reports systemPipeR auto-generates scientific analysis reports in HTML format. These reports compile the results of all workflow steps including text, code, plots and tables.\nsal \u003c- renderReport(sal)  Alternatively, scientific reports can be rendered with the render function from rmarkdown.\nrmarkdown::render(\"systemPipeRNAseq.Rmd\", clean = TRUE, output_format = \"BiocStyle::html_document\")  How to modify workflows? Modifying steps If needed one can modify existing workflow steps in a pre-populated SYSargsList object, and potentially already executed workflows, with the replaceStep(sal) \u003c- replacement function. The following gives an example where step number 3 in a SYSargsList (sal) object will be updated with modified or new code. Note, this is a generalized example where the user needs to insert the code lines between { and }, and also adjust the values assigned to the arguments: step_name and dependency.\nreplaceStep(sal, step = 3) \u003c- LineWise(code = { ... }, step_name = ..., dependency = ...)  Subsequently, one can rerun the corresponding step (here 3) as follows:\nrunWF(sal, steps = 3)  As mentioned above, any step in a workflow can only be run in isolation if its expected input exists (see dependency).\nAdding steps New steps can be added to the Rmd file of a workflow by inserting new R Markdown code blocks (chunks) starting and ending with the usual appendStep\u003c- syntax and then creating a new SYSargsList instance with importWF that contain the new step(s). To add steps to a pre-populated SYSargsList object, one can use the after argument of the appendStep\u003c- function. The following example will add a new step after position 3 to the corresponding sal object. This can be useful if a workflow with a long runtime has already been completed and one only wants to make some refinements without re-running the entire workflow.\nappendStep(sal, after = 3) \u003c- ...  Workflow templates Workflow templates are provided via the affiliated Bioconductor data package named systemPipeRdata, as well as by a dedicated GitHub repository. Instances of these workflows can be created with a single command. The following gives several examples.\nRNA-Seq WF template Load the RNA-Seq sample workflow into your current working directory.\nlibrary(systemPipeRdata) genWorkenvir(workflow = \"rnaseq\") setwd(\"rnaseq\")  Create the workflow This template includes the most common analysis steps of RNAseq workflows. One can add, remove, modify workflow steps by modifying the sal object.\nsal \u003c- SPRproject() sal \u003c- importWF(sal, file_path = \"systemPipeRNAseq.Rmd\", verbose = FALSE)  Workflow steps:\n Read preprocessing  Quality filtering (trimming) FASTQ quality report   Alignments: HISAT2 (or any other RNA-Seq aligner) Alignment stats Read counting Sample-wise correlation analysis Analysis of differentially expressed genes (DEGs) GO term enrichment analysis Gene-wise clustering  Run workflow sal \u003c- runWF(sal)  Workflow visualization\nplotWF(sal)  Report generation\nsal \u003c- renderReport(sal) sal \u003c- renderLogs(sal)  ChIP-Seq WF template Load the ChIP-Seq sample workflow into your current working directory.\nlibrary(systemPipeRdata) genWorkenvir(workflow = \"chipseq\") setwd(\"chipseq\")  Workflow steps:\n Read preprocessing  Quality filtering (trimming) FASTQ quality report   Alignments: Bowtie2 or rsubread Alignment stats Peak calling: MACS2 Peak annotation with genomic context Differential binding analysis GO term enrichment analysis Motif analysis  Create the workflow This template provides some common steps for a ChIPseq workflow. One can add, remove, modify workflow steps by operating on the sal object.\nsal \u003c- SPRproject() sal \u003c- importWF(sal, file_path = \"systemPipeChIPseq.Rmd\", verbose = FALSE)  Run workflow sal \u003c- runWF(sal)  Workflow visualization\nplotWF(sal)  Report generation\nsal \u003c- renderReport(sal) sal \u003c- renderLogs(sal)  VAR-Seq WF template Load the VAR-Seq sample workflow into your current working directory.\nlibrary(systemPipeRdata) genWorkenvir(workflow = \"varseq\") setwd(\"varseq\")  Workflow steps:\n Read preprocessing  Quality filtering (trimming) FASTQ quality report   Alignments: gsnap, bwa Variant calling: VariantTools, GATK, BCFtools Variant filtering: VariantTools and VariantAnnotation Variant annotation: VariantAnnotation Combine results from many samples Summary statistics of samples  Create the workflow This template provides some common steps for a VARseq workflow. One can add, remove, modify workflow steps by operating on the sal object.\nsal \u003c- SPRproject() sal \u003c- importWF(sal, file_path = \"systemPipeVARseq.Rmd\", verbose = FALSE)  Run workflow sal \u003c- runWF(sal)  Workflow visualization\nplotWF(sal)  Report generation\nsal \u003c- renderReport(sal) sal \u003c- renderLogs(sal)  Ribo-Seq WF template Load the Ribo-Seq sample workflow into your current working directory.\nlibrary(systemPipeRdata) genWorkenvir(workflow = \"riboseq\") setwd(\"riboseq\")  Workflow steps:\n Read preprocessing  Adaptor trimming and quality filtering FASTQ quality report   Alignments: HISAT2 (or any other RNA-Seq aligner) Alignment stats Compute read distribution across genomic features Adding custom features to workflow (e.g. uORFs) Genomic read coverage along transcripts Read counting Sample-wise correlation analysis Analysis of differentially expressed genes (DEGs) GO term enrichment analysis Gene-wise clustering Differential ribosome binding (translational efficiency)  Create the workflow This template provides some common steps for a RIBOseq workflow. One can add, remove, modify workflow steps by operating on the sal object.\nsal \u003c- SPRproject() sal \u003c- importWF(sal, file_path = \"systemPipeRIBOseq.Rmd\", verbose = FALSE)  Run workflow sal \u003c- runWF(sal)  Workflow visualization\nplotWF(sal, rstudio = TRUE)  Report generation\nsal \u003c- renderReport(sal) sal \u003c- renderLogs(sal)  Version information Note: the most recent version of this tutorial can be found here.\nsessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] magrittr_2.0.2 batchtools_0.9.15 ## [3] ape_5.5 ggplot2_3.3.5 ## [5] systemPipeR_2.0.8 ShortRead_1.52.0 ## [7] GenomicAlignments_1.30.0 SummarizedExperiment_1.24.0 ## [9] Biobase_2.54.0 MatrixGenerics_1.6.0 ## [11] matrixStats_0.61.0 BiocParallel_1.28.2 ## [13] Rsamtools_2.10.0 Biostrings_2.62.0 ## [15] XVector_0.34.0 GenomicRanges_1.46.1 ## [17] GenomeInfoDb_1.30.0 IRanges_2.28.0 ## [19] S4Vectors_0.32.3 BiocGenerics_0.40.0 ## [21] BiocStyle_2.22.0 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-155 bitops_1.0-7 webshot_0.5.3 ## [4] httr_1.4.2 RColorBrewer_1.1-2 progress_1.2.2 ## [7] tools_4.1.3 backports_1.4.0 bslib_0.3.1 ## [10] utf8_1.2.2 R6_2.5.1 DBI_1.1.1 ## [13] colorspace_2.0-2 withr_2.4.3 tidyselect_1.1.1 ## [16] prettyunits_1.1.1 compiler_4.1.3 rvest_1.0.2 ## [19] cli_3.1.0 formatR_1.11 xml2_1.3.3 ## [22] DelayedArray_0.20.0 bookdown_0.24 sass_0.4.0 ## [25] scales_1.1.1 checkmate_2.0.0 rappdirs_0.3.3 ## [28] systemfonts_1.0.4 stringr_1.4.0 digest_0.6.29 ## [31] svglite_2.1.0 rmarkdown_2.13 jpeg_0.1-9 ## [34] pkgconfig_2.0.3 htmltools_0.5.2 fastmap_1.1.0 ## [37] htmlwidgets_1.5.4 rlang_1.0.2 rstudioapi_0.13 ## [40] jquerylib_0.1.4 generics_0.1.1 hwriter_1.3.2 ## [43] jsonlite_1.8.0 dplyr_1.0.7 RCurl_1.98-1.5 ## [46] kableExtra_1.3.4 GenomeInfoDbData_1.2.7 Matrix_1.4-0 ## [49] Rcpp_1.0.8.2 munsell_0.5.0 fansi_0.5.0 ## [52] lifecycle_1.0.1 stringi_1.7.6 yaml_2.3.5 ## [55] zlibbioc_1.40.0 grid_4.1.3 parallel_4.1.3 ## [58] crayon_1.4.2 lattice_0.20-45 hms_1.1.1 ## [61] knitr_1.37 pillar_1.6.4 base64url_1.4 ## [64] codetools_0.2-18 glue_1.6.2 evaluate_0.15 ## [67] blogdown_1.8.2 latticeExtra_0.6-29 data.table_1.14.2 ## [70] BiocManager_1.30.16 png_0.1-7 vctrs_0.3.8 ## [73] gtable_0.3.0 purrr_0.3.4 assertthat_0.2.1 ## [76] xfun_0.30 viridisLite_0.4.0 tibble_3.1.6 ## [79] ellipsis_0.3.2 brew_1.0-6  Funding This project is funded by NSF award ABI-1661152.\nReferences Amstutz, Peter, Michael R Crusoe, Nebojša Tijanić, Brad Chapman, John Chilton, Michael Heuer, Andrey Kartashov, et al. 2016. “Common Workflow Language, V1.0,” July. https://doi.org/10.6084/m9.figshare.3115156.v2.\n H Backman, Tyler W, and Thomas Girke. 2016. “systemPipeR: NGS workflow and report generation environment.” BMC Bioinformatics 17 (1): 388. https://doi.org/10.1186/s12859-016-1241-0.\n Howard, Brian E, Qiwen Hu, Ahmet Can Babaoglu, Manan Chandra, Monica Borghi, Xiaoping Tan, Luyan He, et al. 2013. “High-Throughput RNA Sequencing of Pseudomonas-Infected Arabidopsis Reveals Hidden Transcriptome Complexity and Novel Splice Variants.” PLoS One 8 (10): e74183. https://doi.org/10.1371/journal.pone.0074183.\n Kim, Daehwan, Ben Langmead, and Steven L Salzberg. 2015. “HISAT: A Fast Spliced Aligner with Low Memory Requirements.” Nat. Methods 12 (4): 357–60.\n  ","categories":"","description":"","excerpt":"                    pre code { white-space: pre !important; …","ref":"/tutorials/systempiper/systempiper/","tags":"","title":"systemPipeR: Workflow Design and Reporting Environment"},{"body":"               pre code { white-space: pre !important; overflow-x: scroll !important; word-break: keep-all !important; word-wrap: initial !important; }  document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code download: [ .Rmd ] [ .R ]\n Introduction This report describes the analysis of the RNA-Seq data set from Howard et al (2013). The corresponding FASTQ files were downloaded from GEO (Accession: SRP010938). This data set contains 18 paired-end (PE) read sets from Arabidposis thaliana. The details about all download steps are provided here.\nUsers want to provide here additional background information about the design of their RNA-Seq project.\nExperimental design Typically, users want to specify here all information relevant for the analysis of their NGS study. This includes detailed descriptions of FASTQ files, experimental design, reference genome, gene annotations, etc.\nWorkflow environment NOTE: this section describes how to set up the proper environment (directory structure) for running systemPipeR workflows. After mastering this task the workflow run instructions can be deleted since they are not expected to be included in a final HTML/PDF report of a workflow.\n  If a remote system or cluster is used, then users need to log in to the remote system first. The following applies to an HPC cluster (e.g. HPCC cluster).\nA terminal application needs to be used to log in to a user’s cluster account. Next, one can open an interactive session on a computer node with srun. More details about argument settings for srun are available in this HPCC manual or the HPCC section of this website here. Next, load the R version required for running the workflow with module load. Sometimes it may be necessary to first unload an active software version before loading another version, e.g. module unload R.\n  srun --x11 --partition=gen242 --mem=20gb --cpus-per-task 8 --ntasks 1 --time 20:00:00 --pty bash -l module unload R; module load R/4.1.2  Load a workflow template with the genWorkenvir function. This can be done from the command-line or from within R. However, only one of the two options needs to be used.  From command-line\n$ Rscript -e \"systemPipeRdata::genWorkenvir(workflow='rnaseq')\" $ cd rnaseq  From R\nlibrary(systemPipeRdata) genWorkenvir(workflow = \"rnaseq\") setwd(\"rnaseq\")   Optional: if the user wishes to use another Rmd file than the template instance provided by the genWorkenvir function, then it can be copied or downloaded into the root directory of the workflow environment (e.g. with cp or wget).\n  Now one can open from the root directory of the workflow the corresponding R Markdown script (e.g. systemPipeChIPseq.Rmd) using an R IDE, such as nvim-r, ESS or RStudio. Subsequently, the workflow can be run as outlined below. For learning purposes it is recommended to run workflows for the first time interactively. Once all workflow steps are understood and possibly modified to custom needs, one can run the workflow from start to finish with a single command using runWF().\n  Load packages The systemPipeR package needs to be loaded to perform the analysis steps shown in this report (H Backman and Girke 2016). The package allows users to run the entire analysis workflow interactively or with a single command while also generating the corresponding analysis report. For details see systemPipeR's main vignette.\nlibrary(systemPipeR)  To apply workflows to custom data, the user needs to modify the targets file and if necessary update the corresponding parameter (.cwl and .yml) files. A collection of pre-generated .cwl and .yml files are provided in the param/cwl subdirectory of each workflow template. They are also viewable in the GitHub repository of systemPipeRdata (see here). For more information of the structure of the targets file, please consult the documentation here. More details about the new parameter files from systemPipeR can be found here.\nImport custom functions Custom functions for the challenge projects can be imported with the source command from a local R script (here challengeProject_Fct.R). Skip this step if such a script is not available. Alternatively, these functions can be loaded from a custom R package.\nsource(\"challengeProject_Fct.R\")  Experiment definition provided by targets file The targets file defines all FASTQ files and sample comparisons of the analysis workflow. If needed the tab separated (TSV) version of this file can be downloaded from here and the corresponding Google Sheet is here.\ntargetspath \u003c- \"targetsPE.txt\" targets \u003c- read.delim(targetspath, comment.char = \"#\") DT::datatable(targets, options = list(scrollX = TRUE, autoWidth = TRUE))   {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\"],[\"./data/SRR446027_1.fastq.gz\",\"./data/SRR446028_1.fastq.gz\",\"./data/SRR446029_1.fastq.gz\",\"./data/SRR446030_1.fastq.gz\",\"./data/SRR446031_1.fastq.gz\",\"./data/SRR446032_1.fastq.gz\",\"./data/SRR446033_1.fastq.gz\",\"./data/SRR446034_1.fastq.gz\",\"./data/SRR446035_1.fastq.gz\",\"./data/SRR446036_1.fastq.gz\",\"./data/SRR446037_1.fastq.gz\",\"./data/SRR446038_1.fastq.gz\",\"./data/SRR446039_1.fastq.gz\",\"./data/SRR446040_1.fastq.gz\",\"./data/SRR446041_1.fastq.gz\",\"./data/SRR446042_1.fastq.gz\",\"./data/SRR446043_1.fastq.gz\",\"./data/SRR446044_1.fastq.gz\"],[\"./data/SRR446027_2.fastq.gz\",\"./data/SRR446028_2.fastq.gz\",\"./data/SRR446029_2.fastq.gz\",\"./data/SRR446030_2.fastq.gz\",\"./data/SRR446031_2.fastq.gz\",\"./data/SRR446032_2.fastq.gz\",\"./data/SRR446033_2.fastq.gz\",\"./data/SRR446034_2.fastq.gz\",\"./data/SRR446035_2.fastq.gz\",\"./data/SRR446036_2.fastq.gz\",\"./data/SRR446037_2.fastq.gz\",\"./data/SRR446038_2.fastq.gz\",\"./data/SRR446039_2.fastq.gz\",\"./data/SRR446040_2.fastq.gz\",\"./data/SRR446041_2.fastq.gz\",\"./data/SRR446042_2.fastq.gz\",\"./data/SRR446043_2.fastq.gz\",\"./data/SRR446044_2.fastq.gz\"],[\"M1A\",\"M1B\",\"A1A\",\"A1B\",\"V1A\",\"V1B\",\"M6A\",\"M6B\",\"A6A\",\"A6B\",\"V6A\",\"V6B\",\"M12A\",\"M12B\",\"A12A\",\"A12B\",\"V12A\",\"V12B\"],[\"M1\",\"M1\",\"A1\",\"A1\",\"V1\",\"V1\",\"M6\",\"M6\",\"A6\",\"A6\",\"V6\",\"V6\",\"M12\",\"M12\",\"A12\",\"A12\",\"V12\",\"V12\"],[\"Mock.1h.A\",\"Mock.1h.B\",\"Avr.1h.A\",\"Avr.1h.B\",\"Vir.1h.A\",\"Vir.1h.B\",\"Mock.6h.A\",\"Mock.6h.B\",\"Avr.6h.A\",\"Avr.6h.B\",\"Vir.6h.A\",\"Vir.6h.B\",\"Mock.12h.A\",\"Mock.12h.B\",\"Avr.12h.A\",\"Avr.12h.B\",\"Vir.12h.A\",\"Vir.12h.B\"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\",\"23-Mar-2012\"]],\"container\":\"\\n \\n \\n  \\n FileName1\\n FileName2\\n SampleName\\n Factor\\n SampleLong\\n Experiment\\n Date\\n \\n \\n\",\"options\":{\"scrollX\":true,\"autoWidth\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":6},{\"orderable\":false,\"targets\":0}],\"order\":[],\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Workflow steps This tutorial will demonstrate how to either build and run the workflow automatically, or in an interactive mode by appending each step with the appendStep method. In both cases the SYSargsList object will be populated with the instructions for running each workflow step, while supporting both command-line steps as well as line-wise R commands defined in the corresponding code chunks of this or any Rmd file that has been properly formatted.\nTo create a workflow within systemPipeR, we can start by defining an empty SYSargsList container. When restarting an existing workflow one can set resume=TRUE under the SPRproject() function call.\nlibrary(systemPipeR) sal \u003c- SPRproject()  ## Creating directory: /home/tgirke/tmp/GEN242/content/en/tutorials/sprnaseq/data ## Creating directory '/home/tgirke/tmp/GEN242/content/en/tutorials/sprnaseq/.SPRproject' ## Creating file '/home/tgirke/tmp/GEN242/content/en/tutorials/sprnaseq/.SPRproject/SYSargsList.yml'  sal  ## Instance of 'SYSargsList': ## No workflow steps added  Next, the importWF function will load the entire workflow into the SYSargsList object (here sal). Subsequently, the runWF() function will run the workflow from start to finish. If needed, specific workflow steps can be executed by assigning their corresponding position numbers within the workflow to the steps argument (see ?runWF). After completion of the workflow one can render a scientific analysis report in HTML format with the renderReport() function that uses R Markdown internally.\nsal \u003c- importWF(sal, file_path = \"systemPipeRNAseq.Rmd\") # Populates SYSargsList object with run instructions for all steps sal sal \u003c- runWF(sal) # Runs workflow. This may take some time. sal \u003c- renderReport(sal) # Renders report rmarkdown::render(\"systemPipeRNAseq.Rmd\", clean = TRUE, output_format = \"BiocStyle::html_document\") # Alternative report rendering  Required packages and resources The systemPipeR package needs to be loaded (H Backman and Girke 2016).\nappendStep(sal) \u003c- LineWise(code = { library(systemPipeR) }, step_name = \"load_SPR\")  Read preprocessing Read quality filtering and trimming The function preprocessReads allows to apply predefined or custom read preprocessing functions to all FASTQ files referenced in a SYSargsList container, such as quality filtering or adapter trimming routines. The paths to the resulting output FASTQ files are stored in the outfiles slot of the SYSargsList object. The following example performs adapter trimming with the trimLRPatterns function from the Biostrings package. After the trimming step a new targets file is generated (here targets_trim.txt) containing the paths to the trimmed FASTQ files. The new targets file can be used for the next workflow step with an updated SYSargs2 instance, e.g. running the NGS alignments using the trimmed FASTQ files.\nHere, we are appending this step to the SYSargsList object created previously. All the parameters are defined on the preprocessReads/preprocessReads-pe.yml and preprocessReads/preprocessReads-pe.cwl files.\nappendStep(sal) \u003c- SYSargsList(step_name = \"preprocessing\", targets = \"targetsPE.txt\", dir = TRUE, wf_file = \"preprocessReads/preprocessReads-pe.cwl\", input_file = \"preprocessReads/preprocessReads-pe.yml\", dir_path = \"param/cwl\", inputvars = c(FileName1 = \"_FASTQ_PATH1_\", FileName2 = \"_FASTQ_PATH2_\", SampleName = \"_SampleName_\"), dependency = c(\"load_SPR\"))  After, we can check the trimLRPatterns function in input parameter:\nyamlinput(sal, \"preprocessing\")$Fct  ## [1] \"'trimLRPatterns(Rpattern=\\\"GCCCGGGTAA\\\", subject=fq)'\"  After the preprocessing step, the outfiles files can be used to generate the new targets files containing the paths to the trimmed FASTQ files. The new targets information can be used for the next workflow step instance, e.g. running the NGS alignments with the trimmed FASTQ files. The appendStep function is automatically handling this connectivity between steps. Please check the Alignments step for more details.\nFASTQ quality report The following seeFastq and seeFastqPlot functions generate and plot a series of useful quality statistics for a set of FASTQ files including per cycle quality box plots, base proportions, base-level quality trends, relative k-mer diversity, length and occurrence distribution of reads, number of reads above quality cutoffs and mean quality distribution. The results are written to a PDF file named fastqReport.pdf.\nappendStep(sal) \u003c- LineWise(code = { fastq \u003c- getColumn(sal, step = \"preprocessing\", \"targetsWF\", column = 1) fqlist \u003c- seeFastq(fastq = fastq, batchsize = 10000, klength = 8) pdf(\"./results/fastqReport.pdf\", height = 18, width = 4 * length(fqlist)) seeFastqPlot(fqlist) dev.off() }, step_name = \"fastq_report\", dependency = \"preprocessing\")  Figure 1: FASTQ quality report for 18 samples\n  Alignments Read mapping with HISAT2 The following steps will demonstrate how to use the short read aligner Hisat2 (Kim, Langmead, and Salzberg 2015) in both interactive job submissions and batch submissions to queuing systems of clusters using the systemPipeR's new CWL command-line interface.\nThe following steps will demonstrate how to use the short read aligner Hisat2 (Kim, Langmead, and Salzberg 2015). First, the Hisat2 index needs to be created.\nappendStep(sal) \u003c- SYSargsList(step_name = \"hisat2_index\", dir = FALSE, targets = NULL, wf_file = \"hisat2/hisat2-index.cwl\", input_file = \"hisat2/hisat2-index.yml\", dir_path = \"param/cwl\", dependency = \"load_SPR\")  The parameter settings of the aligner are defined in the workflow_hisat2-pe.cwl and workflow_hisat2-pe.yml files. The following shows how to construct the corresponding SYSargsList object. Please note that the targets used in this step are the outfiles from preprocessing step.\nappendStep(sal) \u003c- SYSargsList(step_name = \"hisat2_mapping\", dir = TRUE, targets = \"preprocessing\", wf_file = \"workflow-hisat2/workflow_hisat2-pe.cwl\", input_file = \"workflow-hisat2/workflow_hisat2-pe.yml\", dir_path = \"param/cwl\", inputvars = c(preprocessReads_1 = \"_FASTQ_PATH1_\", preprocessReads_2 = \"_FASTQ_PATH2_\", SampleName = \"_SampleName_\"), rm_targets_col = c(\"FileName1\", \"FileName2\"), dependency = c(\"preprocessing\", \"hisat2_index\"))  To double-check the command line for each sample, please use the following:\ncmdlist(sal, step = \"hisat2_mapping\", targets = 1)  ## $hisat2_mapping ## $hisat2_mapping$M1A ## $hisat2_mapping$M1A$hisat2 ## [1] \"hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 --min-intronlen 30 --max-intronlen 3000 -1 ./results/M1A_1.fastq_trim.gz -2 ./results/M1A_2.fastq_trim.gz --threads 4\" ## ## $hisat2_mapping$M1A$`samtools-view` ## [1] \"samtools view -bS -o ./results/M1A.bam ./results/M1A.sam \" ## ## $hisat2_mapping$M1A$`samtools-sort` ## [1] \"samtools sort -o ./results/M1A.sorted.bam ./results/M1A.bam -@ 4\" ## ## $hisat2_mapping$M1A$`samtools-index` ## [1] \"samtools index -b results/M1A.sorted.bam results/M1A.sorted.bam.bai ./results/M1A.sorted.bam \"  Read and alignment stats The following provides an overview of the number of reads in each sample and how many of them aligned to the reference.\nappendStep(sal) \u003c- LineWise(code = { fqpaths \u003c- getColumn(sal, step = \"preprocessing\", \"targetsWF\", column = \"FileName1\") bampaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") read_statsDF \u003c- alignStats(args = bampaths, fqpaths = fqpaths, pairEnd = TRUE) write.table(read_statsDF, \"results/alignStats.xls\", row.names = FALSE, quote = FALSE, sep = \"\\t\") }, step_name = \"align_stats\", dependency = \"hisat2_mapping\")  The following shows the alignment statistics for a sample file provided by the systemPipeR package.\nread.table(\"results/alignStats.xls\", header = TRUE)[1:4, ]  ## FileName Nreads2x Nalign Perc_Aligned Nalign_Primary ## 1 M1A 115994 109977 94.81266 109977 ## 2 M1B 134480 112464 83.62879 112464 ## 3 A1A 127976 122427 95.66403 122427 ## 4 A1B 122486 101369 82.75966 101369 ## Perc_Aligned_Primary ## 1 94.81266 ## 2 83.62879 ## 3 95.66403 ## 4 82.75966  Create symbolic links for viewing BAM files in IGV The symLink2bam function creates symbolic links to view the BAM alignment files in a genome browser such as IGV without moving these large files to a local system. The corresponding URLs are written to a file with a path specified under urlfile, here IGVurl.txt. Please replace the directory and the user name.\nappendStep(sal) \u003c- LineWise(code = { bampaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") bampaths \u003c- setNames(normalizePath(bampaths), names(bampaths)) symLink2bam(sysargs = bampaths, htmldir = c(\"~/.html/\", \"somedir/\"), urlbase = \"http://cluster.hpcc.ucr.edu/~\u003cusername\u003e/\", urlfile = \"./results/IGVurl.txt\") }, step_name = \"bam_urls\", dependency = \"hisat2_mapping\", run_step = \"optional\")  Read quantification Reads overlapping with annotation ranges of interest are counted for each sample using the summarizeOverlaps function (Lawrence et al. 2013). The read counting is preformed for exonic gene regions in a non-strand-specific manner while ignoring overlaps among different genes. Subsequently, the expression count values are normalized by reads per kp per million mapped reads (RPKM). The raw read count table (countDFeByg.xls) and the corresponding RPKM table (rpkmDFeByg.xls) are written to separate files in the directory of this project. Parallelization is achieved with the BiocParallel package, here using 4 CPU cores.\nCreate a database for gene annotation appendStep(sal) \u003c- LineWise(code = { library(GenomicFeatures) txdb \u003c- suppressWarnings(makeTxDbFromGFF(file = \"data/tair10.gff\", format = \"gff\", dataSource = \"TAIR\", organism = \"Arabidopsis thaliana\")) saveDb(txdb, file = \"./data/tair10.sqlite\") }, step_name = \"create_db\", dependency = \"hisat2_mapping\")  Read counting with summarizeOverlaps in parallel mode using multiple cores appendStep(sal) \u003c- LineWise(code = { library(GenomicFeatures) library(BiocParallel) txdb \u003c- loadDb(\"./data/tair10.sqlite\") outpaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") eByg \u003c- exonsBy(txdb, by = c(\"gene\")) bfl \u003c- BamFileList(outpaths, yieldSize = 50000, index = character()) multicoreParam \u003c- MulticoreParam(workers = 4) register(multicoreParam) registered() counteByg \u003c- bplapply(bfl, function(x) summarizeOverlaps(eByg, x, mode = \"Union\", ignore.strand = TRUE, inter.feature = FALSE, singleEnd = FALSE, BPPARAM = multicoreParam)) countDFeByg \u003c- sapply(seq(along = counteByg), function(x) assays(counteByg[[x]])$counts) rownames(countDFeByg) \u003c- names(rowRanges(counteByg[[1]])) colnames(countDFeByg) \u003c- names(bfl) rpkmDFeByg \u003c- apply(countDFeByg, 2, function(x) returnRPKM(counts = x, ranges = eByg)) write.table(countDFeByg, \"results/countDFeByg.xls\", col.names = NA, quote = FALSE, sep = \"\\t\") write.table(rpkmDFeByg, \"results/rpkmDFeByg.xls\", col.names = NA, quote = FALSE, sep = \"\\t\") ## Creating a SummarizedExperiment object colData \u003c- data.frame(row.names = SampleName(sal, \"hisat2_mapping\"), condition = getColumn(sal, \"hisat2_mapping\", position = \"targetsWF\", column = \"Factor\")) colData$condition \u003c- factor(colData$condition) countDF_se \u003c- SummarizedExperiment::SummarizedExperiment(assays = countDFeByg, colData = colData) ## Add results as SummarizedExperiment to the workflow ## object SE(sal, \"read_counting\") \u003c- countDF_se }, step_name = \"read_counting\", dependency = \"create_db\")  When providing a BamFileList as in the example above, summarizeOverlaps methods use by default bplapply and use the register interface from BiocParallel package. If the number of workers is not set, MulticoreParam will use the number of cores returned by parallel::detectCores(). For more information, please check help(\"summarizeOverlaps\") documentation.\nShows count table generated in previous step (countDFeByg.xls). To avoid slowdowns of the load time of this page, ony 200 rows of the source table are imported into the below datatable view .\ncountDF \u003c- read.delim(\"results/countDFeByg.xls\", row.names = 1, check.names = FALSE)[1:200, ] DT::datatable(countDF, options = list(scrollX = TRUE, autoWidth = TRUE))   {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"AT1G01010\",\"AT1G01020\",\"AT1G01030\",\"AT1G01040\",\"AT1G01046\",\"AT1G01050\",\"AT1G01060\",\"AT1G01070\",\"AT1G01073\",\"AT1G01080\",\"AT1G01090\",\"AT1G01100\",\"AT1G01110\",\"AT1G01115\",\"AT2G01008\",\"AT2G01010\",\"AT2G01020\",\"AT2G01021\",\"AT2G01023\",\"AT3G01010\",\"AT3G01015\",\"AT3G01020\",\"AT3G01030\",\"AT3G01040\",\"AT3G01050\",\"AT3G01060\",\"AT3G01070\",\"AT3G01080\",\"AT3G01085\",\"AT3G01090\",\"AT3G01100\",\"AT3G01120\",\"AT3G01130\",\"AT3G01140\",\"AT3G01142\",\"AT3G01150\",\"AT3G01160\",\"AT3G01170\",\"AT4G00005\",\"AT4G00020\",\"AT4G00026\",\"AT4G00030\",\"AT4G00040\",\"AT4G00050\",\"AT4G00060\",\"AT4G00070\",\"AT4G00080\",\"AT4G00085\",\"AT4G00090\",\"AT4G00100\",\"AT4G00110\",\"AT4G00114\",\"AT4G00120\",\"AT4G00124\",\"AT4G00130\",\"AT4G00140\",\"AT5G01010\",\"AT5G01015\",\"AT5G01020\",\"AT5G01030\",\"AT5G01040\",\"AT5G01050\",\"AT5G01060\",\"AT5G01070\",\"AT5G01075\",\"AT5G01080\",\"AT5G01090\",\"AT5G01100\",\"AT5G01110\",\"AT5G01120\",\"AT5G01130\",\"AT5G01140\",\"AT5G01150\",\"AT5G01160\",\"ATCG00010\",\"ATCG00020\",\"ATCG00030\",\"ATCG00040\",\"ATCG00050\",\"ATCG00060\",\"ATCG00070\",\"ATCG00080\",\"ATCG00090\",\"ATCG00100\",\"ATCG00110\",\"ATCG00120\",\"ATCG00130\",\"ATCG00140\",\"ATCG00150\",\"ATCG00160\",\"ATCG00170\",\"ATCG00180\",\"ATCG00190\",\"ATCG00200\",\"ATCG00210\",\"ATCG00220\",\"ATCG00230\",\"ATCG00240\",\"ATCG00250\",\"ATCG00260\",\"ATCG00270\",\"ATCG00280\",\"ATCG00290\",\"ATCG00300\",\"ATCG00310\",\"ATCG00320\",\"ATCG00330\",\"ATCG00340\",\"ATCG00350\",\"ATCG00360\",\"ATCG00370\",\"ATCG00380\",\"ATCG00390\",\"ATCG00400\",\"ATCG00410\",\"ATCG00420\",\"ATCG00430\",\"ATCG00440\",\"ATCG00450\",\"ATCG00460\",\"ATCG00470\",\"ATCG00480\",\"ATCG00490\",\"ATCG00500\",\"ATCG00510\",\"ATMG00010\",\"ATMG00020\",\"ATMG00030\",\"ATMG00040\",\"ATMG00050\",\"ATMG00060\",\"ATMG00070\",\"ATMG00080\",\"ATMG00090\",\"ATMG00100\",\"ATMG00110\",\"ATMG00120\",\"ATMG00130\",\"ATMG00140\",\"ATMG00150\",\"ATMG00160\",\"ATMG00170\",\"ATMG00180\",\"ATMG00190\",\"ATMG00200\",\"NA\",\"NA.1\",\"NA.2\",\"NA.3\",\"NA.4\",\"NA.5\",\"NA.6\",\"NA.7\",\"NA.8\",\"NA.9\",\"NA.10\",\"NA.11\",\"NA.12\",\"NA.13\",\"NA.14\",\"NA.15\",\"NA.16\",\"NA.17\",\"NA.18\",\"NA.19\",\"NA.20\",\"NA.21\",\"NA.22\",\"NA.23\",\"NA.24\",\"NA.25\",\"NA.26\",\"NA.27\",\"NA.28\",\"NA.29\",\"NA.30\",\"NA.31\",\"NA.32\",\"NA.33\",\"NA.34\",\"NA.35\",\"NA.36\",\"NA.37\",\"NA.38\",\"NA.39\",\"NA.40\",\"NA.41\",\"NA.42\",\"NA.43\",\"NA.44\",\"NA.45\",\"NA.46\",\"NA.47\",\"NA.48\",\"NA.49\",\"NA.50\",\"NA.51\",\"NA.52\",\"NA.53\",\"NA.54\"],[286,104,120,911,23,189,98,0,0,377,1167,871,11,0,20,28,49,1,0,0,0,0,0,381,125,890,7,158,2,337,106,2490,247,10,10,170,108,141,0,13,76,237,249,70,280,0,0,0,93,656,64,0,0,0,0,1,608,57,308,111,181,10,1,1,21,0,73,523,13,2,0,0,0,101,24,7014,6,43,3,1,31,92,48,35,2,287,157,199,61,15,48,60,123,2,45,45,2,54,54,0,722,1231,39,38,1,0,173,746,800,20,4,20,5,27,5,38,77,38,30,1,263,718,3399,21,54,3,2112,276,17,5,10,25,28,56,0,21,4,7,5,1,31,0,2,0,3,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[260,136,109,727,12,178,262,1,0,390,1195,478,10,0,10,52,15,1,1,0,0,0,0,507,143,1025,12,40,0,340,157,2764,143,21,21,152,257,141,0,30,127,164,197,161,409,0,0,0,78,440,119,0,0,0,0,0,513,28,241,205,156,3,0,2,21,0,109,1298,30,2,0,0,0,155,11,1866,10,40,4,1,62,93,53,15,4,308,225,64,74,18,75,84,186,5,25,155,2,52,11,0,463,944,31,67,0,3,128,576,642,9,4,28,28,25,5,157,404,110,19,9,562,1750,1286,12,6,1,737,60,21,3,11,34,34,95,0,47,2,7,2,3,41,0,6,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[364,139,167,1030,17,247,86,5,0,363,1323,961,16,0,10,16,33,2,0,0,0,0,0,693,216,935,9,317,0,573,177,3926,265,5,5,324,220,285,0,14,149,421,416,68,369,0,0,0,205,851,159,0,0,0,0,3,1024,11,342,159,274,17,1,4,12,0,84,1466,21,2,0,0,0,235,16,1890,2,13,1,0,41,97,42,9,0,234,82,111,47,12,20,41,99,3,13,21,0,51,50,0,426,588,18,27,0,0,113,471,450,14,4,5,0,6,2,34,72,33,13,0,215,772,1710,11,14,1,1165,171,15,8,10,19,51,72,0,27,3,8,6,7,35,0,2,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[181,131,59,627,13,184,88,0,0,454,1157,385,9,0,4,33,2,5,0,0,0,0,0,528,170,580,6,16,0,349,134,2183,119,36,36,176,295,176,0,62,71,160,182,140,365,0,0,0,97,407,141,0,0,0,0,0,551,19,196,217,108,3,0,0,21,0,102,1168,29,3,0,0,0,152,11,611,4,25,4,0,86,105,62,8,0,190,236,69,95,18,97,81,240,0,28,109,0,50,5,0,266,687,14,68,1,2,144,618,606,9,3,19,23,26,0,169,363,90,5,6,628,1828,909,11,15,0,623,41,18,4,3,38,76,179,0,51,9,3,11,0,49,0,1,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[568,174,136,962,16,226,33,10,0,476,1151,649,8,0,18,51,57,1,0,0,0,0,0,577,238,393,5,74,2,603,187,2674,370,9,9,250,231,230,0,25,92,201,289,35,472,2,0,0,116,669,96,0,0,0,0,0,980,8,313,171,264,7,0,0,9,1,77,1342,36,3,0,0,0,182,17,3808,2,13,4,1,53,116,50,11,1,275,148,94,41,12,57,64,175,1,31,60,0,24,24,0,383,780,10,36,0,0,97,558,575,21,3,11,2,22,2,40,70,37,36,1,218,812,1729,11,25,1,918,94,13,3,16,36,28,75,0,15,3,5,6,5,42,0,7,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[300,156,192,918,26,380,32,5,0,630,2312,1121,35,0,10,55,58,0,0,0,0,0,0,921,192,477,6,114,0,483,240,2866,316,6,6,478,157,395,0,46,75,302,342,28,543,0,0,0,185,1177,168,0,0,0,0,0,1188,14,561,136,526,14,1,1,20,0,101,2719,43,7,0,0,0,286,5,1520,6,16,0,0,63,108,59,4,0,380,121,62,56,18,72,92,270,1,8,59,2,9,6,0,401,576,13,34,0,0,82,653,654,12,8,11,5,4,1,92,156,37,7,0,599,1615,1443,18,5,4,828,57,12,2,17,31,51,71,1,29,4,6,8,7,55,0,2,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[255,148,74,862,19,524,8,11,0,437,1821,2090,5,0,33,43,130,2,0,0,0,0,0,188,188,41,1,89,0,443,186,3407,397,68,68,272,199,179,0,25,262,443,555,346,433,1,0,0,256,1746,118,0,0,0,0,3,1148,99,820,116,13,0,0,1,29,0,130,147,23,2,0,0,0,235,7,8203,3,9,0,0,20,54,21,5,1,185,68,176,22,6,18,18,54,3,15,20,5,83,89,0,427,891,10,20,0,0,97,597,674,10,4,11,1,16,5,26,36,17,10,0,111,387,3002,16,10,5,3061,515,34,21,14,24,30,56,0,25,5,12,10,2,35,0,6,0,3,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[135,131,26,618,14,619,4,13,0,747,1904,2156,11,0,23,55,73,4,1,0,0,0,0,221,235,133,3,88,0,537,243,4187,425,54,54,293,227,168,0,42,221,351,426,150,469,1,0,0,273,1687,103,0,0,0,0,0,1191,225,734,102,16,1,0,0,19,0,158,344,12,2,0,0,0,198,6,4264,5,9,0,0,26,32,21,5,2,209,80,105,33,8,30,51,80,0,21,10,0,17,17,0,306,675,4,7,0,0,67,483,536,11,3,6,1,9,4,27,42,12,14,1,184,642,2624,5,11,5,2099,201,47,28,12,23,24,73,1,34,3,10,6,5,48,0,6,0,3,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[514,114,23,880,23,382,6,29,0,266,1019,2443,11,0,31,18,64,1,1,0,0,0,1,218,181,41,0,197,0,440,238,3760,528,43,43,304,351,233,0,31,312,501,555,520,443,5,0,1,313,2312,98,0,0,0,0,2,1558,20,696,194,63,13,0,2,13,0,125,643,9,2,0,0,0,303,3,3679,1,5,0,0,14,27,17,0,0,139,26,94,16,4,15,12,43,0,5,12,1,41,44,0,287,464,5,7,0,2,61,343,338,3,6,3,0,4,3,16,34,6,5,1,80,292,1497,5,4,3,1972,398,36,22,22,31,32,56,0,33,2,4,8,6,55,0,3,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[318,104,73,639,21,414,3,8,0,350,1286,1429,13,0,12,90,128,10,0,0,1,0,0,165,146,40,0,77,0,352,148,2609,415,29,29,232,178,143,0,43,183,314,345,115,356,3,0,0,208,1331,107,0,0,0,0,0,1101,79,562,101,55,11,0,0,13,0,83,345,15,2,0,0,0,231,3,11266,6,28,2,0,15,38,18,18,0,368,199,227,48,16,35,47,76,1,52,18,0,11,11,1,742,1615,13,26,0,1,181,1073,1169,16,14,23,2,44,5,37,48,20,42,1,143,569,6908,26,34,2,5857,351,5,3,18,17,27,48,0,20,0,4,2,1,38,0,2,0,2,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[757,206,118,1632,24,622,2,28,0,352,1464,2733,28,0,33,18,39,1,0,0,1,0,1,289,259,34,1,129,0,557,331,4121,690,49,49,292,377,261,0,30,340,575,748,405,547,1,0,0,456,2631,112,1,0,0,0,2,1688,9,1103,215,26,2,2,1,20,0,181,602,17,1,0,0,0,321,7,2800,1,10,0,2,12,51,27,6,0,133,37,76,22,5,24,22,44,0,12,9,1,30,31,0,281,493,6,8,0,1,43,318,316,6,0,1,1,5,1,16,22,10,19,1,93,320,1138,5,13,8,1692,289,22,14,11,13,40,59,1,28,1,7,12,4,61,0,2,0,2,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[551,212,214,1552,36,962,10,14,0,765,2319,2760,40,0,14,35,55,1,0,0,0,0,1,316,403,52,2,46,0,647,385,3672,545,85,85,348,348,158,0,45,302,486,632,243,604,3,0,0,409,2447,129,0,0,0,0,0,1651,74,1324,190,25,1,0,1,44,1,206,219,18,1,0,0,0,393,9,562,1,5,0,0,14,29,23,3,0,122,34,40,15,5,25,27,69,2,6,9,0,12,11,0,126,305,2,6,0,0,37,240,232,1,2,4,0,2,2,14,24,6,8,1,127,572,648,3,5,3,660,65,27,14,11,29,46,95,1,24,4,17,14,3,61,0,4,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[198,67,45,651,28,666,220,11,0,384,1354,1439,14,0,16,110,39,7,3,0,0,0,0,208,135,156,0,54,0,468,270,3439,297,67,67,232,111,214,0,27,106,420,598,781,451,3,0,0,173,1395,96,0,0,0,0,1,896,8,1261,124,4,0,0,1,23,0,102,136,6,2,0,0,2,313,2,2551,0,5,0,2,15,9,12,3,0,106,18,27,7,1,13,30,50,0,8,7,0,0,3,0,207,267,0,5,0,0,35,299,329,4,4,4,2,3,1,26,56,10,0,2,141,474,830,3,2,4,1084,149,22,16,7,20,29,35,0,17,1,3,7,4,55,0,2,0,0,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[248,156,51,1095,23,1355,317,65,0,1037,2555,2452,42,0,50,47,124,1,1,0,0,0,0,209,347,304,2,62,0,624,336,3236,561,131,131,284,170,144,0,62,189,720,992,842,738,4,0,0,308,2118,147,0,0,0,0,1,1427,100,1849,178,7,0,0,1,36,0,228,91,39,0,0,0,0,425,8,714,0,4,1,0,18,49,35,5,2,143,50,52,18,4,19,28,73,3,8,15,0,19,17,0,131,331,1,3,0,0,41,238,237,8,2,6,0,2,1,20,34,14,18,1,174,687,662,4,5,3,702,124,26,16,8,26,36,77,0,35,5,8,9,3,35,0,7,0,0,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[527,130,31,1324,33,737,501,64,0,343,1100,2604,17,0,81,60,124,2,2,0,0,0,0,324,304,160,3,276,0,611,463,3093,767,73,73,279,203,292,0,39,226,615,817,933,639,13,0,0,386,2257,111,0,0,0,0,2,2050,23,1034,211,8,2,0,1,9,0,143,370,18,5,0,0,0,433,10,4167,2,10,0,0,10,51,25,6,0,119,48,91,23,9,12,24,49,0,12,14,0,35,33,0,319,619,0,9,0,0,65,434,506,14,3,8,0,6,3,16,34,11,12,1,111,369,1579,10,8,8,2356,567,35,19,22,18,35,49,0,26,3,12,16,5,59,0,4,0,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[417,120,48,702,13,532,198,39,0,299,941,812,26,0,16,137,139,8,1,0,0,0,0,260,253,55,1,115,0,376,219,1566,464,46,46,148,138,166,0,24,146,209,288,144,401,1,0,0,187,864,63,0,0,0,0,1,1258,17,660,123,8,0,0,0,12,0,65,174,16,2,0,0,0,238,3,13987,7,44,2,0,54,60,32,19,3,365,232,158,67,7,30,50,59,2,57,39,0,4,5,0,959,2153,21,23,0,0,208,1290,1195,19,5,32,6,82,5,43,74,18,51,0,195,746,5819,38,23,3,4738,331,12,10,19,21,31,65,0,23,0,3,7,2,59,0,2,0,2,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[650,80,177,671,23,635,164,23,0,267,1188,1546,16,0,31,131,108,4,2,0,0,0,0,318,148,149,1,136,0,405,248,2150,319,44,44,232,109,182,0,15,135,305,353,517,357,1,0,0,256,1623,93,1,0,0,0,4,920,2,901,136,11,1,0,0,15,0,91,478,9,1,0,0,1,327,3,4184,1,13,0,0,21,18,28,3,2,110,36,47,25,6,18,18,45,0,3,8,0,2,1,0,272,409,3,7,0,1,50,292,313,5,0,4,0,3,2,29,62,15,3,1,156,501,1138,7,0,8,1840,179,14,6,14,28,17,41,0,17,4,5,12,1,47,0,1,0,3,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[671,158,442,995,20,1004,159,24,0,373,1511,2145,33,0,27,113,125,3,2,0,0,0,0,321,231,111,2,92,0,464,211,1486,560,40,40,231,198,130,0,16,178,356,338,302,379,0,0,0,287,2106,65,0,0,0,0,1,994,27,1103,121,4,0,0,0,39,2,119,372,16,1,0,0,0,414,6,7310,3,9,3,0,30,51,35,17,1,205,101,173,34,8,22,32,56,2,37,33,0,24,25,0,434,1036,8,12,0,0,107,579,630,17,3,10,1,30,1,23,41,16,30,2,151,549,3176,9,21,6,2986,323,14,4,17,30,28,52,0,21,2,5,9,3,38,0,2,0,3,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]],\"container\":\"\\n \\n \\n  \\n M1A\\n M1B\\n A1A\\n A1B\\n V1A\\n V1B\\n M6A\\n M6B\\n A6A\\n A6B\\n V6A\\n V6B\\n M12A\\n M12B\\n A12A\\n A12B\\n V12A\\n V12B\\n \\n \\n\",\"options\":{\"scrollX\":true,\"autoWidth\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} A data slice of RPKM table (rpkmDFeByg.xls) is shown here.\nread.delim(\"results/rpkmDFeByg.xls\", row.names = 1, check.names = FALSE)[1:4, 1:4]  ## M1A M1B A1A A1B ## AT1G01010 5179.326 5955.322 7558.353 4856.097 ## AT1G01020 1792.088 2964.078 2746.372 3344.252 ## AT1G01030 1925.599 2212.258 3072.697 1402.614 ## AT1G01040 4452.871 4494.494 5772.681 4540.367  Note, for most statistical differential expression or abundance analysis methods, such as edgeR or DESeq2, the raw count values should be used as input. The usage of RPKM values should be restricted to specialty applications required by some users, e.g. manually comparing the expression levels among different genes or features.\nSample-wise correlation analysis The following computes the sample-wise Spearman correlation coefficients from the rlog transformed expression values generated with the DESeq2 package. After transformation to a distance matrix, hierarchical clustering is performed with the hclust function and the result is plotted as a dendrogram (also see file sample_tree.pdf).\nappendStep(sal) \u003c- LineWise(code = { library(DESeq2, quietly = TRUE) library(ape, warn.conflicts = FALSE) ## Extracting SummarizedExperiment object se \u003c- SE(sal, \"read_counting\") dds \u003c- DESeqDataSet(se, design = ~condition) d \u003c- cor(assay(rlog(dds)), method = \"spearman\") hc \u003c- hclust(dist(1 - d)) pdf(\"results/sample_tree.pdf\") plot.phylo(as.phylo(hc), type = \"p\", edge.col = \"blue\", edge.width = 2, show.node.label = TRUE, no.margin = TRUE) dev.off() }, step_name = \"sample_tree\", dependency = \"read_counting\")  Figure 2: Correlation dendrogram of samples\n  Analysis of DEGs The analysis of differentially expressed genes (DEGs) is performed with the glm method of the edgeR package (Robinson, McCarthy, and Smyth 2010). The sample comparisons used by this analysis are defined in the header lines of the targets.txt file starting with \u003cCMP\u003e.\nRun edgeR appendStep(sal) \u003c- LineWise(code = { library(edgeR) countDF \u003c- read.delim(\"results/countDFeByg.xls\", row.names = 1, check.names = FALSE) cmp \u003c- readComp(stepsWF(sal)[[\"hisat2_mapping\"]], format = \"matrix\", delim = \"-\") edgeDF \u003c- run_edgeR(countDF = countDF, targets = targetsWF(sal)[[\"hisat2_mapping\"]], cmp = cmp[[1]], independent = FALSE, mdsplot = \"\") }, step_name = \"run_edger\", dependency = \"read_counting\")  Add gene descriptions appendStep(sal) \u003c- LineWise(code = { library(\"biomaRt\") m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"https://plants.ensembl.org\") desc \u003c- getBM(attributes = c(\"tair_locus\", \"description\"), mart = m) desc \u003c- desc[!duplicated(desc[, 1]), ] descv \u003c- as.character(desc[, 2]) names(descv) \u003c- as.character(desc[, 1]) edgeDF \u003c- data.frame(edgeDF, Desc = descv[rownames(edgeDF)], check.names = FALSE) write.table(edgeDF, \"./results/edgeRglm_allcomp.xls\", quote = FALSE, sep = \"\\t\", col.names = NA) }, step_name = \"custom_annot\", dependency = \"run_edger\")  Plot DEG results Filter and plot DEG results for up and down regulated genes. The definition of up and down is given in the corresponding help file. To open it, type ?filterDEGs in the R console.\nappendStep(sal) \u003c- LineWise(code = { edgeDF \u003c- read.delim(\"results/edgeRglm_allcomp.xls\", row.names = 1, check.names = FALSE) pdf(\"results/DEGcounts.pdf\") DEG_list \u003c- filterDEGs(degDF = edgeDF, filter = c(Fold = 2, FDR = 20)) dev.off() write.table(DEG_list$Summary, \"./results/DEGcounts.xls\", quote = FALSE, sep = \"\\t\", row.names = FALSE) }, step_name = \"filter_degs\", dependency = \"custom_annot\")  Figure 3: Up and down regulated DEGs with FDR of 1%\n  Venn diagrams of DEG sets The overLapper function can compute Venn intersects for large numbers of sample sets (up to 20 or more) and plots 2-5 way Venn diagrams. A useful feature is the possibility to combine the counts from several Venn comparisons with the same number of sample sets in a single Venn diagram (here for 4 up and down DEG sets).\nappendStep(sal) \u003c- LineWise(code = { vennsetup \u003c- overLapper(DEG_list$Up[6:9], type = \"vennsets\") vennsetdown \u003c- overLapper(DEG_list$Down[6:9], type = \"vennsets\") pdf(\"results/vennplot.pdf\") vennPlot(list(vennsetup, vennsetdown), mymain = \"\", mysub = \"\", colmode = 2, ccol = c(\"blue\", \"red\")) dev.off() }, step_name = \"venn_diagram\", dependency = \"filter_degs\")  Figure 4: Venn Diagram for 4 Up and Down DEG Sets\n  GO term enrichment analysis Obtain gene-to-GO mappings The following shows how to obtain gene-to-GO mappings from biomaRt (here for A. thaliana) and how to organize them for the downstream GO term enrichment analysis. Alternatively, the gene-to-GO mappings can be obtained for many organisms from Bioconductor’s *.db genome annotation packages or GO annotation files provided by various genome databases. For each annotation this relatively slow preprocessing step needs to be performed only once. Subsequently, the preprocessed data can be loaded with the load function as shown in the next subsection.\nappendStep(sal) \u003c- LineWise(code = { library(\"biomaRt\") # listMarts() # To choose BioMart database # listMarts(host='plants.ensembl.org') m \u003c- useMart(\"plants_mart\", host = \"https://plants.ensembl.org\") m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"https://plants.ensembl.org\") go \u003c- getBM(attributes = c(\"go_id\", \"tair_locus\", \"namespace_1003\"), mart = m) go \u003c- go[go[, 3] != \"\", ] go[, 3] \u003c- as.character(go[, 3]) go[go[, 3] == \"molecular_function\", 3] \u003c- \"F\" go[go[, 3] == \"biological_process\", 3] \u003c- \"P\" go[go[, 3] == \"cellular_component\", 3] \u003c- \"C\" go[1:4, ] if (!dir.exists(\"./data/GO\")) dir.create(\"./data/GO\") write.table(go, \"data/GO/GOannotationsBiomart_mod.txt\", quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\") catdb \u003c- makeCATdb(myfile = \"data/GO/GOannotationsBiomart_mod.txt\", lib = NULL, org = \"\", colno = c(1, 2, 3), idconv = NULL) save(catdb, file = \"data/GO/catdb.RData\") }, step_name = \"get_go_annot\", dependency = \"filter_degs\")  Batch GO term enrichment analysis Apply the enrichment analysis to the DEG sets obtained the above differential expression analysis. Note, in the following example the FDR filter is set here to an unreasonably high value, simply because of the small size of the toy data set used in this vignette. Batch enrichment analysis of many gene sets is performed with the function. When method=all, it returns all GO terms passing the p-value cutoff specified under the cutoff arguments. When method=slim, it returns only the GO terms specified under the myslimv argument. The given example shows how a GO slim vector for a specific organism can be obtained from BioMart.\nappendStep(sal) \u003c- LineWise(code = { library(\"biomaRt\") load(\"data/GO/catdb.RData\") DEG_list \u003c- filterDEGs(degDF = edgeDF, filter = c(Fold = 2, FDR = 50), plot = FALSE) up_down \u003c- DEG_list$UporDown names(up_down) \u003c- paste(names(up_down), \"_up_down\", sep = \"\") up \u003c- DEG_list$Up names(up) \u003c- paste(names(up), \"_up\", sep = \"\") down \u003c- DEG_list$Down names(down) \u003c- paste(names(down), \"_down\", sep = \"\") DEGlist \u003c- c(up_down, up, down) DEGlist \u003c- DEGlist[sapply(DEGlist, length) \u003e 0] BatchResult \u003c- GOCluster_Report(catdb = catdb, setlist = DEGlist, method = \"all\", id_type = \"gene\", CLSZ = 2, cutoff = 0.9, gocats = c(\"MF\", \"BP\", \"CC\"), recordSpecGO = NULL) m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"https://plants.ensembl.org\") goslimvec \u003c- as.character(getBM(attributes = c(\"goslim_goa_accession\"), mart = m)[, 1]) BatchResultslim \u003c- GOCluster_Report(catdb = catdb, setlist = DEGlist, method = \"slim\", id_type = \"gene\", myslimv = goslimvec, CLSZ = 10, cutoff = 0.01, gocats = c(\"MF\", \"BP\", \"CC\"), recordSpecGO = NULL) write.table(BatchResultslim, \"results/GOBatchSlim.xls\", row.names = FALSE, quote = FALSE, sep = \"\\t\") }, step_name = \"go_enrich\", dependency = \"get_go_annot\")  Shows GO term enrichment results from previous step. The last gene identifier column (10) of this table has been excluded in this viewing instance to minimize the complexity of the result. To avoid slowdowns of the load time of this page, only 10 rows of the source table are shown below.\nBatchResult \u003c- read.delim(\"results/GOBatchAll.xls\")[1:10, ] knitr::kable(BatchResult[, -10])     CLID CLSZ GOID NodeSize SampleMatch Phyper Padj Term Ont     M1-A1_up_down 26 GO:0050291 4 1 0.0039621 0.0396207 sphingosine N-acyltransferase activity MF   M1-A1_up_down 26 GO:0004345 6 1 0.0059375 0.0593750 glucose-6-phosphate dehydrogenase activity MF   M1-A1_up_down 26 GO:0050664 11 1 0.0108597 0.1085975 oxidoreductase activity, acting on NAD(P)H, oxygen as acceptor MF   M1-A1_up_down 26 GO:0052593 11 1 0.0108597 0.1085975 tryptamine:oxygen oxidoreductase (deaminating) activity MF   M1-A1_up_down 26 GO:0052594 11 1 0.0108597 0.1085975 aminoacetone:oxygen oxidoreductase(deaminating) activity MF   M1-A1_up_down 26 GO:0052595 11 1 0.0108597 0.1085975 aliphatic-amine oxidase activity MF   M1-A1_up_down 26 GO:0052596 11 1 0.0108597 0.1085975 phenethylamine:oxygen oxidoreductase (deaminating) activity MF   M1-A1_up_down 26 GO:0052793 12 1 0.0118414 0.1184141 pectin acetylesterase activity MF   M1-A1_up_down 26 GO:0008131 15 1 0.0147808 0.1478083 primary amine oxidase activity MF   M1-A1_up_down 26 GO:0016018 16 1 0.0157588 0.1575878 cyclosporin A binding MF    Plot batch GO term results The data.frame generated by GOCluster can be plotted with the goBarplot function. Because of the variable size of the sample sets, it may not always be desirable to show the results from different DEG sets in the same bar plot. Plotting single sample sets is achieved by subsetting the input data frame as shown in the first line of the following example.\nappendStep(sal) \u003c- LineWise(code = { gos \u003c- BatchResultslim[grep(\"M6-V6_up_down\", BatchResultslim$CLID), ] gos \u003c- BatchResultslim png(\"results/GOslimbarplotMF.png\") goBarplot(gos, gocat = \"MF\") dev.off() png(\"results/GOslimbarplotBP.png\") goBarplot(gos, gocat = \"BP\") dev.off() png(\"results/GOslimbarplotCC.png\") goBarplot(gos, gocat = \"CC\") dev.off() }, step_name = \"go_plot\", dependency = \"go_enrich\")  Figure 5: GO Slim Barplot for MF Ontology\n  Clustering and heat maps The following example performs hierarchical clustering on the rlog transformed expression matrix subsetted by the DEGs identified in the above differential expression analysis. It uses a Pearson correlation-based distance measure and complete linkage for cluster joining.\nappendStep(sal) \u003c- LineWise(code = { library(pheatmap) geneids \u003c- unique(as.character(unlist(DEG_list[[1]]))) y \u003c- assay(rlog(dds))[geneids, ] pdf(\"results/heatmap1.pdf\") pheatmap(y, scale = \"row\", clustering_distance_rows = \"correlation\", clustering_distance_cols = \"correlation\") dev.off() }, step_name = \"heatmap\", dependency = \"go_enrich\")  Figure 6: Heat Map with Hierarchical Clustering Dendrograms of DEGs\n  Version Information appendStep(sal) \u003c- LineWise(code = { sessionInfo() }, step_name = \"sessionInfo\", dependency = \"heatmap\")  Running workflow Interactive job submissions in a single machine For running the workflow, runWF function will execute all the steps store in the SYSargsList workflow container. The execution will be on a single machine without submitting to a queuing system of a computer cluster. Besides, runWF allows the user to create a dedicated results folder for each workflow. This includes all the output and log files for each step. When these options are used, the output location will be updated by default and can be assigned to the same object.\nsal \u003c- runWF(sal)  Parallelization on clusters Alternatively, the computation can be greatly accelerated by processing many files in parallel using several compute nodes of a cluster, where a scheduling/queuing system is used for load balancing.\nThe resources list object provides the number of independent parallel cluster processes defined under the Njobs element in the list. The following example will run 18 processes in parallel using each 4 CPU cores. If the resources available on a cluster allow running all 18 processes at the same time, then the shown sample submission will utilize in a total of 72 CPU cores.\nNote, runWF can be used with most queueing systems as it is based on utilities from the batchtools package, which supports the use of template files (*.tmpl) for defining the run parameters of different schedulers. To run the following code, one needs to have both a conffile (see .batchtools.conf.R samples here) and a template file (see *.tmpl samples here) for the queueing available on a system. The following example uses the sample conffile and template files for the Slurm scheduler provided by this package.\nThe resources can be appended when the step is generated, or it is possible to add these resources later, as the following example using the addResources function:\nresources \u003c- list(conffile=\".batchtools.conf.R\", template=\"batchtools.slurm.tmpl\", Njobs=18, walltime=120, ## minutes ntasks=1, ncpus=4, memory=1024, ## Mb partition = \"short\" ) sal \u003c- addResources(sal, step = c(\"hisat2_mapping\"), resources = resources) sal \u003c- runWF(sal)  Visualize workflow systemPipeR workflows instances can be visualized with the plotWF function.\nplotWF(sal, out_format = \"html\", out_path = \"plotWF.html\")   {\"x\":{\"dot\":\"digraph {\\n node[fontsize=20];\\n subgraph {\\n load_SPR - hisat2_index - hisat2_mapping - create_db - read_counting - run_edger - custom_annot - filter_degs - get_go_annot - go_enrich - heatmap - sessionInfo\\n }\\n load_SPR - preprocessing\\n preprocessing - hisat2_mapping\\n hisat2_mapping - align_stats\\n hisat2_mapping - bam_urls\\n read_counting - sample_tree\\n filter_degs - venn_diagram\\n go_enrich - go_plot\\n preprocessing - fastq_report\\n load_SPR[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=load_SPR\n0/0/0/1; 0s tooltip=\\\"step load_SPR: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n preprocessing[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=preprocessing\n0/0/0/36; 0s , shape=\\\"box\\\" tooltip=\\\"step preprocessing: 0 samples passed; 0 samples have warnings; 0 samples have errors; 36 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n fastq_report[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=fastq_report\n0/0/0/1; 0s tooltip=\\\"step fastq_report: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n hisat2_index[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=hisat2_index\n0/0/0/8; 0s , shape=\\\"box\\\" tooltip=\\\"step hisat2_index: 0 samples passed; 0 samples have warnings; 0 samples have errors; 8 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n hisat2_mapping[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=hisat2_mapping\n0/0/0/72; 0s , shape=\\\"box\\\" tooltip=\\\"step hisat2_mapping: 0 samples passed; 0 samples have warnings; 0 samples have errors; 72 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n align_stats[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=align_stats\n0/0/0/1; 0s tooltip=\\\"step align_stats: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n bam_urls[style=\\\"solid, \\\"label=bam_urls\n0/0/0/1; 0s tooltip=\\\"step bam_urls: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n create_db[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=create_db\n0/0/0/1; 0s tooltip=\\\"step create_db: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n read_counting[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=read_counting\n0/0/0/1; 0s tooltip=\\\"step read_counting: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n sample_tree[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=sample_tree\n0/0/0/1; 0s tooltip=\\\"step sample_tree: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n run_edger[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=run_edger\n0/0/0/1; 0s tooltip=\\\"step run_edger: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n custom_annot[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=custom_annot\n0/0/0/1; 0s tooltip=\\\"step custom_annot: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n filter_degs[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=filter_degs\n0/0/0/1; 0s tooltip=\\\"step filter_degs: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n venn_diagram[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=venn_diagram\n0/0/0/1; 0s tooltip=\\\"step venn_diagram: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n get_go_annot[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=get_go_annot\n0/0/0/1; 0s tooltip=\\\"step get_go_annot: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n go_enrich[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=go_enrich\n0/0/0/1; 0s tooltip=\\\"step go_enrich: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n go_plot[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=go_plot\n0/0/0/1; 0s tooltip=\\\"step go_plot: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n heatmap[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=heatmap\n0/0/0/1; 0s tooltip=\\\"step heatmap: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n sessionInfo[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=sessionInfo\n0/0/0/1; 0s tooltip=\\\"step sessionInfo: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-25 13:50:56; End time: 2022-04-25 13:50:56; Duration: 00:00:00\\\"]\\n subgraph cluster_legend {\\n rankdir=TB;\\n color=\\\"#eeeeee\\\";\\n style=filled;\\n ranksep =1;\\n label=\\\"Legends\\\";\\n fontsize = 30;\\n node [style=filled, fontsize=10];\\n legend_img- step_state[color=\\\"#eeeeee\\\"];\\n\\n legend_img[shape=none, image=\\\"plotwf_legend-src.png\\\", label = \\\" \\\", height=1, width=3, style=\\\"\\\"];\\n\\n step_state[style=\\\"filled\\\", shape=\\\"box\\\" color=white, label =\\n Step Colors\\n Pending steps; Successful steps; Failed steps\\n Targets Files / Code Chunk 0 (pass)  | 0 (warning)  | 0 (error)  | 0 (total); Duration\\n ];\\n\\n }\\n\\n}\\n\",\"plotid\":\"sprwf-43175286\",\"responsive\":true,\"width\":null,\"height\":null,\"plot_method\":\"renderSVGElement\",\"rmd\":true,\"msg\":\"\",\"plot_ctr\":true,\"pan_zoom\":false,\"legend_uri\":\"data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!-- Do not edit this file with editors other than diagrams.net -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="496px" height="278px" viewBox="-0.5 -0.5 496 278" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2021-11-24T20:39:44.923Z&quot; agent=&quot;5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&quot; version=&quot;15.8.3&quot; etag=&quot;T_dExnL7SLX2buX8PaX3&quot; type=&quot;google&quot;&gt;&lt;diagram id=&quot;Vw9VViGsx-sL_NjKe7JP&quot;&gt;7VrbcuI4EP0aHpOyJd94DARmXmZ3K2zVPitYGFVkyWPLgczXr2TL+CID3sGQTRUkldgtqS2d02p1t5nAebz/lqJk+4OHmE6AFe4n8HkCgA0AmKhfK/zQEsvxS0mUklDLasGK/MJVRy3NSYizVkfBORUkaQvXnDG8Fi0ZSlO+a3fbcNp+aoIibAhWa0RN6T8kFNtSOrWsWv4dk2hbPdmrWmJUddaCbItCvmuI4GIC5ynnoryK93NMFXoVLuW45ZHWw8RSzMSQAZqJd0RzvTY9L/FRLTZKeZ5M4Ez+YyFW4yx5d5i4utFKcCrwvo8A9Fops8wJ2odlS4PBPMYi/ZBdKkWBHqJtBUB9v6uBB45byrZN0KGnCddkRwfdNR7yQkPSDw88D08blt2WCLxK0Fq17qT5S9lWxFL/sy0vN4TSOac8LcbC0MVB6Cg0RcrfcNXCOJPDZxFFWdYHd/aGxXp7DvsmxqAfY42pA4dB6oyAqHNdRJfLRfC8OIboVVEMpgNR9C9H0b0uiovlEiznt0XxLGzwcti8sWHjTOjTKTBR3LjqR8oRJRGTMoo34hioSlVjbPmRci4fToQysWBEhH3rSg7TNxDOOJWNXZjl9EUbywqktVwGlijM1CKJPHOfdENMwlANn6U4I7/0kaIwSThhopizO5u4z0pXLnhW8mIfBbxBVoODJp3ydoliQhX6f5NYRhzA+gPv5N8XHiM2lI7TBxzw/Ue3fcQB05PAHsKqw/QSvgKDrxBlW3wn7Dhh0HUNwgKDMOdKhE0Nwn4gJuPNWM3+q5Fm27fbZm7w6Hdoc81Qss8xjkFblbk0eJvzOMkFvpN2Yqs5xlbr4cy7Fmd2T7jg0eIIT+QSm6x5P3OVshW4PJT4PskOQbKv2+RVpP7Lp8hEFVe65DRKdWWrYQ9V9/8UxIV4g3Iqbre9vKDLlO8ZTLn2Y09kB0YIiG0zkX2576wTfAV2hy9o9WSB19pZZl4tvaFcXPhACbu7xFMuEXZdIrRsg7jgWsSZ6Xuq6jssoiZrdSplX+K8PiHAA0N8Gej1Ze4IIA/I7jELn1QhszbHBpadSl1hz1WdEh4wwqFR5DyLUHP9PSZWyVJMkSDvbfV9kOgn/KX2ZPMwmT6CDgPdqknG83SN9UDQKHB2dEFgG3FfV5dAaYSFoavg6bD4YdQNqDBcRJ3OzsrO/3ceZabUAd57DKxp/XF/j1PHMlLmM5pHZNiscMgELESC601+P7eOBIhdpwod06leLZQ36xx/JoJwhuidtFMnoTWAtD4PMgppZq1jlPxLwTs0+bro7Q4MPfx6quB7i+pi+/UZBGaU7/i9kcwIDAKz6jEKg4zLllvRuCw+n0ujGZLemEizFPKSM0ZYNFHqSxJe0yGE2pZktKCyQ+oKZ5n0yF/IH+s5dt4E2dbNzMIOgnYYBL3KAs74ZziGUZg1l8oUQvJ+kS2c9RKvaP0WFbv6YV0yo/SJFLGsgm+mfMTA5x2M+Yg7OYiLhekx+GdO0q9WeP98m/XbNguBGVH0VeFHsViz7rQSOBnPh82Lr2zcrWG4NUxh24MFZjGr71Xab1iDvK2/V1VmfvXX0+DiXw==&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g><rect x="4" y="90" width="490" height="92" fill="#d5e8d4" stroke="none" pointer-events="none"/><rect x="4" y="182" width="490" height="94" fill="#ffe8de" stroke="none" pointer-events="none"/><rect x="4" y="4" width="490" height="86" fill="#eff2fc" stroke="none" pointer-events="none"/><rect x="4" y="4" width="140" height="272" fill-opacity="0.8" fill="#f5f5f5" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 11px; margin-left: 115px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">solid</div></div></div></foreignObject><text x="115" y="13" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">solid</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 10px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">dashed</div></div></div></foreignObject><text x="198" y="12" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">dashed</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 32px; margin-left: 116px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Management</div></div></div></foreignObject><text x="116" y="35" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Management</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 32px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Compute</div></div></div></foreignObject><text x="198" y="35" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Compute</text></switch></g><ellipse cx="232.5" cy="123" rx="51.5" ry="27" fill="rgba(255, 255, 255, 1)" stroke="rgba(0, 0, 0, 1)" stroke-width="2" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 50px; height: 1px; padding-top: 62px; margin-left: 92px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">ellipse</span></div></div></div></foreignObject><text x="116" y="65" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">ellipse</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 85px; margin-left: 114px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">R</div></div></div></foreignObject><text x="114" y="88" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">R</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 83px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Command-line</div></div></div></foreignObject><text x="198" y="86" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Command-line</text></switch></g><rect x="349" y="96" width="105" height="50" rx="7.5" ry="7.5" fill="rgba(255, 255, 255, 1)" stroke="rgba(0, 0, 0, 1)" stroke-width="2" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 51px; height: 1px; padding-top: 61px; margin-left: 176px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;">rectangle</div></div></div></foreignObject><text x="201" y="63" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">rectangle</text></switch></g><path d="M 182.5 38 L 287.5 38" fill="none" stroke="rgba(0, 0, 0, 1)" stroke-width="6" stroke-miterlimit="10" pointer-events="none"/><path d="M 354 37.62 L 459 37.62" fill="none" stroke="rgba(0, 0, 0, 1)" stroke-width="6" stroke-miterlimit="10" stroke-dasharray="18 18" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 128px; margin-left: 115px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Mandatory</div></div></div></foreignObject><text x="115" y="131" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Mandatory</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 128px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Optional</div></div></div></foreignObject><text x="198" y="131" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Optional</text></switch></g><rect x="184" y="190" width="95" height="40" fill="#d3d6eb" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 46px; height: 1px; padding-top: 105px; margin-left: 93px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">fill</span></div></div></div></foreignObject><text x="116" y="109" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">fill</text></switch></g><rect x="349" y="190" width="95" height="40" fill="#ffffff" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 46px; height: 1px; padding-top: 105px; margin-left: 176px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">no fill</span></div></div></div></foreignObject><text x="198" y="109" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">no fill</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 24px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;">Running <br style="font-size: 10px" />Session</div></div></div></foreignObject><text x="35" y="27" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">Running...</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 113px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;"><div style="font-size: 10px"><span style="background-color: transparent ; font-size: 10px">Running</span></div>Requirement</div></div></div></foreignObject><text x="35" y="116" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">RunningRequire...</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 68px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;">Step <br style="font-size: 10px" />Class</div></div></div></foreignObject><text x="35" y="71" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">Step...</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Viewer does not support full SVG 1.1</text></a></switch></svg>\"},\"evals\":[],\"jsHooks\":[]} Checking workflow status To check the summary of the workflow, we can use:\nsal statusWF(sal)  Technical report systemPipeR compiles all the workflow execution logs in one central location, making it easier to check any standard output (stdout) or standard error (stderr) for any command-line tools used on the workflow or the R code stdout.\nsal \u003c- renderLogs(sal)  Scientific report systemPipeR auto-generates scientific analysis reports in HTML format.\nsal \u003c- renderReport(sal)  Alternatively, scientific reports can be rendered with the render function from rmarkdown.\nrmarkdown::render(\"systemPipeRNAseq.Rmd\", clean = TRUE, output_format = \"BiocStyle::html_document\")  Funding This project is funded by NSF award ABI-1661152.\nReferences H Backman, Tyler W, and Thomas Girke. 2016. “systemPipeR: NGS workflow and report generation environment.” BMC Bioinformatics 17 (1): 388. https://doi.org/10.1186/s12859-016-1241-0.\n Howard, Brian E, Qiwen Hu, Ahmet Can Babaoglu, Manan Chandra, Monica Borghi, Xiaoping Tan, Luyan He, et al. 2013. “High-Throughput RNA Sequencing of Pseudomonas-Infected Arabidopsis Reveals Hidden Transcriptome Complexity and Novel Splice Variants.” PLoS One 8 (10): e74183. https://doi.org/10.1371/journal.pone.0074183.\n Kim, Daehwan, Ben Langmead, and Steven L Salzberg. 2015. “HISAT: A Fast Spliced Aligner with Low Memory Requirements.” Nat. Methods 12 (4): 357–60.\n Lawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Comput. Biol. 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n Robinson, M D, D J McCarthy, and G K Smyth. 2010. “EdgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\n  ","categories":"","description":"","excerpt":"               pre code { white-space: pre !important; overflow-x: …","ref":"/tutorials/sprnaseq/sprnaseq/","tags":"","title":"RNA-Seq Workflow Template"},{"body":"               pre code { white-space: pre !important; overflow-x: scroll !important; word-break: keep-all !important; word-wrap: initial !important; }  document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: [ .Rmd ] [ .R ]\n Introduction The following analyzes the ChIP-Seq data from Kaufman et al. (2010) using for peak calling MACS2 where the uninduced sample serves as input (reference). Prior to running this analysis the corresponding FASTQ files need to be downloaded following the instructions here.\nFor learning purposes one can use the much smaller toy data set and ChIP-Seq Rmd workflow instance provided by the systemPipeRdata package. This toy workflow instance can be conveniently obtained by running genWorkenvir(workflow='chipseq') (see below). The FASTQ data used for this toy instance are the same as for the RNA-Seq workflow, but the analysis code in the Rmd file is almost identical to the ChIP-Seq workflow below.\nFor the analysis of the Kaufman et al. (2010) data set (see download here) users want to use the Rmd instance linked from the top right corner of this page. Additional detail about this is provided here.\nExperimental design Typically, users want to specify here all information relevant for the analysis of their NGS study. This includes detailed descriptions of FASTQ files, experimental design, reference genome, gene annotations, etc.\nWorkflow environment NOTE: this section describes how to set up the proper environment (directory structure) for running systemPipeR workflows. After mastering this task the workflow run instructions can be deleted since they are not expected to be included in a final HTML/PDF report of a workflow.\n  If a remote system or cluster is used, then users need to log in to the remote system first. The following applies to an HPC cluster (e.g. HPCC cluster).\nA terminal application needs to be used to log in to a user’s cluster account. Next, one can open an interactive session on a computer node with srun --x11. More details about argument settings for srun are available in this HPCC manual or the HPCC section of this website here. Next, load the R version required for running the workflow with module load. Sometimes it may be necessary to first unload an active software version before loading another version, e.g. module unload R.\n  Experimental design Typically, users want to specify here all information relevant for the analysis of their NGS study. This includes detailed descriptions of FASTQ files, experimental design, reference genome, gene annotations, etc.\nWorkflow environment NOTE: this section describes how to set up the proper environment (directory structure) for running systemPipeR workflows. After mastering this task the workflow run instructions can be deleted since they are not expected to be included in a final HTML/PDF report of a workflow.\n  If a remote system or cluster is used, then users need to log in to the remote system first. The following applies to an HPC cluster (e.g. HPCC cluster).\nA terminal application needs to be used to log in to a user’s cluster account. Next, one can open an interactive session on a computer node with srun --x11. More details about argument settings for srun are available in this HPCC manual or the HPCC section of this website here. Next, load the R version required for running the workflow with module load. Sometimes it may be necessary to first unload an active software version before loading another version, e.g. module unload R.\n  srun --x11 --partition=gen242 --mem=20gb --cpus-per-task 8 --ntasks 1 --time 20:00:00 --pty bash -l module unload R; module load R/4.1.2  Load a workflow template with the genWorkenvir function. This can be done from the command-line or from within R. However, only one of the two options needs to be used.  From command-line\n$ Rscript -e \"systemPipeRdata::genWorkenvir(workflow='chipseq')\" $ cd chipseq  From R\nlibrary(systemPipeRdata) genWorkenvir(workflow = \"chipseq\") setwd(\"chipseq\")   Optional: if the user wishes to use another Rmd file than the template instance provided by the genWorkenvir function, then it can be copied or downloaded into the root directory of the workflow environment (e.g. with cp or wget).\n  Now one can open from the root directory of the workflow the corresponding R Markdown script (e.g. systemPipeChIPseq.Rmd) using an R IDE, such as nvim-r, ESS or RStudio. Subsequently, the workflow can be run as outlined below. For learning purposes it is recommended to run workflows for the first time interactively. Once all workflow steps are understood and possibly modified to custom needs, one can run the workflow from start to finish with a single command using runWF().\n  Load packages The systemPipeR package needs to be loaded to perform the analysis steps shown in this report (H Backman and Girke 2016). The package allows users to run the entire analysis workflow interactively or with a single command while also generating the corresponding analysis report. For details see systemPipeR's main vignette.\nlibrary(systemPipeR)  To apply workflows to custom data, the user needs to modify the targets file and if necessary update the corresponding parameter (.cwl and .yml) files. A collection of pre-generated .cwl and .yml files are provided in the param/cwl subdirectory of each workflow template. They are also viewable in the GitHub repository of systemPipeRdata (see here). For more information of the structure of the targets file, please consult the documentation here. More details about the new parameter files from systemPipeR can be found here.\nImport custom functions Custom functions for the challenge projects can be imported with the source command from a local R script (here challengeProject_Fct.R). Skip this step if such a script is not available. Alternatively, these functions can be loaded from a custom R package.\nsource(\"challengeProject_Fct.R\")  Experiment definition provided by targets file The targets file defines all FASTQ files and sample comparisons of the analysis workflow. If needed the tab separated (TSV) version of this file can be downloaded from here and the corresponding Google Sheet is here.\ntargetspath \u003c- \"targets_chipseq.txt\" targets \u003c- read.delim(targetspath, comment.char = \"#\") knitr::kable(targets)     FileName SampleName Factor SampleLong Experiment Date SampleReference     ./data/SRR038845_1.fastq.gz AP1_1 AP1 APETALA1 Induced 1 23-Mar-12    ./data/SRR038846_1.fastq.gz AP1_2A AP1 APETALA1 Induced 1 23-Mar-12    ./data/SRR038847_1.fastq.gz AP1_2B AP1 APETALA1 Induced 1 23-Mar-12    ./data/SRR038848_1.fastq.gz C_1A C Control Mock 1 23-Mar-12 AP1_1   ./data/SRR038849_1.fastq.gz C_1B C Control Mock 1 23-Mar-12 AP1_1   ./data/SRR038850_1.fastq.gz C_2A C Control Mock 1 23-Mar-12 AP1_2A   ./data/SRR038851_1.fastq.gz C_2B C Control Mock 1 23-Mar-12 AP1_2B    Workflow steps This tutorial will demonstrate how to either build and run the workflow automatically, or in an interactive mode by appending each step with the appendStep method. In both cases the SYSargsList object will be populated with the instructions for running each workflow step, while supporting both command-line steps as well as line-wise R commands defined in the corresponding code chunks of this or any Rmd file that has been properly formatted.\nTo create a workflow within systemPipeR, we can start by defining an empty SYSargsList container. When restarting an existing workflow one can set resume=TRUE under the SPRproject() function call.\nlibrary(systemPipeR) sal \u003c- SPRproject()  ## Creating directory: /home/tgirke/tmp/GEN242/content/en/tutorials/spchipseq/data ## Creating directory '/home/tgirke/tmp/GEN242/content/en/tutorials/spchipseq/.SPRproject' ## Creating file '/home/tgirke/tmp/GEN242/content/en/tutorials/spchipseq/.SPRproject/SYSargsList.yml'  sal  ## Instance of 'SYSargsList': ## No workflow steps added  Next, the importWF function will load the entire workflow into the SYSargsList object (here sal). Subsequently, the runWF() function will run the workflow from start to finish. If needed, specific workflow steps can be executed by assigning their corresponding position numbers within the workflow to the steps argument (see ?runWF). After completion of the workflow one can render a scientific analysis report in HTML format with the renderReport() function that uses R Markdown internally.\nsal \u003c- importWF(sal, file_path = \"systemPipeChIPseq.Rmd\") ## Import all the Workflow steps sal sal \u003c- runWF(sal) # Runs workflow sal \u003c- renderReport(sal) # Renders report rmarkdown::render(\"systemPipeChIPseq.Rmd\", clean = TRUE, output_format = \"BiocStyle::html_document\") # Alternative report rendering  Required packages and resources The systemPipeR package needs to be loaded (H Backman and Girke 2016).\nappendStep(sal) \u003c- LineWise(code = { library(systemPipeR) }, step_name = \"load_SPR\")  Read preprocessing Read quality filtering and trimming The function preprocessReads allows to apply predefined or custom read preprocessing functions to all FASTQ files referenced in a SYSargsList container, such as quality filtering or adapter trimming routines. The paths to the resulting output FASTQ files are stored in the outfiles slot of the SYSargsList object. The following example performs adapter trimming with the trimLRPatterns function from the Biostrings package. After the trimming step a new targets file is generated (here targets_trim.txt) containing the paths to the trimmed FASTQ files. The new targets file can be used for the next workflow step with an updated SYSargs2 instance, e.g. running the NGS alignments using the trimmed FASTQ files.\nHere, we are appending this step to the SYSargsList object created previously. All the parameters are defined on the preprocessReads/preprocessReads-pe.yml and preprocessReads/preprocessReads-pe.cwl files.\nappendStep(sal) \u003c- SYSargsList(step_name = \"preprocessing\", targets = \"targets_chipseq.txt\", dir = TRUE, wf_file = \"preprocessReads/preprocessReads-se.cwl\", input_file = \"preprocessReads/preprocessReads-se.yml\", dir_path = \"param/cwl\", inputvars = c(FileName = \"_FASTQ_PATH1_\", SampleName = \"_SampleName_\"), dependency = c(\"load_SPR\"))  After, we can check the trimLRPatterns function in input parameter:\nyamlinput(sal, \"preprocessing\")$Fct  ## [1] \"'trimLRPatterns(Rpattern=\\\"GCCCGGGTAA\\\", subject=fq)'\"  After the preprocessing step, the outfiles files can be used to generate the new targets files containing the paths to the trimmed FASTQ files. The new targets information can be used for the next workflow step instance, e.g. running the NGS alignments with the trimmed FASTQ files. The appendStep function is automatically handling this connectivity between steps. Please check the Alignments step for more details.\nFASTQ quality report The following seeFastq and seeFastqPlot functions generate and plot a series of useful quality statistics for a set of FASTQ files including per cycle quality box plots, base proportions, base-level quality trends, relative k-mer diversity, length and occurrence distribution of reads, number of reads above quality cutoffs and mean quality distribution. The results are written to a PDF file named fastqReport.pdf.\nappendStep(sal) \u003c- LineWise(code = { fastq \u003c- getColumn(sal, step = \"preprocessing\", \"targetsWF\", column = 1) fqlist \u003c- seeFastq(fastq = fastq, batchsize = 10000, klength = 8) pdf(\"./results/fastqReport.pdf\", height = 18, width = 4 * length(fqlist)) seeFastqPlot(fqlist) dev.off() }, step_name = \"fastq_report\", dependency = \"preprocessing\")  Figure 1: FASTQ quality report for 7 samples.\n  Alignments Read mapping with Bowtie2 The NGS reads of this project will be aligned with Bowtie2 against the reference genome sequence (Langmead and Salzberg 2012). The parameter settings of the Bowtie2 index are defined in the bowtie2-index.cwl and bowtie2-index.yml files.\nBuilding the index:\nappendStep(sal) \u003c- SYSargsList(step_name = \"bowtie2_index\", dir = FALSE, targets = NULL, wf_file = \"bowtie2/bowtie2-index.cwl\", input_file = \"bowtie2/bowtie2-index.yml\", dir_path = \"param/cwl\", inputvars = NULL, dependency = c(\"preprocessing\"))  The parameter settings of the aligner are defined in the workflow_bowtie2-se.cwl and workflow_bowtie2-se.yml files. The following shows how to construct the corresponding SYSargsList object.\nIn ChIP-Seq experiments it is usually more appropriate to eliminate reads mapping to multiple locations. To achieve this, users want to remove the argument setting -k 50 non-deterministic in the configuration files.\nappendStep(sal) \u003c- SYSargsList(step_name = \"bowtie2_alignment\", dir = TRUE, targets = \"targets_chipseq.txt\", wf_file = \"workflow-bowtie2/workflow_bowtie2-se.cwl\", input_file = \"workflow-bowtie2/workflow_bowtie2-se.yml\", dir_path = \"param/cwl\", inputvars = c(FileName = \"_FASTQ_PATH1_\", SampleName = \"_SampleName_\"), dependency = c(\"bowtie2_index\"))  To double-check the command line for each sample, please use the following:\ncmdlist(sal, step = \"bowtie2_alignment\", targets = 1)  ## $bowtie2_alignment ## $bowtie2_alignment$AP1_1 ## $bowtie2_alignment$AP1_1$bowtie2 ## [1] \"bowtie2 -S ./results/AP1_1.sam -x ./data/tair10.fasta -k 50 --non-deterministic -U ./data/SRR038845_1.fastq.gz -p 4\" ## ## $bowtie2_alignment$AP1_1$`samtools-view` ## [1] \"samtools view -bS -o ./results/AP1_1.bam ./results/AP1_1.sam \" ## ## $bowtie2_alignment$AP1_1$`samtools-sort` ## [1] \"samtools sort -o ./results/AP1_1.sorted.bam ./results/AP1_1.bam -@ 4\" ## ## $bowtie2_alignment$AP1_1$`samtools-index` ## [1] \"samtools index -b results/AP1_1.sorted.bam results/AP1_1.sorted.bam.bai ./results/AP1_1.sorted.bam \"  Read and alignment stats The following provides an overview of the number of reads in each sample and how many of them aligned to the reference.\nappendStep(sal) \u003c- LineWise(code = { fqpaths \u003c- getColumn(sal, step = \"bowtie2_alignment\", \"targetsWF\", column = \"FileName\") bampaths \u003c- getColumn(sal, step = \"bowtie2_alignment\", \"outfiles\", column = \"samtools_sort_bam\") read_statsDF \u003c- alignStats(args = bampaths, fqpaths = fqpaths, pairEnd = TRUE) write.table(read_statsDF, \"results/alignStats.xls\", row.names = FALSE, quote = FALSE, sep = \"\\t\") }, step_name = \"align_stats\", dependency = \"bowtie2_alignment\")  Create symbolic links for viewing BAM files in IGV The symLink2bam function creates symbolic links to view the BAM alignment files in a genome browser such as IGV without moving these large files to a local system. The corresponding URLs are written to a file with a path specified under urlfile, here IGVurl.txt. Please replace the directory and the user name. The following parameter settings will create a subdirectory under ~/.html called somedir of the user account. The user name under urlbase, here \u003cusername\u003e, needs to be changed to the corresponding user name of the person running this function.\nappendStep(sal) \u003c- LineWise(code = { bampaths \u003c- getColumn(sal, step = \"bowtie2_alignment\", \"outfiles\", column = \"samtools_sort_bam\") bampaths \u003c- setNames(normalizePath(bampaths), names(bampaths)) symLink2bam(sysargs = bampaths, htmldir = c(\"~/.html/\", \"somedir/\"), urlbase = \"http://cluster.hpcc.ucr.edu/~\u003cusername\u003e/\", urlfile = \"./results/IGVurl.txt\") }, step_name = \"bam_IGV\", dependency = \"bowtie2_alignment\", run_step = \"optional\")  Peak calling with MACS2 Merge BAM files of replicates prior to peak calling Merging BAM files of technical and/or biological replicates can improve the sensitivity of the peak calling by increasing the depth of read coverage. The mergeBamByFactor function merges BAM files based on grouping information specified by a factor, here the Factor column of the imported targets file. It also returns an updated targets object containing the paths to the merged BAM files as well as to any unmerged files without replicates. The updated targets object can be used to update the SYSargsList object.\nThis step can be skipped if merging of BAM files is not desired.\nappendStep(sal) \u003c- LineWise(code = { bampaths \u003c- getColumn(sal, step = \"bowtie2_alignment\", \"outfiles\", column = \"samtools_sort_bam\") merge_bams \u003c- mergeBamByFactor(args = bampaths, targetsDF = targetsWF(sal)[[\"bowtie2_alignment\"]], overwrite = TRUE) updateColumn(sal, step = \"merge_bams\", position = \"targetsWF\") \u003c- merge_bams writeTargets(sal, step = \"merge_bams\", file = \"targets_merge_bams.txt\", overwrite = TRUE) }, step_name = \"merge_bams\", dependency = \"bowtie2_alignment\")  Peak calling with input/reference sample MACS2 can perform peak calling on ChIP-Seq data with and without input samples (Zhang et al. 2008).\nThe following performs peak calling with input sample. The input sample can be most conveniently specified in the SampleReference column of the initial targets file. The writeTargetsRef function uses this information to create a targets file intermediate for running MACS2 with the corresponding input sample(s).\nappendStep(sal) \u003c- LineWise(code = { writeTargetsRef(infile = \"targets_merge_bams.txt\", outfile = \"targets_bam_ref.txt\", silent = FALSE, overwrite = TRUE) }, step_name = \"writeTargetsRef\", dependency = \"merge_bams\")  appendStep(sal) \u003c- SYSargsList(step_name = \"call_peaks_macs_withref\", targets = \"targets_bam_ref.txt\", wf_file = \"MACS2/macs2-input.cwl\", input_file = \"MACS2/macs2-input.yml\", dir_path = \"param/cwl\", inputvars = c(FileName1 = \"_FASTQ_PATH1_\", FileName2 = \"_FASTQ_PATH2_\", SampleReference = \"_SampleName_\"), id = \"SampleReference\", dependency = c(\"writeTargetsRef\"))  The peak calling results from MACS2 are written for each sample to separate files in the results/call_peaks_macs_withref directory. They are named after the corresponding files with extensions used by MACS2.\nAnnotate peaks with genomic context Annotation with ChIPseeker package The following annotates the identified peaks with genomic context information using the ChIPseeker package (Yu, Wang, and He 2015).\nappendStep(sal) \u003c- LineWise(code = { library(ChIPseeker) library(GenomicFeatures) peaks_files \u003c- getColumn(sal, step = \"call_peaks_macs_withref\", \"outfiles\", column = \"peaks_xls\") txdb \u003c- suppressWarnings(makeTxDbFromGFF(file = \"data/tair10.gff\", format = \"gff\", dataSource = \"TAIR\", organism = \"Arabidopsis thaliana\")) for (i in seq(along = peaks_files)) { peakAnno \u003c- annotatePeak(peaks_files[i], TxDb = txdb, verbose = FALSE) df \u003c- as.data.frame(peakAnno) outpaths \u003c- paste0(\"./results/\", names(peaks_files), \"_ChIPseeker_annotated.xls\") names(outpaths) \u003c- names(peaks_files) write.table(df, outpaths[i], quote = FALSE, row.names = FALSE, sep = \"\\t\") } updateColumn(sal, step = \"annotation_ChIPseeker\", position = \"outfiles\") \u003c- data.frame(outpaths) }, step_name = \"annotation_ChIPseeker\", dependency = \"call_peaks_macs_withref\")  The peak annotation results are written to the results directory.\nSummary plots provided by the ChIPseeker package. Here applied only to one sample for demonstration purposes.\nappendStep(sal) \u003c- LineWise(code = { peaks_files \u003c- getColumn(sal, step = \"call_peaks_macs_withref\", \"outfiles\", column = \"peaks_xls\") peak \u003c- readPeakFile(peaks_files[1]) pdf(\"results/peakscoverage.pdf\") covplot(peak, weightCol = \"X.log10.pvalue.\") dev.off() pdf(\"results/peaksHeatmap.pdf\") peakHeatmap(peaks_files[1], TxDb = txdb, upstream = 1000, downstream = 1000, color = \"red\") dev.off() pdf(\"results/peaksProfile.pdf\") plotAvgProf2(peaks_files[1], TxDb = txdb, upstream = 1000, downstream = 1000, xlab = \"Genomic Region (5'-\u003e3')\", ylab = \"Read Count Frequency\", conf = 0.05) dev.off() }, step_name = \"ChIPseeker_plots\", dependency = \"annotation_ChIPseeker\")  Count reads overlapping peaks The countRangeset function is a convenience wrapper to perform read counting iteratively over serveral range sets, here peak range sets. Internally, the read counting is performed with the summarizeOverlaps function from the GenomicAlignments package. The resulting count tables are directly saved to files, one for each peak set.\nappendStep(sal) \u003c- LineWise(code = { library(GenomicRanges) bam_files \u003c- getColumn(sal, step = \"bowtie2_alignment\", \"outfiles\", column = \"samtools_sort_bam\") args \u003c- getColumn(sal, step = \"call_peaks_macs_withref\", \"outfiles\", column = \"peaks_xls\") outfiles \u003c- paste0(\"./results/\", names(args), \"_countDF.xls\") bfl \u003c- BamFileList(bam_files, yieldSize = 50000, index = character()) countDFnames \u003c- countRangeset(bfl, args, outfiles, mode = \"Union\", ignore.strand = TRUE) updateColumn(sal, step = \"count_peak_ranges\", position = \"outfiles\") \u003c- data.frame(countDFnames) }, step_name = \"count_peak_ranges\", dependency = \"call_peaks_macs_withref\", )  Shows count table generated in previous step (results/AP1_1_countDF.xls). To avoid slowdowns of the load time of this page, ony 200 rows of the source table are imported into the below datatable view .\ncountDF \u003c- read.delim(\"results/AP1_1_countDF.xls\")[1:200, ] colnames(countDF)[1] \u003c- \"PeakIDs\" library(DT) datatable(countDF)   {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\"],[\"Chr1_10000123-10001504\",\"Chr1_10015050-10015329\",\"Chr1_10044084-10044292\",\"Chr1_10053060-10053403\",\"Chr1_10065376-10065801\",\"Chr1_10077918-10078299\",\"Chr1_10111557-10111830\",\"Chr1_10137133-10137852\",\"Chr1_10139865-10140426\",\"Chr1_10141346-10141599\",\"Chr1_10142995-10143536\",\"Chr1_10143619-10144022\",\"Chr1_10145791-10146202\",\"Chr1_10146333-10147178\",\"Chr1_10186868-10187102\",\"Chr1_10187659-10187893\",\"Chr1_10203888-10204202\",\"Chr1_10219688-10219985\",\"Chr1_10228577-10229094\",\"Chr1_10229262-10229710\",\"Chr1_10242962-10243288\",\"Chr1_10244590-10244954\",\"Chr1_10247304-10247528\",\"Chr1_10251234-10251447\",\"Chr1_10263957-10264248\",\"Chr1_102694-103029\",\"Chr1_10294478-10294800\",\"Chr1_10295972-10296231\",\"Chr1_10353709-10354318\",\"Chr1_10364919-10365439\",\"Chr1_10369960-10370330\",\"Chr1_10375634-10375976\",\"Chr1_10376847-10377070\",\"Chr1_10489119-10489357\",\"Chr1_10489629-10489865\",\"Chr1_1049818-1050057\",\"Chr1_10504608-10505267\",\"Chr1_10505986-10506492\",\"Chr1_10519205-10519487\",\"Chr1_10520174-10520649\",\"Chr1_10520971-10521455\",\"Chr1_10530766-10530996\",\"Chr1_10535179-10535644\",\"Chr1_10537198-10537646\",\"Chr1_10538872-10539144\",\"Chr1_10541237-10541831\",\"Chr1_10542202-10542449\",\"Chr1_10553478-10553874\",\"Chr1_10562570-10563068\",\"Chr1_10563816-10565000\",\"Chr1_10565793-10566457\",\"Chr1_10567369-10567945\",\"Chr1_10569384-10569620\",\"Chr1_105715-106037\",\"Chr1_10581383-10581685\",\"Chr1_10622431-10622757\",\"Chr1_10631994-10632273\",\"Chr1_10649137-10649367\",\"Chr1_10668660-10668958\",\"Chr1_10669009-10669489\",\"Chr1_10676832-10677414\",\"Chr1_10678636-10679743\",\"Chr1_10679884-10680612\",\"Chr1_10683747-10684382\",\"Chr1_10684727-10685161\",\"Chr1_10685376-10685989\",\"Chr1_10691341-10691562\",\"Chr1_10691767-10692130\",\"Chr1_10692583-10693433\",\"Chr1_10693954-10694617\",\"Chr1_10695463-10696565\",\"Chr1_10701975-10702395\",\"Chr1_10704093-10704316\",\"Chr1_10713567-10714255\",\"Chr1_10715301-10715933\",\"Chr1_1072348-1072586\",\"Chr1_10726169-10726568\",\"Chr1_1072802-1073152\",\"Chr1_10737905-10738222\",\"Chr1_10793236-10793520\",\"Chr1_10794884-10795869\",\"Chr1_10799998-10800389\",\"Chr1_10800942-10801268\",\"Chr1_10801874-10802349\",\"Chr1_10803701-10803926\",\"Chr1_1084359-1084961\",\"Chr1_10845312-10845874\",\"Chr1_10864966-10865601\",\"Chr1_10871974-10872308\",\"Chr1_10874129-10874328\",\"Chr1_10874476-10875185\",\"Chr1_10891639-10891997\",\"Chr1_10909961-10910163\",\"Chr1_10953463-10953804\",\"Chr1_10954240-10954628\",\"Chr1_10956271-10956623\",\"Chr1_10958841-10959244\",\"Chr1_10959439-10960076\",\"Chr1_10972812-10973048\",\"Chr1_1103200-1103416\",\"Chr1_11033716-11034097\",\"Chr1_11059998-11060940\",\"Chr1_11061272-11061800\",\"Chr1_11062322-11062750\",\"Chr1_11067587-11068014\",\"Chr1_11071124-11071500\",\"Chr1_11084496-11084695\",\"Chr1_11128-11443\",\"Chr1_11155158-11155410\",\"Chr1_11196849-11197415\",\"Chr1_11200138-11200385\",\"Chr1_11210773-11211568\",\"Chr1_11212068-11212348\",\"Chr1_11212560-11212978\",\"Chr1_1121262-1121569\",\"Chr1_11222722-11222952\",\"Chr1_11227689-11228070\",\"Chr1_11231069-11231415\",\"Chr1_1129384-1129772\",\"Chr1_1134217-1134971\",\"Chr1_11343316-11343530\",\"Chr1_11350683-11350893\",\"Chr1_11374190-11375132\",\"Chr1_11377612-11378008\",\"Chr1_11385968-11386241\",\"Chr1_11386635-11386872\",\"Chr1_11388323-11388599\",\"Chr1_11421622-11422073\",\"Chr1_11428546-11428802\",\"Chr1_11429167-11429366\",\"Chr1_11429941-11430188\",\"Chr1_11430639-11430926\",\"Chr1_11441925-11442147\",\"Chr1_11446002-11446822\",\"Chr1_11447290-11447887\",\"Chr1_115641-116419\",\"Chr1_1159091-1159510\",\"Chr1_11594952-11595190\",\"Chr1_11598295-11598860\",\"Chr1_11600614-11600983\",\"Chr1_11601139-11601823\",\"Chr1_11605622-11605926\",\"Chr1_11620054-11620600\",\"Chr1_11621059-11621447\",\"Chr1_11621523-11623089\",\"Chr1_11624914-11625625\",\"Chr1_11626261-11626946\",\"Chr1_11628379-11628622\",\"Chr1_11628853-11629244\",\"Chr1_11630475-11631803\",\"Chr1_11662337-11662575\",\"Chr1_1172920-1173129\",\"Chr1_1173195-1173526\",\"Chr1_11763531-11763800\",\"Chr1_11770202-11770807\",\"Chr1_11771164-11771564\",\"Chr1_11798751-11798970\",\"Chr1_11801046-11801322\",\"Chr1_11804660-11805026\",\"Chr1_11807363-11807831\",\"Chr1_11808375-11808791\",\"Chr1_11809718-11810175\",\"Chr1_11829516-11829982\",\"Chr1_11830901-11831419\",\"Chr1_11874425-11874646\",\"Chr1_1188104-1188901\",\"Chr1_11925063-11925373\",\"Chr1_11927715-11928115\",\"Chr1_11928466-11928909\",\"Chr1_11978528-11979276\",\"Chr1_1197874-1198387\",\"Chr1_12004740-12004982\",\"Chr1_12005493-12006453\",\"Chr1_12026542-12027136\",\"Chr1_12050647-12051422\",\"Chr1_12051618-12051822\",\"Chr1_12054702-12055258\",\"Chr1_12055957-12056342\",\"Chr1_12056485-12057547\",\"Chr1_12059598-12059861\",\"Chr1_12064129-12064328\",\"Chr1_12070355-12070698\",\"Chr1_12123473-12123764\",\"Chr1_12132602-12132955\",\"Chr1_12177510-12177803\",\"Chr1_12180624-12180825\",\"Chr1_12254744-12255260\",\"Chr1_1233120-1233354\",\"Chr1_1235877-1236728\",\"Chr1_12458546-12458848\",\"Chr1_12474908-12475186\",\"Chr1_128465-128823\",\"Chr1_12916443-12916688\",\"Chr1_12986436-12986729\",\"Chr1_12986791-12987139\",\"Chr1_13029530-13030312\",\"Chr1_1303036-1303264\",\"Chr1_130507-131207\",\"Chr1_1306355-1306663\",\"Chr1_1307140-1307396\"],[384,19,54,31,89,82,36,95,135,29,44,44,59,134,27,14,50,49,193,75,36,29,22,24,33,21,19,16,108,125,38,17,20,20,28,36,135,53,23,168,101,19,67,26,31,55,40,39,40,290,229,40,21,39,28,13,36,16,30,40,96,271,81,63,48,106,18,40,207,86,121,85,19,158,100,38,72,40,30,16,277,51,54,97,22,88,120,77,19,18,137,64,23,44,53,47,33,150,22,29,39,107,161,49,94,23,8,60,53,91,27,58,32,69,33,26,26,47,49,222,15,32,191,35,17,26,43,71,17,16,13,39,6,120,80,93,116,17,86,41,178,36,125,48,178,59,92,23,41,284,80,17,38,23,104,104,12,13,35,72,80,82,49,98,15,100,16,36,32,251,72,25,143,127,177,16,94,37,219,22,12,80,55,37,30,19,102,9,268,26,13,74,31,23,37,304,30,93,40,16],[571,50,31,71,122,115,59,201,191,38,102,72,120,296,32,44,59,56,357,129,54,62,36,35,47,64,66,43,213,149,69,48,33,45,37,19,281,95,63,301,180,39,109,83,58,86,50,80,100,625,371,100,35,69,46,78,46,44,68,106,148,447,152,152,123,205,38,71,347,159,264,96,31,196,144,37,150,42,47,37,434,93,79,135,37,132,186,137,71,38,236,85,33,54,86,63,76,243,53,29,114,319,248,135,277,73,30,92,37,165,62,157,91,138,61,34,81,82,63,330,31,32,295,78,58,48,52,131,46,26,43,54,36,167,112,154,128,31,89,49,199,60,165,90,399,162,177,29,84,485,141,32,70,41,152,139,39,46,75,127,120,169,157,141,35,199,74,82,114,343,164,44,237,228,284,28,164,61,433,37,43,99,44,75,69,33,160,34,285,59,62,153,37,52,45,481,32,281,53,38],[282,26,19,32,89,46,21,90,95,26,58,30,58,149,25,20,35,24,158,80,26,22,22,21,23,26,49,17,96,83,24,28,13,18,16,15,107,40,32,152,74,21,49,36,27,36,19,31,45,348,151,54,14,31,15,32,31,18,21,41,83,222,82,62,40,102,10,30,143,98,119,38,12,92,66,24,94,21,20,27,189,41,52,70,19,41,94,59,33,19,132,26,11,24,29,25,39,119,13,14,53,155,110,55,110,50,14,42,15,77,21,91,27,58,37,24,42,44,31,151,15,25,142,32,22,20,18,67,28,15,22,21,19,75,46,58,43,14,45,27,90,25,51,44,191,76,83,19,48,248,67,21,35,16,68,81,14,19,42,57,78,67,74,58,9,76,24,23,45,169,72,28,117,96,141,14,67,29,195,20,17,57,25,38,29,13,78,26,118,16,25,71,15,28,16,200,16,129,27,23],[56,1,4,6,1,12,3,15,7,5,10,34,1,1,2,0,4,13,1,17,1,0,0,0,4,3,1,1,15,15,10,9,2,5,3,1,6,19,0,3,13,6,11,0,8,2,18,4,5,9,7,7,2,10,0,4,5,0,10,7,0,11,3,5,1,17,0,10,12,0,25,3,0,4,19,10,4,15,5,0,12,2,1,2,2,60,16,2,5,14,2,3,2,12,12,11,3,6,4,1,2,4,9,0,24,3,14,1,7,0,6,8,2,9,4,20,1,10,6,1,1,7,10,34,7,4,3,4,6,2,0,22,1,6,13,1,23,0,33,9,1,17,61,2,21,1,19,0,9,3,5,5,10,5,0,16,3,34,6,14,0,0,5,25,0,8,11,1,1,43,9,0,10,15,10,0,22,16,14,4,0,1,14,2,19,21,3,1,34,5,11,67,29,5,15,8,1,9,1,3],[43,3,3,6,6,4,8,8,8,8,10,12,1,4,3,1,9,5,2,5,7,1,6,1,6,3,4,1,9,10,8,6,2,8,1,0,6,22,2,3,18,4,10,3,8,7,9,8,5,32,17,7,7,10,0,3,4,0,8,3,0,21,23,9,1,20,1,11,12,1,33,7,2,9,13,8,9,16,3,2,9,4,8,8,2,19,14,11,5,5,17,1,0,7,6,5,5,9,2,4,6,17,12,3,16,1,8,8,5,1,7,2,3,3,9,16,0,7,7,19,9,2,23,19,4,0,7,14,2,3,0,15,0,6,24,4,19,2,10,7,8,12,17,0,39,14,11,4,3,6,22,4,12,7,1,11,5,10,11,15,0,1,9,17,2,13,11,0,4,26,12,0,21,18,17,0,15,5,19,1,1,5,6,8,11,6,1,1,22,9,5,27,7,1,9,14,0,6,1,7],[104,12,14,25,18,10,17,20,20,5,21,11,11,15,8,10,13,16,32,16,12,10,9,7,18,11,7,4,21,27,12,8,2,6,10,4,22,16,13,30,11,17,13,15,13,16,6,14,14,33,36,11,7,2,6,11,12,12,16,19,13,41,21,20,10,25,6,22,30,15,33,15,14,17,19,9,35,10,14,9,32,22,8,8,8,15,18,17,20,14,22,15,5,7,13,5,12,11,6,7,14,41,22,20,46,8,11,14,7,16,5,9,12,10,9,5,12,19,9,31,8,12,48,15,11,4,5,36,4,4,12,5,13,20,20,18,28,6,13,15,31,10,12,7,41,23,26,5,22,36,43,5,13,2,28,29,7,5,16,18,18,18,24,12,10,30,9,9,13,55,26,3,46,37,30,6,29,5,21,11,7,21,7,13,9,3,24,5,36,23,16,25,4,6,11,31,12,43,18,4],[55,5,3,4,9,4,9,15,17,6,4,8,10,14,4,4,7,8,19,5,4,12,5,5,2,7,5,4,12,17,6,6,4,1,3,5,16,5,2,19,10,4,7,6,2,5,8,7,9,14,24,3,7,3,3,9,7,5,7,11,9,22,11,2,8,16,4,10,14,7,13,10,2,10,11,4,30,5,8,3,20,3,6,7,5,9,5,8,10,4,12,9,10,0,9,2,7,18,2,6,11,26,11,15,23,9,4,10,6,9,6,9,4,9,4,1,3,8,7,13,3,8,20,10,5,5,6,12,3,1,5,9,3,8,11,10,17,1,4,5,18,4,12,6,25,13,11,0,10,19,22,5,10,4,19,17,2,8,6,18,7,9,7,7,4,8,3,2,4,23,11,1,24,13,14,4,16,7,12,5,2,9,3,11,7,4,20,6,13,6,14,14,2,4,9,17,4,32,3,4]],\"container\":\"\\n \\n \\n  \\n PeakIDs\\n AP1_1\\n AP1_2A\\n AP1_2B\\n C_1A\\n C_1B\\n C_2A\\n C_2B\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Differential binding analysis The runDiff function performs differential binding analysis in batch mode for several count tables using edgeR or DESeq2 (Robinson, McCarthy, and Smyth 2010; Love, Huber, and Anders 2014). Internally, it calls the functions run_edgeR and run_DESeq2. It also returns the filtering results and plots from the downstream filterDEGs function using the fold change and FDR cutoffs provided under the dbrfilter argument.\nappendStep(sal) \u003c- LineWise(code = { countDF_files \u003c- getColumn(sal, step = \"count_peak_ranges\", \"outfiles\") outfiles \u003c- paste0(\"./results/\", names(countDF_files), \"_peaks_edgeR.xls\") names(outfiles) \u003c- names(countDF_files) cmp \u003c- readComp(file = stepsWF(sal)[[\"bowtie2_alignment\"]], format = \"matrix\") dbrlist \u003c- runDiff(args = countDF_files, outfiles = outfiles, diffFct = run_edgeR, targets = targetsWF(sal)[[\"bowtie2_alignment\"]], cmp = cmp[[1]], independent = TRUE, dbrfilter = c(Fold = 2, FDR = 1)) }, step_name = \"diff_bind_analysis\", dependency = \"count_peak_ranges\", )  GO term enrichment analysis Obtain gene-to-GO mappings The following shows how to obtain gene-to-GO mappings from biomaRt (here for A. thaliana) and how to organize them for the downstream GO term enrichment analysis. Alternatively, the gene-to-GO mappings can be obtained for many organisms from Bioconductor’s *.db genome annotation packages or GO annotation files provided by various genome databases. For each annotation this relatively slow preprocessing step needs to be performed only once. Subsequently, the preprocessed data can be loaded with the load function as shown in the next subsection.\nGO term enrichment analysis The following performs GO term enrichment analysis for each annotated peak set. Note: the following assumes that the GO annotation data exists under data/GO/catdb.RData.\nappendStep(sal) \u003c- LineWise(code = { library(\"biomaRt\") # listMarts() # To choose BioMart database # listMarts(host='plants.ensembl.org') m \u003c- useMart(\"plants_mart\", host = \"https://plants.ensembl.org\") m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"https://plants.ensembl.org\") go \u003c- getBM(attributes = c(\"go_id\", \"tair_locus\", \"namespace_1003\"), mart = m) go \u003c- go[go[, 3] != \"\", ] go[, 3] \u003c- as.character(go[, 3]) go[go[, 3] == \"molecular_function\", 3] \u003c- \"F\" go[go[, 3] == \"biological_process\", 3] \u003c- \"P\" go[go[, 3] == \"cellular_component\", 3] \u003c- \"C\" go[1:4, ] if (!dir.exists(\"./data/GO\")) dir.create(\"./data/GO\") write.table(go, \"data/GO/GOannotationsBiomart_mod.txt\", quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\") catdb \u003c- makeCATdb(myfile = \"data/GO/GOannotationsBiomart_mod.txt\", lib = NULL, org = \"\", colno = c(1, 2, 3), idconv = NULL) save(catdb, file = \"data/GO/catdb.RData\") }, step_name = \"get_go_annot\", dependency = \"annotation_ChIPseeker\", )  GO term enrichment analysis The following performs GO term enrichment analysis for each annotated peak set. Note: the following assumes that the GO annotation data exists under data/GO/catdb.RData.\nappendStep(sal) \u003c- LineWise(code = { annofiles \u003c- getColumn(sal, step = \"annotation_ChIPseeker\", \"outfiles\") gene_ids \u003c- sapply(annofiles, function(x) unique(as.character(read.delim(x)[, \"geneId\"])), simplify = FALSE) load(\"data/GO/catdb.RData\") BatchResult \u003c- GOCluster_Report(catdb = catdb, setlist = gene_ids, method = \"all\", id_type = \"gene\", CLSZ = 2, cutoff = 0.9, gocats = c(\"MF\", \"BP\", \"CC\"), recordSpecGO = NULL) write.table(BatchResult, \"results/GOBatchAll.xls\", quote = FALSE, row.names = FALSE, sep = \"\\t\") }, step_name = \"go_enrich\", dependency = \"annotation_ChIPseeker\", )  Shows GO term enrichment results from previous step. The last gene identifier column (10) of this table has been excluded in this viewing instance to minimize the complexity of the result. To avoid slowdowns of the load time of this page, only 50 rows of the source table are imported into the below datatable view .\nBatchResult \u003c- read.delim(\"results/GOBatchAll.xls\")[1:50, ] library(DT) datatable(BatchResult[, -10], options = list(scrollX = TRUE, autoWidth = TRUE))   {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"],[\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\",\"C_1A\"],[4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903,4903],[\"GO:0003700\",\"GO:0140110\",\"GO:0098772\",\"GO:0043565\",\"GO:0000976\",\"GO:0001067\",\"GO:1990837\",\"GO:0003677\",\"GO:0003690\",\"GO:0042802\",\"GO:0000981\",\"GO:0046983\",\"GO:0000987\",\"GO:0003676\",\"GO:0000978\",\"GO:1901363\",\"GO:0033612\",\"GO:0097159\",\"GO:0042562\",\"GO:0005515\",\"GO:0000977\",\"GO:0060089\",\"GO:0004672\",\"GO:0019840\",\"GO:0016773\",\"GO:0043178\",\"GO:0004860\",\"GO:0019210\",\"GO:0016705\",\"GO:0015291\",\"GO:0038023\",\"GO:0001653\",\"GO:0016301\",\"GO:0022857\",\"GO:0004861\",\"GO:0030291\",\"GO:0022804\",\"GO:0042803\",\"GO:0000156\",\"GO:0005215\",\"GO:0004674\",\"GO:0019900\",\"GO:0015293\",\"GO:0005102\",\"GO:0004805\",\"GO:0005516\",\"GO:0010427\",\"GO:0080161\",\"GO:0008509\",\"GO:0019207\"],[1667,1837,2497,1298,1007,1007,1032,2566,1142,394,336,664,306,4598,284,7859,46,7889,32,8016,324,236,1133,23,1296,39,27,30,400,313,207,17,1498,1200,8,8,515,208,19,1273,880,138,101,126,12,235,19,19,240,111],[682,698,807,506,412,412,416,777,432,133,117,195,102,966,94,1550,26,1552,20,1566,95,73,260,14,289,19,15,16,104,85,61,11,325,266,7,7,127,60,11,276,198,42,33,39,8,62,10,10,63,34],[3.49341724975865e-120,1.67823421235582e-104,2.18308574281622e-80,2.03153636203605e-78,2.16539339765293e-70,2.16539339765293e-70,7.55925494113875e-69,8.11545366519395e-63,5.44122511323004e-62,8.45596025352572e-15,3.02911820739919e-14,5.20591490791628e-14,2.60120228948827e-11,4.80717206533446e-11,2.37684179178145e-10,3.18096489951193e-09,3.70334206206444e-09,6.71010417736292e-09,2.23753662482306e-08,4.36325237409659e-08,1.70493636893304e-07,4.830138137427e-07,2.6496064818211e-06,4.80082717610204e-06,8.57859203202179e-06,9.09042267661985e-06,1.04457669068979e-05,1.06626043165579e-05,1.95557875198908e-05,2.00638856116351e-05,2.16501006655692e-05,2.30169720521419e-05,2.58444261599642e-05,3.08098675362986e-05,3.69557186521355e-05,3.69557186521355e-05,3.83718364858986e-05,5.00968427978002e-05,9.90476514846966e-05,0.000111473574255863,0.000134651553167959,0.000179143121967622,0.000200017440650394,0.000203270191184441,0.000242118052007588,0.000569854299398944,0.000584020859104921,0.000584020859104921,0.000596646365491369,0.000620806400430897],[2.12399768785326e-117,1.02036640111234e-101,1.32731613163226e-77,1.23517410811792e-75,1.31655918577298e-67,1.31655918577298e-67,4.59602700421236e-66,4.93419582843792e-60,3.30826486884386e-59,5.14122383414364e-12,1.8417038700987e-11,3.1651962640131e-11,1.58153099200887e-08,2.92276061572335e-08,1.44511980940312e-07,1.93402665890325e-06,2.25163197373518e-06,4.07974333983666e-06,1.36042226789242e-05,2.65285744345073e-05,0.000103660131231129,0.000293672398755561,0.00161096074094723,0.00291890292307004,0.00521578395546925,0.00552697698738487,0.00635102627939394,0.00648286342446718,0.0118899188120936,0.0121988424518741,0.0131632612046661,0.0139943190077023,0.0157134111052582,0.0187323994620695,0.0224690769404984,0.0224690769404984,0.0233300765834263,0.0304588804210625,0.0602209721026955,0.0677759331475647,0.0818681443261192,0.108919018156314,0.121610603915439,0.12358827624014,0.147207775620613,0.346471414034558,0.355084682335792,0.355084682335792,0.362760990218752,0.377450291461985],[\"DNA-binding transcription factor activity\",\"transcription regulator activity\",\"molecular function regulator\",\"sequence-specific DNA binding\",\"transcription regulatory region sequence-specific DNA binding\",\"regulatory region nucleic acid binding\",\"sequence-specific double-stranded DNA binding\",\"DNA binding\",\"double-stranded DNA binding\",\"identical protein binding\",\"DNA-binding transcription factor activity, RNA polymerase II-specific\",\"protein dimerization activity\",\"cis-regulatory region sequence-specific DNA binding\",\"nucleic acid binding\",\"RNA polymerase II cis-regulatory region sequence-specific DNA binding\",\"heterocyclic compound binding\",\"receptor serine/threonine kinase binding\",\"organic cyclic compound binding\",\"hormone binding\",\"protein binding\",\"RNA polymerase II transcription regulatory region sequence-specific DNA binding\",\"molecular transducer activity\",\"protein kinase activity\",\"isoprenoid binding\",\"phosphotransferase activity, alcohol group as acceptor\",\"alcohol binding\",\"protein kinase inhibitor activity\",\"kinase inhibitor activity\",\"oxidoreductase activity, acting on paired donors, with incorporation or reduction of molecular oxygen\",\"secondary active transmembrane transporter activity\",\"signaling receptor activity\",\"peptide receptor activity\",\"kinase activity\",\"transmembrane transporter activity\",\"cyclin-dependent protein serine/threonine kinase inhibitor activity\",\"protein serine/threonine kinase inhibitor activity\",\"active transmembrane transporter activity\",\"protein homodimerization activity\",\"phosphorelay response regulator activity\",\"transporter activity\",\"protein serine/threonine kinase activity\",\"kinase binding\",\"symporter activity\",\"signaling receptor binding\",\"trehalose-phosphatase activity\",\"calmodulin binding\",\"abscisic acid binding\",\"auxin transmembrane transporter activity\",\"anion transmembrane transporter activity\",\"kinase regulator activity\"],[\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\",\"MF\"]],\"container\":\"\\n \\n \\n  \\n CLID\\n CLSZ\\n GOID\\n NodeSize\\n SampleMatch\\n Phyper\\n Padj\\n Term\\n Ont\\n \\n \\n\",\"options\":{\"scrollX\":true,\"autoWidth\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,4,5,6,7]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Motif analysis Parse DNA sequences of peak regions from genome Enrichment analysis of known DNA binding motifs or de novo discovery of novel motifs requires the DNA sequences of the identified peak regions. To parse the corresponding sequences from the reference genome, the getSeq function from the Biostrings package can be used. The following example parses the sequences for each peak set and saves the results to separate FASTA files, one for each peak set. In addition, the sequences in the FASTA files are ranked (sorted) by increasing p-values as expected by some motif discovery tools, such as BCRANK.\nappendStep(sal) \u003c- LineWise(code = { library(Biostrings) library(seqLogo) library(BCRANK) rangefiles \u003c- getColumn(sal, step = \"call_peaks_macs_withref\", \"outfiles\") for (i in seq(along = rangefiles)) { df \u003c- read.delim(rangefiles[i], comment = \"#\") peaks \u003c- as(df, \"GRanges\") names(peaks) \u003c- paste0(as.character(seqnames(peaks)), \"_\", start(peaks), \"-\", end(peaks)) peaks \u003c- peaks[order(values(peaks)$X.log10.pvalue., decreasing = TRUE)] pseq \u003c- getSeq(FaFile(\"./data/tair10.fasta\"), peaks) names(pseq) \u003c- names(peaks) writeXStringSet(pseq, paste0(rangefiles[i], \".fasta\")) } }, step_name = \"parse_peak_sequences\", dependency = \"call_peaks_macs_withref\")  Motif discovery with BCRANK The Bioconductor package BCRANK is one of the many tools available for de novo discovery of DNA binding motifs in peak regions of ChIP-Seq experiments. The given example applies this method on the first peak sample set and plots the sequence logo of the highest ranking motif.\nappendStep(sal) \u003c- LineWise(code = { library(Biostrings) library(seqLogo) library(BCRANK) rangefiles \u003c- getColumn(sal, step = \"call_peaks_macs_withref\", \"outfiles\") set.seed(0) BCRANKout \u003c- bcrank(paste0(rangefiles[1], \".fasta\"), restarts = 25, use.P1 = TRUE, use.P2 = TRUE) toptable(BCRANKout) topMotif \u003c- toptable(BCRANKout, 1) weightMatrix \u003c- pwm(topMotif, normalize = FALSE) weightMatrixNormalized \u003c- pwm(topMotif, normalize = TRUE) pdf(\"results/seqlogo.pdf\") seqLogo(weightMatrixNormalized) dev.off() }, step_name = \"bcrank_enrich\", dependency = \"call_peaks_macs_withref\")  Figure 2: One of the motifs identified by BCRANK\n  Version Information appendStep(sal) \u003c- LineWise(code = { sessionInfo() }, step_name = \"sessionInfo\", dependency = \"bcrank_enrich\")  Running workflow Interactive job submissions in a single machine For running the workflow, runWF function will execute all the steps store in the workflow container. The execution will be on a single machine without submitting to a queuing system of a computer cluster.\nsal \u003c- runWF(sal)  Parallelization on clusters Alternatively, the computation can be greatly accelerated by processing many files in parallel using several compute nodes of a cluster, where a scheduling/queuing system is used for load balancing.\nThe resources list object provides the number of independent parallel cluster processes defined under the Njobs element in the list. The following example will run 18 processes in parallel using each 4 CPU cores. If the resources available on a cluster allow running all 18 processes at the same time, then the shown sample submission will utilize in a total of 72 CPU cores.\nNote, runWF can be used with most queueing systems as it is based on utilities from the batchtools package, which supports the use of template files (*.tmpl) for defining the run parameters of different schedulers. To run the following code, one needs to have both a conffile (see .batchtools.conf.R samples here) and a template file (see *.tmpl samples here) for the queueing available on a system. The following example uses the sample conffile and template files for the Slurm scheduler provided by this package.\nThe resources can be appended when the step is generated, or it is possible to add these resources later, as the following example using the addResources function:\nresources \u003c- list(conffile=\".batchtools.conf.R\", template=\"batchtools.slurm.tmpl\", Njobs=8, walltime=120, ## minutes ntasks=1, ncpus=4, memory=1024, ## Mb partition = \"short\" ) sal \u003c- addResources(sal, c(\"bowtie2_alignment\"), resources = resources) sal \u003c- runWF(sal)  Visualize workflow systemPipeR workflows instances can be visualized with the plotWF function.\nplotWF(sal)   {\"x\":{\"dot\":\"digraph {\\n node[fontsize=20];\\n subgraph {\\n load_SPR - preprocessing - bowtie2_index - bowtie2_alignment - merge_bams - writeTargetsRef - call_peaks_macs_withref - bcrank_enrich - sessionInfo\\n }\\n preprocessing - fastq_report\\n bowtie2_alignment - align_stats\\n bowtie2_alignment - bam_IGV\\n call_peaks_macs_withref - annotation_ChIPseeker\\n call_peaks_macs_withref - count_peak_ranges\\n call_peaks_macs_withref - parse_peak_sequences\\n annotation_ChIPseeker - ChIPseeker_plots\\n annotation_ChIPseeker - go_enrich\\n count_peak_ranges - diff_bind_analysis\\n load_SPR[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=load_SPR\n0/0/0/1; 0s tooltip=\\\"step load_SPR: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n preprocessing[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=preprocessing\n0/0/0/7; 0s , shape=\\\"box\\\" tooltip=\\\"step preprocessing: 0 samples passed; 0 samples have warnings; 0 samples have errors; 7 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n fastq_report[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=fastq_report\n0/0/0/1; 0s tooltip=\\\"step fastq_report: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n bowtie2_index[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=bowtie2_index\n0/0/0/6; 0s , shape=\\\"box\\\" tooltip=\\\"step bowtie2_index: 0 samples passed; 0 samples have warnings; 0 samples have errors; 6 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n bowtie2_alignment[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=bowtie2_alignment\n0/0/0/28; 0s , shape=\\\"box\\\" tooltip=\\\"step bowtie2_alignment: 0 samples passed; 0 samples have warnings; 0 samples have errors; 28 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n align_stats[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=align_stats\n0/0/0/1; 0s tooltip=\\\"step align_stats: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n bam_IGV[style=\\\"solid, \\\"label=bam_IGV\n0/0/0/1; 0s tooltip=\\\"step bam_IGV: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n merge_bams[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=merge_bams\n0/0/0/1; 0s tooltip=\\\"step merge_bams: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n writeTargetsRef[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=writeTargetsRef\n0/0/0/1; 0s tooltip=\\\"step writeTargetsRef: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n call_peaks_macs_withref[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, rounded\\\" label=call_peaks_macs_withref\n0/0/0/5; 0s , shape=\\\"box\\\" tooltip=\\\"step call_peaks_macs_withref: 0 samples passed; 0 samples have warnings; 0 samples have errors; 5 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n annotation_ChIPseeker[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=annotation_ChIPseeker\n0/0/0/1; 0s tooltip=\\\"step annotation_ChIPseeker: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n ChIPseeker_plots[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=ChIPseeker_plots\n0/0/0/1; 0s tooltip=\\\"step ChIPseeker_plots: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n count_peak_ranges[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=count_peak_ranges\n0/0/0/1; 0s tooltip=\\\"step count_peak_ranges: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n diff_bind_analysis[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=diff_bind_analysis\n0/0/0/1; 0s tooltip=\\\"step diff_bind_analysis: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n go_enrich[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=go_enrich\n0/0/0/1; 0s tooltip=\\\"step go_enrich: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n parse_peak_sequences[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=parse_peak_sequences\n0/0/0/1; 0s tooltip=\\\"step parse_peak_sequences: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n bcrank_enrich[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=bcrank_enrich\n0/0/0/1; 0s tooltip=\\\"step bcrank_enrich: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n sessionInfo[fillcolor=\\\"#d3d6eb\\\" style=\\\"filled, \\\"label=sessionInfo\n0/0/0/1; 0s tooltip=\\\"step sessionInfo: 0 samples passed; 0 samples have warnings; 0 samples have errors; 1 samples in total; Start time: 2022-04-30 18:12:57; End time: 2022-04-30 18:12:57; Duration: 00:00:00\\\"]\\n subgraph cluster_legend {\\n rankdir=TB;\\n color=\\\"#eeeeee\\\";\\n style=filled;\\n ranksep =1;\\n label=\\\"Legends\\\";\\n fontsize = 30;\\n node [style=filled, fontsize=10];\\n legend_img- step_state[color=\\\"#eeeeee\\\"];\\n\\n legend_img[shape=none, image=\\\"plotwf_legend-src.png\\\", label = \\\" \\\", height=1, width=3, style=\\\"\\\"];\\n\\n step_state[style=\\\"filled\\\", shape=\\\"box\\\" color=white, label =\\n Step Colors\\n Pending steps; Successful steps; Failed steps\\n Targets Files / Code Chunk 0 (pass)  | 0 (warning)  | 0 (error)  | 0 (total); Duration\\n ];\\n\\n }\\n\\n}\\n\",\"plotid\":\"sprwf-75681234\",\"responsive\":true,\"width\":null,\"height\":null,\"plot_method\":\"renderSVGElement\",\"rmd\":true,\"msg\":\"\",\"plot_ctr\":true,\"pan_zoom\":false,\"legend_uri\":\"data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!-- Do not edit this file with editors other than diagrams.net -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="496px" height="278px" viewBox="-0.5 -0.5 496 278" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2021-11-24T20:39:44.923Z&quot; agent=&quot;5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&quot; version=&quot;15.8.3&quot; etag=&quot;T_dExnL7SLX2buX8PaX3&quot; type=&quot;google&quot;&gt;&lt;diagram id=&quot;Vw9VViGsx-sL_NjKe7JP&quot;&gt;7VrbcuI4EP0aHpOyJd94DARmXmZ3K2zVPitYGFVkyWPLgczXr2TL+CID3sGQTRUkldgtqS2d02p1t5nAebz/lqJk+4OHmE6AFe4n8HkCgA0AmKhfK/zQEsvxS0mUklDLasGK/MJVRy3NSYizVkfBORUkaQvXnDG8Fi0ZSlO+a3fbcNp+aoIibAhWa0RN6T8kFNtSOrWsWv4dk2hbPdmrWmJUddaCbItCvmuI4GIC5ynnoryK93NMFXoVLuW45ZHWw8RSzMSQAZqJd0RzvTY9L/FRLTZKeZ5M4Ez+YyFW4yx5d5i4utFKcCrwvo8A9Fops8wJ2odlS4PBPMYi/ZBdKkWBHqJtBUB9v6uBB45byrZN0KGnCddkRwfdNR7yQkPSDw88D08blt2WCLxK0Fq17qT5S9lWxFL/sy0vN4TSOac8LcbC0MVB6Cg0RcrfcNXCOJPDZxFFWdYHd/aGxXp7DvsmxqAfY42pA4dB6oyAqHNdRJfLRfC8OIboVVEMpgNR9C9H0b0uiovlEiznt0XxLGzwcti8sWHjTOjTKTBR3LjqR8oRJRGTMoo34hioSlVjbPmRci4fToQysWBEhH3rSg7TNxDOOJWNXZjl9EUbywqktVwGlijM1CKJPHOfdENMwlANn6U4I7/0kaIwSThhopizO5u4z0pXLnhW8mIfBbxBVoODJp3ydoliQhX6f5NYRhzA+gPv5N8XHiM2lI7TBxzw/Ue3fcQB05PAHsKqw/QSvgKDrxBlW3wn7Dhh0HUNwgKDMOdKhE0Nwn4gJuPNWM3+q5Fm27fbZm7w6Hdoc81Qss8xjkFblbk0eJvzOMkFvpN2Yqs5xlbr4cy7Fmd2T7jg0eIIT+QSm6x5P3OVshW4PJT4PskOQbKv2+RVpP7Lp8hEFVe65DRKdWWrYQ9V9/8UxIV4g3Iqbre9vKDLlO8ZTLn2Y09kB0YIiG0zkX2576wTfAV2hy9o9WSB19pZZl4tvaFcXPhACbu7xFMuEXZdIrRsg7jgWsSZ6Xuq6jssoiZrdSplX+K8PiHAA0N8Gej1Ze4IIA/I7jELn1QhszbHBpadSl1hz1WdEh4wwqFR5DyLUHP9PSZWyVJMkSDvbfV9kOgn/KX2ZPMwmT6CDgPdqknG83SN9UDQKHB2dEFgG3FfV5dAaYSFoavg6bD4YdQNqDBcRJ3OzsrO/3ceZabUAd57DKxp/XF/j1PHMlLmM5pHZNiscMgELESC601+P7eOBIhdpwod06leLZQ36xx/JoJwhuidtFMnoTWAtD4PMgppZq1jlPxLwTs0+bro7Q4MPfx6quB7i+pi+/UZBGaU7/i9kcwIDAKz6jEKg4zLllvRuCw+n0ujGZLemEizFPKSM0ZYNFHqSxJe0yGE2pZktKCyQ+oKZ5n0yF/IH+s5dt4E2dbNzMIOgnYYBL3KAs74ZziGUZg1l8oUQvJ+kS2c9RKvaP0WFbv6YV0yo/SJFLGsgm+mfMTA5x2M+Yg7OYiLhekx+GdO0q9WeP98m/XbNguBGVH0VeFHsViz7rQSOBnPh82Lr2zcrWG4NUxh24MFZjGr71Xab1iDvK2/V1VmfvXX0+DiXw==&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g><rect x="4" y="90" width="490" height="92" fill="#d5e8d4" stroke="none" pointer-events="none"/><rect x="4" y="182" width="490" height="94" fill="#ffe8de" stroke="none" pointer-events="none"/><rect x="4" y="4" width="490" height="86" fill="#eff2fc" stroke="none" pointer-events="none"/><rect x="4" y="4" width="140" height="272" fill-opacity="0.8" fill="#f5f5f5" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 11px; margin-left: 115px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">solid</div></div></div></foreignObject><text x="115" y="13" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">solid</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 10px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">dashed</div></div></div></foreignObject><text x="198" y="12" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">dashed</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 32px; margin-left: 116px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Management</div></div></div></foreignObject><text x="116" y="35" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Management</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 32px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Compute</div></div></div></foreignObject><text x="198" y="35" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Compute</text></switch></g><ellipse cx="232.5" cy="123" rx="51.5" ry="27" fill="rgba(255, 255, 255, 1)" stroke="rgba(0, 0, 0, 1)" stroke-width="2" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 50px; height: 1px; padding-top: 62px; margin-left: 92px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">ellipse</span></div></div></div></foreignObject><text x="116" y="65" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">ellipse</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 85px; margin-left: 114px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">R</div></div></div></foreignObject><text x="114" y="88" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">R</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 83px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Command-line</div></div></div></foreignObject><text x="198" y="86" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Command-line</text></switch></g><rect x="349" y="96" width="105" height="50" rx="7.5" ry="7.5" fill="rgba(255, 255, 255, 1)" stroke="rgba(0, 0, 0, 1)" stroke-width="2" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 51px; height: 1px; padding-top: 61px; margin-left: 176px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 8px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;">rectangle</div></div></div></foreignObject><text x="201" y="63" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="8px" text-anchor="middle">rectangle</text></switch></g><path d="M 182.5 38 L 287.5 38" fill="none" stroke="rgba(0, 0, 0, 1)" stroke-width="6" stroke-miterlimit="10" pointer-events="none"/><path d="M 354 37.62 L 459 37.62" fill="none" stroke="rgba(0, 0, 0, 1)" stroke-width="6" stroke-miterlimit="10" stroke-dasharray="18 18" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 128px; margin-left: 115px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Mandatory</div></div></div></foreignObject><text x="115" y="131" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Mandatory</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 128px; margin-left: 198px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: nowrap;">Optional</div></div></div></foreignObject><text x="198" y="131" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="11px" text-anchor="middle">Optional</text></switch></g><rect x="184" y="190" width="95" height="40" fill="#d3d6eb" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 46px; height: 1px; padding-top: 105px; margin-left: 93px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">fill</span></div></div></div></foreignObject><text x="116" y="109" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">fill</text></switch></g><rect x="349" y="190" width="95" height="40" fill="#ffffff" stroke="none" pointer-events="none"/><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 46px; height: 1px; padding-top: 105px; margin-left: 176px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; white-space: normal; overflow-wrap: normal;"><span style="font-size: 8px">no fill</span></div></div></div></foreignObject><text x="198" y="109" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="12px" text-anchor="middle">no fill</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 24px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;">Running <br style="font-size: 10px" />Session</div></div></div></foreignObject><text x="35" y="27" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">Running...</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 113px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;"><div style="font-size: 10px"><span style="background-color: transparent ; font-size: 10px">Running</span></div>Requirement</div></div></div></foreignObject><text x="35" y="116" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">RunningRequire...</text></switch></g><g transform="translate(-0.5 -0.5)scale(2)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 68px; margin-left: 35px;"><div data-drawio-colors="color: rgba(0, 0, 0, 1); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 10px; font-family: &quot;Times New Roman&quot;; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: none; font-weight: bold; white-space: nowrap;">Step <br style="font-size: 10px" />Class</div></div></div></foreignObject><text x="35" y="71" fill="rgba(0, 0, 0, 1)" font-family="Times New Roman" font-size="10px" text-anchor="middle" font-weight="bold">Step...</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Viewer does not support full SVG 1.1</text></a></switch></svg>\"},\"evals\":[],\"jsHooks\":[]} Checking workflow status To check the summary of the workflow, we can use:\nsal statusWF(sal)  Technical report systemPipeR compiles all the workflow execution logs in one central location, making it easier to check any standard output (stdout) or standard error (stderr) for any command-line tools used on the workflow or the R code stdout.\nsal \u003c- renderLogs(sal)  Scientific report systemPipeR auto-generates scientific analysis reports in HTML format.\nsal \u003c- renderReport(sal)  Alternatively, scientific reports can be rendered with the render function from rmarkdown.\nrmarkdown::render(\"systemPipeChIPseq.Rmd\", clean = TRUE, output_format = \"BiocStyle::html_document\")  Funding This project was supported by funds from the National Institutes of Health (NIH) and the National Science Foundation (NSF).\nReferences H Backman, Tyler W, and Thomas Girke. 2016. “systemPipeR: NGS workflow and report generation environment.” BMC Bioinformatics 17 (1): 388. https://doi.org/10.1186/s12859-016-1241-0.\n Kaufmann, Kerstin, Frank Wellmer, Jose M Muiño, Thilia Ferrier, Samuel E Wuest, Vijaya Kumar, Antonio Serrano-Mislata, et al. 2010. “Orchestration of floral initiation by APETALA1.” Science 328 (5974): 85–89. https://doi.org/10.1126/science.1185244.\n Langmead, Ben, and Steven L Salzberg. 2012. “Fast Gapped-Read Alignment with Bowtie 2.” Nat. Methods 9 (4): 357–59. https://doi.org/10.1038/nmeth.1923.\n Love, Michael, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Genome Biol. 15 (12): 550. https://doi.org/10.1186/s13059-014-0550-8.\n Robinson, M D, D J McCarthy, and G K Smyth. 2010. “EdgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\n Yu, Guangchuang, Li-Gen Wang, and Qing-Yu He. 2015. “ChIPseeker: An R/Bioconductor Package for ChIP Peak Annotation, Comparison and Visualization.” Bioinformatics 31 (14): 2382–3. https://doi.org/10.1093/bioinformatics/btv145.\n Zhang, Y, T Liu, C A Meyer, J Eeckhoute, D S Johnson, B E Bernstein, C Nussbaum, et al. 2008. “Model-Based Analysis of ChIP-Seq (MACS).” Genome Biol. 9 (9). https://doi.org/10.1186/gb-2008-9-9-r137.\n  ","categories":"","description":"","excerpt":"               pre code { white-space: pre !important; overflow-x: …","ref":"/tutorials/spchipseq/spchipseq/","tags":"","title":"ChIP-Seq Workflow Template"},{"body":"Source code downloads: [ .Rmd ] [ .R ]\n Introduction This tutorial introduces the usage of several software implementations of embedding algorithms for high-dimensional gene expression data (Duò, Robinson, and Soneson 2018) that are often used for single cell RNA-Seq (scRNA-Seq) data. Many of them are available as R packages on CRAN, Bioconductor and/or GitHub. Examples include PCA, MDS, SC3 (Kiselev et al. 2017), isomap, t-SNE (Donaldson and Donaldson 2010), FIt-SNE (Linderman et al. 2019), and UMAP (McInnes, Healy, and Melville 2018). In addition, some packages such as Bioconductor’s scater package provide in a single environment access to a wide range of embedding methods that can be conveniently and uniformly applied to Bioconductor’s S4 object class called SingleCellExperiment for handling scRNA-Seq data (Senabouth et al. 2019; Amezquita et al. 2020). The performance of the different embedding methods for scRNA-Seq data has been intensively tested by several studies, including Sun et al. (2019; 2020).\nFor illustration purposes, the following example code first applies four widely used embedding methods to a bulk RNA-Seq data set (Howard et al. 2013), and then to a much more complex scRNA-Seq data set (Aztekin et al. 2019) obtained from the scRNAseq package.\nBulk RNA-Seq data Generate SummarizedExperiment and SingleCellExperiment The following loads the bulk RNA-Seq data from Howard et al. (2013) into SummarizedExperiment and SingleCellExperiment objects. This is done by first creating a SummarizedExperiment object and then coercing it to a SingleCellExperiment object, as well as intializing the SingleCellExperiment directly.\nCreate SummarizedExperiment and coerce to SingleCellExperiment The required targetsPE.txt and countDFeByg.xls files can be downloaded from here.\nlibrary(SummarizedExperiment); library(SingleCellExperiment) targetspath \u003c- \"results/targetsPE.txt\" countpath \u003c- \"results/countDFeByg.xls\" targets \u003c- read.delim(targetspath, comment.char = \"#\") rownames(targets) \u003c- targets$SampleName countDF \u003c- read.delim(countpath, row.names=1, check.names=FALSE) (se \u003c- SummarizedExperiment(assays=list(counts=countDF), colData=targets))  ## class: SummarizedExperiment ## dim: 29699 18 ## metadata(0): ## assays(1): counts ## rownames(29699): AT1G01010 AT1G01020 ... ATMG01400 ATMG01410 ## rowData names(0): ## colnames(18): M1A M1B ... V12A V12B ## colData names(7): FileName1 FileName2 ... Experiment Date  (sce \u003c- as(se, \"SingleCellExperiment\"))  ## class: SingleCellExperiment ## dim: 29699 18 ## metadata(0): ## assays(1): counts ## rownames(29699): AT1G01010 AT1G01020 ... ATMG01400 ATMG01410 ## rowData names(0): ## colnames(18): M1A M1B ... V12A V12B ## colData names(7): FileName1 FileName2 ... Experiment Date ## reducedDimNames(0): ## mainExpName: NULL ## altExpNames(0):  Create SingleCellExperiment directly sce2 \u003c- SingleCellExperiment(assays=list(counts=countDF), colData=targets)  Prepare data for plotting with embedding methods The data are preprocessed (_e.g._normalized) to plot them with the run embedding functions from the scran and scater packages.\nlibrary(scran); library(scater) sce \u003c- logNormCounts(sce) colLabels(sce) \u003c- factor(colData(sce)$Factor) # This uses replicate info from above targets file as pseudo-clusters  Embed with different methods and plot results Note, the embedding results are sequentially appended to the SingleCellExperiment object, meaning one can use the plot function whenever necessary.\n(a) tSNE sce \u003c- runTSNE(sce) reducedDimNames(sce)  ## [1] \"TSNE\"  plotTSNE(sce, colour_by=\"label\", text_by=\"label\")  (b) MDS sce \u003c- runMDS(sce) reducedDimNames(sce)  ## [1] \"TSNE\" \"MDS\"  plotMDS(sce, colour_by=\"label\", text_by=\"label\")  (c) UMAP sce \u003c- runUMAP(sce) reducedDimNames(sce)  ## [1] \"TSNE\" \"MDS\" \"UMAP\"  plotUMAP(sce, colour_by=\"label\", text_by=\"label\")  (d) PCA PCA plot for first two components.\nsce \u003c- runPCA(sce) # gives a warning due to small size of data set but it still works reducedDimNames(sce)  ## [1] \"TSNE\" \"MDS\" \"UMAP\" \"PCA\"  plotPCA(sce, colour_by=\"label\", text_by=\"label\")  Multiple components can be plotted in a series of pairwise plots. When more than two components are plotted, the diagonal boxes in the scatter plot matrix show the density for each component.\nsce \u003c- runPCA(sce, ncomponents=20) # gives a warning due to small size of data set but it still works reducedDimNames(sce)  ## [1] \"TSNE\" \"MDS\" \"UMAP\" \"PCA\"  plotPCA(sce, colour_by=\"label\", text_by=\"label\", ncomponents = 4)  scRNA-Seq data Load scRNA-Seq data The scRNAseq package is used to load the scRNA-Seq data set from Xenopus tail directly into a SingleCellExperiment object (Aztekin et al. 2019).\nlibrary(scRNAseq) sce \u003c- AztekinTailData()  Prepare data for plotting with embedding methods Similarly as above, the data are preprocessed (_e.g._normalized) to plot them with the run embedding functions from the scran package. In addition, the data is clustered with the quickCluster function.\nlibrary(scran); library(scater) sce \u003c- logNormCounts(sce) clusters \u003c- quickCluster(sce) # sce \u003c- computeSumFactors(sce, clusters=clusters) colLabels(sce) \u003c- factor(clusters) table(colLabels(sce))  To acclerate the testing performance of the following code, the size of the expression matrix is reduced to cell types with values \\(\\ge10^4\\).\nfilter \u003c- colSums(assays(sce)$counts) \u003e= 10^4 sce \u003c- sce[, filter]  To color items in the downstream dot plots by cell type instead of the above clustering result, one can use the cell type info under colData(). Note, this step is not evaluated here.\n# colLabels(sce) \u003c- colData(sce)$cluster  Embed with different methods and plot results As under the bulk RNA-Seq section, the embedding results are sequentially appended to the SingleCellExperiment object, meaning one can use the plot function whenever necessary.\n(a) tSNE sce \u003c- runTSNE(sce) reducedDimNames(sce) plotTSNE(sce, colour_by=\"label\", text_by=\"label\")  tSNE embedding of scRNA-Seq data\n  (b) MDS sce \u003c- runMDS(sce) reducedDimNames(sce) plotMDS(sce, colour_by=\"label\", text_by=\"label\")  MDS embedding of scRNA-Seq data\n  (c) UMAP sce \u003c- runUMAP(sce) # Note, the UMAP embedding is already stored in downloaded SingleCellExperiment object by authers. So one can just use this one or recompute it. reducedDimNames(sce) plotUMAP(sce, colour_by=\"label\", text_by=\"label\")  UMAP embedding of scRNA-Seq data\n  (d) PCA PCA result plotted for first two components.\nsce \u003c- runPCA(sce) reducedDimNames(sce) plotPCA(sce, colour_by=\"label\", text_by=\"label\")  PCA embedding of scRNA-Seq data\n  Multiple components can be plotted in a series of pairwise plots. When more than two components are plotted, the diagonal boxes in the scatter plot matrix show the density for each component.\nsce \u003c- runPCA(sce, ncomponents=20) reducedDimNames(sce) plotPCA(sce, colour_by=\"label\", text_by=\"label\", ncomponents = 4)  PCA embedding of scRNA-Seq data for multiple components\n  Version Information sessionInfo()  ## R version 4.1.0 (2021-05-18) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats4 stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] scater_1.20.0 ggplot2_3.3.3 ## [3] scran_1.20.0 scuttle_1.2.0 ## [5] SingleCellExperiment_1.14.0 SummarizedExperiment_1.22.0 ## [7] Biobase_2.52.0 GenomicRanges_1.44.0 ## [9] GenomeInfoDb_1.28.0 IRanges_2.26.0 ## [11] S4Vectors_0.30.0 BiocGenerics_0.38.0 ## [13] MatrixGenerics_1.4.0 matrixStats_0.58.0 ## ## loaded via a namespace (and not attached): ## [1] bitops_1.0-7 tools_4.1.0 ## [3] bslib_0.2.5.1 utf8_1.2.1 ## [5] R6_2.5.0 irlba_2.3.3 ## [7] vipor_0.4.5 uwot_0.1.10 ## [9] DBI_1.1.1 colorspace_2.0-1 ## [11] withr_2.4.2 gridExtra_2.3 ## [13] tidyselect_1.1.1 compiler_4.1.0 ## [15] BiocNeighbors_1.10.0 DelayedArray_0.18.0 ## [17] labeling_0.4.2 bookdown_0.22 ## [19] sass_0.4.0 scales_1.1.1 ## [21] stringr_1.4.0 digest_0.6.27 ## [23] rmarkdown_2.8 XVector_0.32.0 ## [25] pkgconfig_2.0.3 htmltools_0.5.1.1 ## [27] sparseMatrixStats_1.4.0 highr_0.9 ## [29] limma_3.48.0 rlang_0.4.11 ## [31] FNN_1.1.3 DelayedMatrixStats_1.14.0 ## [33] farver_2.1.0 jquerylib_0.1.4 ## [35] generics_0.1.0 jsonlite_1.7.2 ## [37] BiocParallel_1.26.0 dplyr_1.0.6 ## [39] RCurl_1.98-1.3 magrittr_2.0.1 ## [41] BiocSingular_1.8.0 GenomeInfoDbData_1.2.6 ## [43] Matrix_1.3-3 Rcpp_1.0.6 ## [45] ggbeeswarm_0.6.0 munsell_0.5.0 ## [47] fansi_0.4.2 viridis_0.6.1 ## [49] lifecycle_1.0.0 stringi_1.6.2 ## [51] yaml_2.2.1 edgeR_3.34.0 ## [53] zlibbioc_1.38.0 Rtsne_0.15 ## [55] grid_4.1.0 dqrng_0.3.0 ## [57] crayon_1.4.1 lattice_0.20-44 ## [59] cowplot_1.1.1 beachmat_2.8.0 ## [61] locfit_1.5-9.4 metapod_1.0.0 ## [63] knitr_1.33 pillar_1.6.1 ## [65] igraph_1.2.6 ScaledMatrix_1.0.0 ## [67] glue_1.4.2 evaluate_0.14 ## [69] blogdown_1.3 vctrs_0.3.8 ## [71] gtable_0.3.0 purrr_0.3.4 ## [73] assertthat_0.2.1 xfun_0.23 ## [75] rsvd_1.0.5 RSpectra_0.16-0 ## [77] viridisLite_0.4.0 tibble_3.1.2 ## [79] beeswarm_0.3.1 cluster_2.1.2 ## [81] bluster_1.2.0 statmod_1.4.36 ## [83] ellipsis_0.3.2  References Amezquita, Robert A, Aaron T L Lun, Etienne Becht, Vince J Carey, Lindsay N Carpp, Ludwig Geistlinger, Federico Marini, et al. 2020. “Orchestrating single-cell analysis with Bioconductor.” Nat. Methods 17 (2): 137–45. https://doi.org/10.1038/s41592-019-0654-x.\n Aztekin, C, T W Hiscock, J C Marioni, J B Gurdon, B D Simons, and J Jullien. 2019. “Identification of a regeneration-organizing cell in the Xenopus tail.” Science 364 (6441): 653–58. https://doi.org/10.1126/science.aav9996.\n Donaldson, Justin, and Maintainer Justin Donaldson. 2010. “Package ‘Tsne’.” CRAN Repository.\n Duò, Angelo, Mark D Robinson, and Charlotte Soneson. 2018. “A systematic performance evaluation of clustering methods for single-cell RNA-seq data.” F1000Res. 7 (July): 1141. https://doi.org/10.12688/f1000research.15666.3.\n Howard, Brian E, Qiwen Hu, Ahmet Can Babaoglu, Manan Chandra, Monica Borghi, Xiaoping Tan, Luyan He, et al. 2013. “High-Throughput RNA Sequencing of Pseudomonas-Infected Arabidopsis Reveals Hidden Transcriptome Complexity and Novel Splice Variants.” PLoS One 8 (10): e74183. https://doi.org/10.1371/journal.pone.0074183.\n Kiselev, Vladimir Yu, Kristina Kirschner, Michael T Schaub, Tallulah Andrews, Andrew Yiu, Tamir Chandra, Kedar N Natarajan, et al. 2017. “SC3: consensus clustering of single-cell RNA-seq data.” Nat. Methods 14 (5): 483–86. https://doi.org/10.1038/nmeth.4236.\n Linderman, George C, Manas Rachh, Jeremy G Hoskins, Stefan Steinerberger, and Yuval Kluger. 2019. “Fast interpolation-based t-SNE for improved visualization of single-cell RNA-seq data.” Nat. Methods 16 (3): 243–45. https://doi.org/10.1038/s41592-018-0308-4.\n McInnes, Leland, John Healy, and James Melville. 2018. “UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,” February. http://arxiv.org/abs/1802.03426.\n Senabouth, Anne, Samuel W Lukowski, Jose Alquicira Hernandez, Stacey B Andersen, Xin Mei, Quan H Nguyen, and Joseph E Powell. 2019. “ascend: R package for analysis of single-cell RNA-seq data.” Gigascience 8 (8). https://doi.org/10.1093/gigascience/giz087.\n Sun, Shiquan, Jiaqiang Zhu, Ying Ma, and Xiang Zhou. 2019. “Accuracy, robustness and scalability of dimensionality reduction methods for single-cell RNA-seq analysis.” Genome Biol. 20 (1): 269. https://doi.org/10.1186/s13059-019-1898-6.\n Sun, Shiquan, Jiaqiang Zhu, and Xiang Zhou. 2020. “Statistical analysis of spatial expression patterns for spatially resolved transcriptomic studies.” Nat. Methods, January. https://doi.org/10.1038/s41592-019-0701-7.\n  ","categories":"","description":"","excerpt":"Source code downloads: [ .Rmd ] [ .R ]\n Introduction This tutorial …","ref":"/tutorials/scrnaseq/scrnaseq/","tags":"","title":"scRNA-Seq Embedding Methods"},{"body":"     Source code downloads: [ .Rmd ] [ .html ] [ .R ]\n R Markdown Overview R Markdown combines markdown (an easy to write plain text format) with embedded R code chunks. When compiling R Markdown documents, the code components can be evaluated so that both the code and its output can be included in the final document. This makes analysis reports highly reproducible by allowing to automatically regenerate them when the underlying R code or data changes. R Markdown documents (.Rmd files) can be rendered to various formats including HTML and PDF. The R code in an .Rmd document is processed by knitr, while the resulting .md file is rendered by pandoc to the final output formats (e.g. HTML or PDF). Historically, R Markdown is an extension of the older Sweave/Latex environment. Rendering of mathematical expressions and reference management is also supported by R Markdown using embedded Latex syntax and Bibtex, respectively.\nQuick Start Install R Markdown To work with this tutorial, the rmarkdown package needs to be installed on a system.\ninstall.packages(\"rmarkdown\")  Initialize a new R Markdown (Rmd) script To minimize typing, it can be helful to start with an R Markdown template and then modify it as needed. Note the file name of an R Markdown scirpt needs to have the extension .Rmd. Template files for the following examples are available here:\n R Markdown sample script: sample.Rmd Bibtex file for handling citations and reference section: bibtex.bib  Users want to download these files, open the sample.Rmd file with their preferred R IDE (e.g. RStudio, vim or emacs), initilize an R session and then direct their R session to the location of these two files.\nMetadata section The metadata section (YAML header) in an R Markdown script defines how it will be processed and rendered. The metadata section also includes both title, author, and date information as well as options for customizing the output format. For instance, PDF and HTML output can be defined with pdf_document and html_document, respectively. The BiocStyle:: prefix will use the formatting style of the BiocStyle package from Bioconductor.\n --- title: \"My First R Markdown Document\" author: \"Author: First Last\" date: \"Last update: 12 May, 2022\" output: BiocStyle::html_document: toc: true toc_depth: 3 fig_caption: yes fontsize: 14pt bibliography: bibtex.bib ---  Render Rmd script An R Markdown script can be evaluated and rendered with the following render command or by pressing the knit button in RStudio. The output_format argument defines the format of the output (e.g. html_document or pdf_document). The setting output_format=\"all\" will generate all supported output formats. Alternatively, one can specify several output formats in the metadata section.\nrmarkdown::render(\"sample.Rmd\", clean=TRUE, output_format=\"BiocStyle::html_document\")  The following shows two options how to run the rendering from the command-line. To render to PDF format, use the argument setting: output_format=\"pdf_document\".\n$ Rscript -e \"rmarkdown::render('sample.Rmd', output_format='BiocStyle::html_document', clean=TRUE)\"  Alternatively, one can use a Makefile to evaluate and render an R Markdown script. A sample Makefile for rendering the above sample.Rmd can be downloaded here. To apply it to a custom Rmd file, one needs open the Makefile in a text editor and change the value assigned to MAIN (line 13) to the base name of the corresponding .Rmd file (e.g. assign systemPipeRNAseq if the file name is systemPipeRNAseq.Rmd). To execute the Makefile, run the following command from the command-line.\n$ make -B  R code chunks R Code Chunks can be embedded in an R Markdown script by using three backticks at the beginning of a new line along with arguments enclosed in curly braces controlling the behavior of the code. The following lines contain the plain R code. A code chunk is terminated by a new line starting with three backticks. The following shows an example of such a code chunk. Note the backslashes are not part of it. They have been added to print the code chunk syntax in this document.\n ```\\{r code_chunk_name, eval=FALSE\\} x \u003c- 1:10 ```  The following lists the most important arguments to control the behavior of R code chunks:\n r: specifies language for code chunk, here R chode_chunk_name: name of code chunk; this name needs to be unique within an Rmd eval: if assigned TRUE the code will be evaluated warning: if assigned FALSE warnings will not be shown message: if assigned FALSE messages will not be shown cache: if assigned TRUE results will be cached to reuse in future rendering instances fig.height: allows to specify height of figures in inches fig.width: allows to specify width of figures in inches  For more details on code chunk options see here.\nLearning Markdown The basic syntax of Markdown and derivatives like kramdown is extremely easy to learn. Rather than providing another introduction on this topic, here are some useful sites for learning Markdown:\n R Markdown Online Book Markdown Intro on GitHub Markdown Cheet Sheet Markdown Basics from RStudio R Markdown Cheat Sheet kramdown Syntax  Tables There are several ways to render tables. First, they can be printed within the R code chunks. Second, much nicer formatted tables can be generated with the functions kable, kableExtra, pander or xtable. The following example uses kable from the knitr package.\nWith knitr::kable library(knitr) kable(iris[1:12,])     Sepal.Length Sepal.Width Petal.Length Petal.Width Species     5.1 3.5 1.4 0.2 setosa   4.9 3.0 1.4 0.2 setosa   4.7 3.2 1.3 0.2 setosa   4.6 3.1 1.5 0.2 setosa   5.0 3.6 1.4 0.2 setosa   5.4 3.9 1.7 0.4 setosa   4.6 3.4 1.4 0.3 setosa   5.0 3.4 1.5 0.2 setosa   4.4 2.9 1.4 0.2 setosa   4.9 3.1 1.5 0.1 setosa   5.4 3.7 1.5 0.2 setosa   4.8 3.4 1.6 0.2 setosa    A much more elegant and powerful solution is to create fully interactive tables with the DT package. This JavaScirpt based environment provides a wrapper to the DataTables library using jQuery. The resulting tables can be sorted, queried and resized by the user. Note, R Markdown source files containing JavaScript components can only be rendered into HTML and not PDF.\nWith DT::datatable library(DT) datatable(iris)   {\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\"],[5.1,4.9,4.7,4.6,5,5.4,4.6,5,4.4,4.9,5.4,4.8,4.8,4.3,5.8,5.7,5.4,5.1,5.7,5.1,5.4,5.1,4.6,5.1,4.8,5,5,5.2,5.2,4.7,4.8,5.4,5.2,5.5,4.9,5,5.5,4.9,4.4,5.1,5,4.5,4.4,5,5.1,4.8,5.1,4.6,5.3,5,7,6.4,6.9,5.5,6.5,5.7,6.3,4.9,6.6,5.2,5,5.9,6,6.1,5.6,6.7,5.6,5.8,6.2,5.6,5.9,6.1,6.3,6.1,6.4,6.6,6.8,6.7,6,5.7,5.5,5.5,5.8,6,5.4,6,6.7,6.3,5.6,5.5,5.5,6.1,5.8,5,5.6,5.7,5.7,6.2,5.1,5.7,6.3,5.8,7.1,6.3,6.5,7.6,4.9,7.3,6.7,7.2,6.5,6.4,6.8,5.7,5.8,6.4,6.5,7.7,7.7,6,6.9,5.6,7.7,6.3,6.7,7.2,6.2,6.1,6.4,7.2,7.4,7.9,6.4,6.3,6.1,7.7,6.3,6.4,6,6.9,6.7,6.9,5.8,6.8,6.7,6.7,6.3,6.5,6.2,5.9],[3.5,3,3.2,3.1,3.6,3.9,3.4,3.4,2.9,3.1,3.7,3.4,3,3,4,4.4,3.9,3.5,3.8,3.8,3.4,3.7,3.6,3.3,3.4,3,3.4,3.5,3.4,3.2,3.1,3.4,4.1,4.2,3.1,3.2,3.5,3.6,3,3.4,3.5,2.3,3.2,3.5,3.8,3,3.8,3.2,3.7,3.3,3.2,3.2,3.1,2.3,2.8,2.8,3.3,2.4,2.9,2.7,2,3,2.2,2.9,2.9,3.1,3,2.7,2.2,2.5,3.2,2.8,2.5,2.8,2.9,3,2.8,3,2.9,2.6,2.4,2.4,2.7,2.7,3,3.4,3.1,2.3,3,2.5,2.6,3,2.6,2.3,2.7,3,2.9,2.9,2.5,2.8,3.3,2.7,3,2.9,3,3,2.5,2.9,2.5,3.6,3.2,2.7,3,2.5,2.8,3.2,3,3.8,2.6,2.2,3.2,2.8,2.8,2.7,3.3,3.2,2.8,3,2.8,3,2.8,3.8,2.8,2.8,2.6,3,3.4,3.1,3,3.1,3.1,3.1,2.7,3.2,3.3,3,2.5,3,3.4,3],[1.4,1.4,1.3,1.5,1.4,1.7,1.4,1.5,1.4,1.5,1.5,1.6,1.4,1.1,1.2,1.5,1.3,1.4,1.7,1.5,1.7,1.5,1,1.7,1.9,1.6,1.6,1.5,1.4,1.6,1.6,1.5,1.5,1.4,1.5,1.2,1.3,1.4,1.3,1.5,1.3,1.3,1.3,1.6,1.9,1.4,1.6,1.4,1.5,1.4,4.7,4.5,4.9,4,4.6,4.5,4.7,3.3,4.6,3.9,3.5,4.2,4,4.7,3.6,4.4,4.5,4.1,4.5,3.9,4.8,4,4.9,4.7,4.3,4.4,4.8,5,4.5,3.5,3.8,3.7,3.9,5.1,4.5,4.5,4.7,4.4,4.1,4,4.4,4.6,4,3.3,4.2,4.2,4.2,4.3,3,4.1,6,5.1,5.9,5.6,5.8,6.6,4.5,6.3,5.8,6.1,5.1,5.3,5.5,5,5.1,5.3,5.5,6.7,6.9,5,5.7,4.9,6.7,4.9,5.7,6,4.8,4.9,5.6,5.8,6.1,6.4,5.6,5.1,5.6,6.1,5.6,5.5,4.8,5.4,5.6,5.1,5.1,5.9,5.7,5.2,5,5.2,5.4,5.1],[0.2,0.2,0.2,0.2,0.2,0.4,0.3,0.2,0.2,0.1,0.2,0.2,0.1,0.1,0.2,0.4,0.4,0.3,0.3,0.3,0.2,0.4,0.2,0.5,0.2,0.2,0.4,0.2,0.2,0.2,0.2,0.4,0.1,0.2,0.2,0.2,0.2,0.1,0.2,0.2,0.3,0.3,0.2,0.6,0.4,0.3,0.2,0.2,0.2,0.2,1.4,1.5,1.5,1.3,1.5,1.3,1.6,1,1.3,1.4,1,1.5,1,1.4,1.3,1.4,1.5,1,1.5,1.1,1.8,1.3,1.5,1.2,1.3,1.4,1.4,1.7,1.5,1,1.1,1,1.2,1.6,1.5,1.6,1.5,1.3,1.3,1.3,1.2,1.4,1.2,1,1.3,1.2,1.3,1.3,1.1,1.3,2.5,1.9,2.1,1.8,2.2,2.1,1.7,1.8,1.8,2.5,2,1.9,2.1,2,2.4,2.3,1.8,2.2,2.3,1.5,2.3,2,2,1.8,2.1,1.8,1.8,1.8,2.1,1.6,1.9,2,2.2,1.5,1.4,2.3,2.4,1.8,1.8,2.1,2.4,2.3,1.9,2.3,2.5,2.3,1.9,2,2.3,1.8],[\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"setosa\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"versicolor\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\",\"virginica\"]],\"container\":\"\\n \\n \\n  \\n Sepal.Length\\n Sepal.Width\\n Petal.Length\\n Petal.Width\\n Species\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]} Figures Plots generated by the R code chunks in an R Markdown document can be automatically inserted in the output file. The size of the figure can be controlled with the fig.height and fig.width arguments.\nlibrary(ggplot2) dsmall \u003c- diamonds[sample(nrow(diamonds), 1000), ] ggplot(dsmall, aes(color, price/carat)) + geom_jitter(alpha = I(1 / 2), aes(color=color))  Sometimes it can be useful to explicitly write an image to a file and then insert that image into the final document by referencing its file name in the R Markdown source. For instance, this can be useful for time consuming analyses. The following code will generate a file named myplot.png. To insert the file in the final document, one can use standard Markdown or HTML syntax, e.g.: \u003cimg src=\"myplot.png\"/\u003e.\npng(\"myplot.png\") ggplot(dsmall, aes(color, price/carat)) + geom_jitter(alpha = I(1 / 2), aes(color=color)) dev.off()  ## png ## 2    Inline R code To evaluate R code inline, one can enclose an R expression with a single back-tick followed by r and then the actual expression. For instance, the back-ticked version of ‘r 1 + 1’ evaluates to 2 and ‘r pi’ evaluates to 3.1415927.\nMathematical equations To render mathematical equations, one can use standard Latex syntax. When expressions are enclosed with single $ signs then they will be shown inline, while enclosing them with double $$ signs will show them in display mode. For instance, the following Latex syntax d(X,Y) = \\sqrt[]{ \\sum_{i=1}^{n}{(x_{i}-y_{i})^2} } renders in display mode as follows:\n$$d(X,Y) = \\sqrt[]{ \\sum_{i=1}^{n}{(x_{i}-y_{i})^2} }$$\nTo learn LaTeX syntax for mathematical equations, one can consult various online manuals, such as this Wikibooks tutorial, or use an online equation rendering and checking tool, such as this one.\nCitations and bibliographies Citations and bibliographies can be autogenerated in R Markdown in a similar way as in Latex/Bibtex. Reference collections should be stored in a separate file in Bibtex or other supported formats. To cite a publication in an R Markdown script, one uses the syntax [@\u003cid1\u003e] where \u003cid1\u003e needs to be replaced with a reference identifier present in the Bibtex database listed in the metadata section of the R Markdown script (e.g. bibtex.bib). For instance, to cite Lawrence et al. (2013), one uses its reference identifier (e.g. Lawrence2013-kt) as \u003cid1\u003e (Lawrence et al. 2013). This will place the citation inline in the text and add the corresponding reference to a reference list at the end of the output document. For the latter a special section called References needs to be specified at the end of the R Markdown script. To fine control the formatting of citations and reference lists, users want to consult this R Markdown page. Also, for general reference management and obtaining references in Bibtex format Paperpile can be very helpful.\nViewing R Markdown report on HPCC cluster R Markdown reports located on UCR’s HPCC Cluster can be viewed locally in a web browser (without moving the source HTML) by creating a symbolic link from a user’s .html directory. This way any updates to the report will show up immediately without creating another copy of the HTML file. For instance, if user ttest has generated an R Markdown report under ~/bigdata/today/rmarkdown/sample.html, then the symbolic link can be created as follows:\ncd ~/.html ln -s ~/bigdata/today/rmarkdown/sample.html sample.html  After this one can view the report in a web browser using this URL https://cluster.hpcc.ucr.edu/~ttest/rmarkdown/sample.html. If necessary access to the URL can be restricted with a password following the instructions here.\nSession Info sessionInfo()  ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 11 (bullseye) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggplot2_3.3.6 DT_0.22 knitr_1.39 ## ## loaded via a namespace (and not attached): ## [1] bslib_0.3.1 compiler_4.2.0 pillar_1.7.0 jquerylib_0.1.4 ## [5] highr_0.9 tools_4.2.0 digest_0.6.29 viridisLite_0.4.0 ## [9] jsonlite_1.8.0 evaluate_0.15 lifecycle_1.0.1 tibble_3.1.7 ## [13] gtable_0.3.0 pkgconfig_2.0.3 rlang_1.0.2 DBI_1.1.2 ## [17] cli_3.3.0 crosstalk_1.2.0 yaml_2.3.5 blogdown_1.9 ## [21] xfun_0.30 fastmap_1.1.0 withr_2.5.0 dplyr_1.0.9 ## [25] stringr_1.4.0 generics_0.1.2 vctrs_0.4.1 htmlwidgets_1.5.4 ## [29] sass_0.4.1 tidyselect_1.1.2 grid_4.2.0 glue_1.6.2 ## [33] R6_2.5.1 fansi_1.0.3 rmarkdown_2.14 bookdown_0.26 ## [37] farver_2.1.0 purrr_0.3.4 magrittr_2.0.3 scales_1.2.0 ## [41] htmltools_0.5.2 ellipsis_0.3.2 assertthat_0.2.1 colorspace_2.0-3 ## [45] labeling_0.4.2 utf8_1.2.2 stringi_1.7.6 munsell_0.5.0 ## [49] crayon_1.5.1  References Lawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Comput. Biol. 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n  ","categories":"","description":"","excerpt":"     Source code downloads: [ .Rmd ] [ .html ] [ .R ]\n R Markdown …","ref":"/tutorials/rmarkdown/rmarkdown/","tags":"","title":"R Markdown Tutorial"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: .Rmd .R\n Introduction The following introduces gene and protein annotation systems that are widely used for functional enrichment analysis (FEA). These include among many other annotation systems: Gene Ontology (GO), Disease Ontology (DO) and pathway annotations, such as KEGG and Reactome. Examples of widely used statistical enrichment methods are introduced as well. These statistical FEA methods assess whether functional annotation terms are over-represented in a query gene set. In case of so called over-represention analysis (ORA) methods, such as Fisher’s exact and hypergeometric distribution tests, the query is usually a list of unranked gene identifiers (Falcon and Gentleman 2007). In contrast to this, Gene Set Enrichment Analysis (GSEA) algorithms use as query a score ranked list (e.g. all genes profiled by an assay) and assess whether annotation categories are more highly enriched among the highest ranking genes compared to random rankings (Subramanian et al. 2005; Sergushichev 2016; Duan et al. 2020). The sets in both the query and the annotation databases can be composed of genes, proteins, compounds or other factors. For simplicity, the term gene sets is used throughtout this text.\nFunctional Annotations Systems This section introduces a small selection of functional annotation systems, largely provided by Bioconductor packages. This includes code to inspect how the annotations are organized and how to access them.\nGene Ontology DB GO.db is a data package that stores the GO term information from the GO consortium in an SQLite database. Several accessor functions are provided to query the database. Organism specific gene to GO annotations are provied by organism data packages and/or Bioconductor’s AnntationHub. The following provide sample code for using GO.db as well as a organism database example.\n## Load GOstats library library(GOstats); library(GO.db) ## Print complete GO term information for \"GO:0003700\" GOTERM$\"GO:0003700\" ## Print parent and children terms for a GO ID GOMFPARENTS$\"GO:0003700\"; GOMFCHILDREN$\"GO:0003700\" ## Print complete lineages of parents and children for a GO ID GOMFANCESTOR$\"GO:0003700\"; GOMFOFFSPRING$\"GO:0003700\" ## Print number of GO terms in each of the 3 ontologies zz \u003c- eapply(GOTERM, function(x) x@Ontology); table(unlist(zz)) ## Gene to GO mappings for an organism (here Arabidopsis) library(org.At.tair.db) # For human use org.Hs.eg.db xx \u003c- as.list(org.At.tairGO2ALLTAIRS)  Pathway DBs KEGG KEGG.db The following load_keggList function returns the pathway annotations from the KEGG.db package for a species selected under the org argument (e.g. hsa, ath, dme, mmu, …). The resulting list object can be used for ORA or GSEA methods, e.g. by fgsea.\n## Define function to create KEGG pathway list db load_keggList \u003c- function(org=\"ath\") { suppressMessages(suppressWarnings(library(KEGG.db))) kegg_gene_list \u003c- as.list(KEGGPATHID2EXTID) # All organisms in kegg kegg_gene_list \u003c- kegg_gene_list[grepl(org, names(kegg_gene_list))] # Only human kegg_name_list \u003c- unlist(as.list(KEGGPATHID2NAME)) # All organisms in kegg kegg_name_list \u003c- kegg_name_list[gsub(paste0(\"^\", org), \"\", names(kegg_gene_list))] names(kegg_gene_list) \u003c- paste0(names(kegg_gene_list), \" (\", names(kegg_name_list), \") - \", kegg_name_list) return(kegg_gene_list) } ## Usage: keggdb \u003c- load_keggList(org=\"ath\") # org can be: hsa, ath, dme, mmu, ...  Additional packages for KEGG pathways:\n pathview: plotting pathways with quantitative information embedded KEGGREST: access via KEGG REST API Many additional packages can be found under Bioc’s KEGG View page here  Reactome reactome.db The following load_reacList function returns the pathway annotations from the reactome.db package for a species selected under the org argument (e.g. R-HSA, R-MMU, R-DME, R-CEL, …). The resulting list object can be used for various ORA or GSEA methods, e.g. by fgsea.\n## Define function to create Reactome pathway list db load_reacList \u003c- function(org=\"R-HSA\") { library(reactome.db) reac_gene_list \u003c- as.list(reactomePATHID2EXTID) # All organisms in reactome reac_gene_list \u003c- reac_gene_list[grepl(org, names(reac_gene_list))] # Only human reac_name_list \u003c- unlist(as.list(reactomePATHID2NAME)) # All organisms in reactome reac_name_list \u003c- reac_name_list[names(reac_gene_list)] names(reac_gene_list) \u003c- paste0(names(reac_gene_list), \" (\", names(reac_name_list), \") - \", gsub(\"^.*: \", \"\", reac_name_list)) return(reac_gene_list) } ## Usage: reacdb \u003c- load_reacList(org=\"R-HSA\")  A very useful query interface for Reactome is the ReactomeContentService4R package. Its vignette provides many useful examples, see here. A sample plot from ReactomeContentService4R is shown below. For metabolite (set) enrichment analysis (MEA/MSEA) users might also be interested in the MetaboAnalystR package that interfaces with the MataboAnalyst web service.\nFigure 1: Fireworks plot depicting genome-wide view of reactome pathways.\n  Functional Enrichment Analysis Methods Over-representation analysis (ORA) GOstats Package The GOstats package allows testing for both over and under representation of GO terms using either the standard Hypergeometric test or a conditional Hypergeometric test that uses the relationships among the GO terms for conditioning (Falcon and Gentleman 2007).\n## Load required packages library(GOstats); library(GO.db); library(org.At.tair.db) ## Define universe and test sample set geneUniverse \u003c- keys(org.At.tairGENENAME) geneSample \u003c- c(\"AT2G46210\", \"AT2G19880\", \"AT2G38910\", \"AT5G25140\", \"AT2G44525\") ## Generate params object params \u003c- new(\"GOHyperGParams\", geneIds = geneSample, universeGeneIds = geneUniverse, annotation=\"org.At.tair\", ontology = \"MF\", pvalueCutoff = 0.5, conditional = FALSE, testDirection = \"over\") ## Run enrichment test hgOver \u003c- hyperGTest(params) ## Viewing of results summary(hgOver)[1:4,] htmlReport(hgOver, file = \"MyhyperGresult.html\") # html file will be written to current working directory  GOHyperGAll and GOCluster_Report The following introduceds a GOCluster_Report convenience function from the systemPipeR package. The first part shows how to generate the proper catdb lookup data structure for any organism supported by BioMart (H Backman and Girke 2016). This more time consuming step needs to be performed only once.\n## Create a custom genome-to-GO lookup table for enrichment testing library(systemPipeR); library(biomaRt) listMarts() # To choose BioMart database listMarts(host = \"plants.ensembl.org\") ## Obtain annotations from BioMart listMarts() # To choose BioMart database m \u003c- useMart(\"plants_mart\", host = \"plants.ensembl.org\") listDatasets(m) m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"plants.ensembl.org\") listAttributes(m) # Choose data types you want to download go \u003c- getBM(attributes = c(\"go_id\", \"tair_locus\", \"namespace_1003\"), mart = m) go \u003c- go[go[, 3] != \"\", ]; go[, 3] \u003c- as.character(go[, 3]) go[go[, 3] == \"molecular_function\", 3] \u003c- \"F\"; go[go[, 3] == \"biological_process\", 3] \u003c- \"P\"; go[go[, 3] == \"cellular_component\", 3] \u003c- \"C\" go[1:4, ] dir.create(\"./GO\") write.table(go, \"GO/GOannotationsBiomart_mod.txt\", quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\") catdb \u003c- makeCATdb(myfile = \"GO/GOannotationsBiomart_mod.txt\", lib = NULL, org = \"\", colno = c(1, 2, 3), idconv = NULL) save(catdb, file=\"GO/catdb.RData\")  For the actual enrichment analysis one can load the catdb object from the corresponding file, and then perform batch GO term analysis where the results include all terms meeting a user-provided P-value cutoff as well as GO Slim terms.\n## Next time catDB can be loaded from file load(\"GO/catdb.RData\") ## Perform enrichment test on single gene set geneids \u003c- unique(as.character(catmap(catdb)$D_MF[,\"GeneID\"])) gene_set_list \u003c- sapply(c(\"Set1\", \"Set2\", \"Set3\"), function(x) sample(geneids, 100), simplify=FALSE) GOHyperGAll(catdb=catdb, gocat=\"MF\", sample=gene_set_list[[1]], Nannot=2)[1:20,] ## Batch analysis of many gene sets for all and slim terms goall \u003c- GOCluster_Report(catdb=catdb, setlist=gene_set_list, method=\"all\", id_type=\"gene\", CLSZ=2, cutoff=0.01, gocats=c(\"MF\", \"BP\", \"CC\"), recordSpecGO = NULL) ## GO Slim analysis by subsetting enrichment results accordingly m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"plants.ensembl.org\") goslimvec \u003c- as.character(getBM(attributes = c(\"goslim_goa_accession\"), mart = m)[, 1]) goslim \u003c- GOCluster_Report(catdb=catdb, setlist=gene_set_list, method=\"slim\",id_type=\"gene\", myslimv=goslimvec, CLSZ=2, cutoff=0.01, gocats = c(\"MF\", \"BP\", \"CC\"), recordSpecGO = NULL) ## Plot 'GOBatchResult' as bar plot goBarplot(goslim, gocat=\"MF\")  Figure 2: Batch ORA result of GO slim terms using 3 test gene sets.\n  Set enrichment analysis (SEA) fgsea Package The fgsea function performs gene set enrichment analysis (GSEA) on a score ranked gene list (Sergushichev 2016). Compared to other GESA implementations, fgsea is very fast. Its P-value estimation is based on an adaptive multi-level split Monte-Carlo scheme. In addition to its speed, it is very flexible in adopting custom annotation systems since it stores the gene-to-category annotations in a simple list object that is easy to create. The following uses the keegdb and reacdb lists created above as annotation systems.\n## Load packages and create sample ranked gene list library(fgsea); library(data.table); library(ggplot2); library(org.At.tair.db) set.seed(42) ## fgsea with KEGG (Arabidopsis) geneids \u003c- mappedkeys(org.At.tairCHR) exampleRanks \u003c- sort(setNames(sample(seq(-100,100, by=0.001), length(geneids)), geneids)) fgseaResKegg \u003c- fgsea(pathways=keggdb, stats=exampleRanks, minSize=15, maxSize=500) head(fgseaResKegg[order(pval), ]) plotEnrichment(keggdb[[\"ath00052 (00052) - Galactose metabolism\"]], exampleRanks) + labs(title=\"Galactose metabolism\") ## fgsea with Reactome (Human) geneids \u003c- unique(as.character(unlist(reacdb))) exampleRanks \u003c- sort(setNames(sample(seq(-100,100, by=0.001), length(geneids)), geneids)) fgseaResReac \u003c- fgsea(pathways=reacdb, stats=exampleRanks, minSize=15, maxSize=500) head(fgseaResReac[order(pval), ]) plotEnrichment(reacdb[[\"R-HSA-3247509 (R-HSA-3247509) - Chromatin modifying enzymes\"]], exampleRanks) + labs(title=\"Chromatin modifying enzymes\")  The plotEnrichment can be used to create enrichment plots. Additional examples are available in the vignette of the fgsea package here.\nFigure 3: Enrichment plot for selected pathway.\n  Version Information sessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] fgsea_1.20.0 ggplot2_3.3.5 BiocStyle_2.22.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.1.1 xfun_0.30 bslib_0.3.1 purrr_0.3.4 ## [5] lattice_0.20-45 colorspace_2.0-2 vctrs_0.3.8 generics_0.1.1 ## [9] htmltools_0.5.2 yaml_2.3.5 utf8_1.2.2 rlang_1.0.2 ## [13] jquerylib_0.1.4 pillar_1.6.4 glue_1.6.2 withr_2.4.3 ## [17] DBI_1.1.1 BiocParallel_1.28.2 lifecycle_1.0.1 stringr_1.4.0 ## [21] munsell_0.5.0 blogdown_1.8.2 gtable_0.3.0 codetools_0.2-18 ## [25] evaluate_0.15 knitr_1.37 fastmap_1.1.0 parallel_4.1.3 ## [29] fansi_0.5.0 Rcpp_1.0.8.2 scales_1.1.1 BiocManager_1.30.16 ## [33] jsonlite_1.8.0 gridExtra_2.3 fastmatch_1.1-3 digest_0.6.29 ## [37] stringi_1.7.6 bookdown_0.24 dplyr_1.0.7 grid_4.1.3 ## [41] cli_3.1.0 tools_4.1.3 magrittr_2.0.2 sass_0.4.0 ## [45] tibble_3.1.6 crayon_1.4.2 pkgconfig_2.0.3 ellipsis_0.3.2 ## [49] Matrix_1.4-0 data.table_1.14.2 assertthat_0.2.1 rmarkdown_2.13 ## [53] R6_2.5.1 compiler_4.1.3  References Duan, Yuzhu, Daniel S Evans, Richard A Miller, Nicholas J Schork, Steven R Cummings, and Thomas Girke. 2020. “signatureSearch: environment for gene expression signature searching and functional interpretation.” Nucleic Acids Res., October. https://doi.org/10.1093/nar/gkaa878.\n Falcon, S, and R Gentleman. 2007. “Using GOstats to test gene lists for GO term association.” Bioinformatics 23 (2): 257–58. https://doi.org/10.1093/bioinformatics/btl567.\n H Backman, Tyler W, and Thomas Girke. 2016. “systemPipeR: NGS workflow and report generation environment.” BMC Bioinformatics 17 (September): 388. https://doi.org/10.1186/s12859-016-1241-0.\n Sergushichev, Alexey. 2016. “An algorithm for fast preranked gene set enrichment analysis using cumulative statistic calculation.” bioRxiv. https://doi.org/10.1101/060012.\n Subramanian, A, P Tamayo, V K Mootha, S Mukherjee, B L Ebert, M A Gillette, A Paulovich, et al. 2005. “Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles.” Proc. Natl. Acad. Sci. U. S. A. 102 (43): 15545–50. https://doi.org/10.1073/pnas.0506580102.\n  ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/rfea/rfea/","tags":"","title":"Functional Enrichment Analysis"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: [ .Rmd ] [ .R ]\n Introduction  What is Clustering?  Clustering is the classification of data objects into similarity groups (clusters) according to a defined distance measure. It is used in many fields, such as machine learning, data mining, pattern recognition, image analysis, genomics, systems biology, etc. Machine learning typically regards data clustering as a form of unsupervised learning.   Why Clustering and Data Mining in R?}  Efficient data structures and functions for clustering Reproducible and programmable Comprehensive set of clustering and machine learning libraries Integration with many other data analysis tools   Useful Links  Cluster Task Views Machine Learning Task Views UCR Manual    Data Preprocessing Data Transformations Choice depends on data set!\n Center and standardize  Center: subtract from each value the mean of the corresponding vector Standardize: devide by standard deviation   Result: Mean = 0 and STDEV = 1   Center and scale with the scale() function  Center: subtract from each value the mean of the corresponding vector Scale: divide centered vector by their root mean square (rms): [ x_{rms} = \\sqrt[]{\\frac{1}{n-1}\\sum_{i=1}^{n}{x_{i}{^2}}} ]   Result: Mean = 0 and STDEV = 1   Log transformation Rank transformation: replace measured values by ranks No transformation  Distance Methods List of most common ones!\n Euclidean distance for two profiles X and Y: [ d(X,Y) = \\sqrt[]{ \\sum_{i=1}^{n}{(x_{i}-y_{i})^2} }]  Disadvantages: not scale invariant, not for negative correlations   Maximum, Manhattan, Canberra, binary, Minowski, … Correlation-based distance: 1-r  Pearson correlation coefficient (PCC): $$r = \\frac{n\\sum_{i=1}^{n}{x_{i}y_{i}} - \\sum_{i=1}^{n}{x_{i}} \\sum_{i=1}^{n}{y_{i}}}{ \\sqrt[]{(\\sum_{i=1}^{n}{x_{i}^2} - (\\sum_{i=1}^{n}{x_{i})^2}) (\\sum_{i=1}^{n}{y_{i}^2} - (\\sum_{i=1}^{n}{y_{i})^2})} }$$  Disadvantage: outlier sensitive   Spearman correlation coefficient (SCC)  Same calculation as PCC but with ranked values!      There are many more distance measures\n If the distances among items are quantifiable, then clustering is possible. Choose the most accurate and meaningful distance measure for a given field of application. If uncertain then choose several distance measures and compare the results.  Cluster Linkage   Clustering Algorithms Hierarchical Clustering Overview of algorithm  Identify clusters (items) with closest distance Join them to new clusters Compute distance between clusters (items) Return to step 1  Hierarchical clustering: agglomerative Approach   Hierarchical Clustering with Heatmap    A heatmap is a color coded table. To visually identify patterns, the rows and columns of a heatmap are often sorted by hierarchical clustering trees. In case of gene expression data, the row tree usually represents the genes, the column tree the treatments and the colors in the heat table represent the intensities or ratios of the underlying gene expression data set.  Hierarchical Clustering Approaches  Agglomerative approach (bottom-up)  R functions: hclust() and agnes()   Divisive approach (top-down)  R function: diana()    Tree Cutting to Obtain Discrete Clusters  Node height in tree Number of clusters Search tree nodes by distance cutoff  Examples Using hclust and heatmap.2 library(gplots) y \u003c- matrix(rnorm(500), 100, 5, dimnames=list(paste(\"g\", 1:100, sep=\"\"), paste(\"t\", 1:5, sep=\"\"))) heatmap.2(y) # Shortcut to final result  Stepwise Approach with Tree Cutting ## Row- and column-wise clustering hr \u003c- hclust(as.dist(1-cor(t(y), method=\"pearson\")), method=\"complete\") hc \u003c- hclust(as.dist(1-cor(y, method=\"spearman\")), method=\"complete\") ## Tree cutting mycl \u003c- cutree(hr, h=max(hr$height)/1.5); mycolhc \u003c- rainbow(length(unique(mycl)), start=0.1, end=0.9); mycolhc \u003c- mycolhc[as.vector(mycl)] ## Plot heatmap mycol \u003c- colorpanel(40, \"darkblue\", \"yellow\", \"white\") # or try redgreen(75) heatmap.2(y, Rowv=as.dendrogram(hr), Colv=as.dendrogram(hc), col=mycol, scale=\"row\", density.info=\"none\", trace=\"none\", RowSideColors=mycolhc)  K-Means Clustering Overview of algorithm  Choose the number of k clusters Randomly assign items to the k clusters Calculate new centroid for each of the k clusters Calculate the distance of all items to the k centroids Assign items to closest centroid Repeat until clusters assignments are stable    Examples km \u003c- kmeans(t(scale(t(y))), 3) km$cluster  ## g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 g11 g12 g13 g14 g15 g16 g17 g18 g19 g20 ## 1 3 1 1 1 1 3 2 1 3 1 3 1 2 3 2 2 3 1 1 ## g21 g22 g23 g24 g25 g26 g27 g28 g29 g30 g31 g32 g33 g34 g35 g36 g37 g38 g39 g40 ## 1 2 1 2 2 3 2 1 1 3 3 3 2 3 1 1 3 3 1 1 ## g41 g42 g43 g44 g45 g46 g47 g48 g49 g50 g51 g52 g53 g54 g55 g56 g57 g58 g59 g60 ## 2 2 1 2 2 2 1 3 3 2 2 2 3 3 2 2 1 2 3 2 ## g61 g62 g63 g64 g65 g66 g67 g68 g69 g70 g71 g72 g73 g74 g75 g76 g77 g78 g79 g80 ## 2 3 3 3 3 3 2 1 1 2 1 3 1 1 1 2 3 3 1 2 ## g81 g82 g83 g84 g85 g86 g87 g88 g89 g90 g91 g92 g93 g94 g95 g96 g97 g98 g99 g100 ## 2 1 3 3 2 2 1 2 1 1 2 3 3 2 2 2 3 1 1 1  Fuzzy C-Means Clustering  In contrast to strict (hard) clustering approaches, fuzzy (soft) clustering methods allow multiple cluster memberships of the clustered items (Hathaway, Bezdek, and Pal 1996). This is commonly achieved by assigning to each item a weight of belonging to each cluster. Thus, items at the edge of a cluster, may be in a cluster to a lesser degree than items at the center of a cluster. Typically, each item has as many coefficients (weights) as there are clusters that sum up for each item to one.  Examples Fuzzy Clustering with fanny library(cluster) # Loads the cluster library. fannyy \u003c- fanny(y, k=4, metric = \"euclidean\", memb.exp = 1.2) round(fannyy$membership, 2)[1:4,]  ## [,1] [,2] [,3] [,4] ## g1 0.97 0.02 0.02 0.00 ## g2 0.07 0.91 0.01 0.00 ## g3 0.28 0.08 0.44 0.20 ## g4 0.84 0.03 0.12 0.01  fannyy$clustering # Hard clustering result  ## g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 g11 g12 g13 g14 g15 g16 g17 g18 g19 g20 ## 1 2 3 1 3 3 1 2 1 2 3 4 1 2 4 2 2 4 1 4 ## g21 g22 g23 g24 g25 g26 g27 g28 g29 g30 g31 g32 g33 g34 g35 g36 g37 g38 g39 g40 ## 1 4 3 2 2 2 2 1 3 4 1 1 4 4 1 3 1 1 4 1 ## g41 g42 g43 g44 g45 g46 g47 g48 g49 g50 g51 g52 g53 g54 g55 g56 g57 g58 g59 g60 ## 2 4 1 2 2 3 3 1 2 2 3 4 4 1 4 3 3 4 2 2 ## g61 g62 g63 g64 g65 g66 g67 g68 g69 g70 g71 g72 g73 g74 g75 g76 g77 g78 g79 g80 ## 3 2 3 2 4 2 2 3 1 2 3 1 3 1 1 3 4 1 3 3 ## g81 g82 g83 g84 g85 g86 g87 g88 g89 g90 g91 g92 g93 g94 g95 g96 g97 g98 g99 g100 ## 3 1 2 2 2 2 3 2 1 3 4 2 2 2 2 2 2 1 3 1  (fannyyMA \u003c- fannyy$membership \u003e 0.20)[1:4,] # Soft clustering result  ## [,1] [,2] [,3] [,4] ## g1 TRUE FALSE FALSE FALSE ## g2 FALSE TRUE FALSE FALSE ## g3 TRUE FALSE TRUE FALSE ## g4 TRUE FALSE FALSE FALSE  apply(fannyyMA, 1, which)[1:4] # First 4 clusters  ## $g1 ## [1] 1 ## ## $g2 ## [1] 2 ## ## $g3 ## [1] 1 3 ## ## $g4 ## [1] 1  Principal Component Analysis (PCA) Principal components analysis (PCA) is a data reduction technique that allows to simplify multidimensional data sets to 2 or 3 dimensions for plotting purposes and visual variance analysis.\nBasic Steps  Center (and standardize) data First principal component axis  Across centroid of data cloud Distance of each point to that line is minimized, so that it crosses the maximum variation of the data cloud   Second principal component axis  Orthogonal to first principal component Along maximum variation in the data   First PCA axis becomes x-axis and second PCA axis y-axis Continue process until the necessary number of principal components is obtained    Example pca \u003c- prcomp(y, scale=T) summary(pca) # Prints variance summary for all principal components  ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 ## Standard deviation 1.179 1.1177 0.9965 0.8492 0.8041 ## Proportion of Variance 0.278 0.2498 0.1986 0.1442 0.1293 ## Cumulative Proportion 0.278 0.5279 0.7265 0.8707 1.0000  plot(pca$x, pch=20, col=\"blue\", type=\"n\") # To plot dots, drop type=\"n\" text(pca$x, rownames(pca$x), cex=0.8)  1st and 2nd principal components explain x% of variance in data. Multidimensional Scaling (MDS)  Alternative dimensionality reduction approach Represents distances in 2D or 3D space Starts from distance matrix (PCA uses data points)  Example The following example performs MDS analysis with cmdscale on the geographic distances among European cities.\nloc \u003c- cmdscale(eurodist) plot(loc[,1], -loc[,2], type=\"n\", xlab=\"\", ylab=\"\", main=\"cmdscale(eurodist)\") text(loc[,1], -loc[,2], rownames(loc), cex=0.8)  Biclustering Finds in matrix subgroups of rows and columns which are as similar as possible to each other and as different as possible to the remaining data points.\n   Unclustered ————————–\u003e Clustered\n Similarity Measures for Clusters  Compare the numbers of identical and unique item pairs appearing in cluster sets Achieved by counting the number of item pairs found in both clustering sets (a) as well as the pairs appearing only in the first (b) or the second (c) set. With this a similarity coefficient, such as the Jaccard index, can be computed. The latter is defined as the size of the intersect divided by the size of the union of two sample sets: a/(a+b+c). In case of partitioning results, the Jaccard Index measures how frequently pairs of items are joined together in two clustering data sets and how often pairs are observed only in one set. Related coefficient are the Rand Index and the Adjusted Rand Index. These indices also consider the number of pairs (d) that are not joined together in any of the clusters in both sets.  Example: Jaccard index for cluster sets The following imports the cindex() function and computes the Jaccard Index for two sample clusters.\nsource(\"http://faculty.ucr.edu/~tgirke/Documents/R_BioCond/My_R_Scripts/clusterIndex.R\") library(cluster); y \u003c- matrix(rnorm(5000), 1000, 5, dimnames=list(paste(\"g\", 1:1000, sep=\"\"), paste(\"t\", 1:5, sep=\"\"))); clarax \u003c- clara(y, 49); clV1 \u003c- clarax$clustering; clarax \u003c- clara(y, 50); clV2 \u003c- clarax$clustering ci \u003c- cindex(clV1=clV1, clV2=clV2, self=FALSE, minSZ=1, method=\"jaccard\") ci[2:3] # Returns Jaccard index and variables used to compute it  ## $variables ## a b c ## 6905 7441 7145 ## ## $Jaccard_Index ## [1] 0.3212973  Clustering cluster sets with Jaccard index The following example shows how one can cluster entire cluster result sets. First, 10 sample cluster results are created with Clara using k-values from 3 to 12. The results are stored as named clustering vectors in a list object. Then a nested sapply loop is used to generate a similarity matrix of Jaccard Indices for the clustering results. After converting the result into a distance matrix, hierarchical clustering is performed with hclust.}\nclVlist \u003c- lapply(3:12, function(x) clara(y[1:30, ], k=x)$clustering); names(clVlist) \u003c- paste(\"k\", \"=\", 3:12) d \u003c- sapply(names(clVlist), function(x) sapply(names(clVlist), function(y) cindex(clV1=clVlist[[y]], clV2=clVlist[[x]], method=\"jaccard\")[[3]])) hv \u003c- hclust(as.dist(1-d)) plot(as.dendrogram(hv), edgePar=list(col=3, lwd=4), horiz=T, main=\"Similarities of 10 Clara Clustering Results for k: 3-12\")   Remember: there are many additional clustering algorithms. Additional details can be found in the Clustering Section of the R/Bioconductor Manual.  Clustering Exercises Data Preprocessing Scaling ## Sample data set set.seed(1410) y \u003c- matrix(rnorm(50), 10, 5, dimnames=list(paste(\"g\", 1:10, sep=\"\"), paste(\"t\", 1:5, sep=\"\"))) dim(y)  ## [1] 10 5  ## Scaling yscaled \u003c- t(scale(t(y))) # Centers and scales y row-wise apply(yscaled, 1, sd)  ## g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 ## 1 1 1 1 1 1 1 1 1 1  Distance Matrices Euclidean distance matrix dist(y[1:4,], method = \"euclidean\")  ## g1 g2 g3 ## g2 4.793697 ## g3 4.932658 6.354978 ## g4 4.033789 4.788508 1.671968  Correlation-based distance matrix Correlation matrix\nc \u003c- cor(t(y), method=\"pearson\") as.matrix(c)[1:4,1:4]  ## g1 g2 g3 g4 ## g1 1.00000000 -0.2965885 -0.00206139 -0.4042011 ## g2 -0.29658847 1.0000000 -0.91661118 -0.4512912 ## g3 -0.00206139 -0.9166112 1.00000000 0.7435892 ## g4 -0.40420112 -0.4512912 0.74358925 1.0000000  Correlation-based distance matrix\nd \u003c- as.dist(1-c) as.matrix(d)[1:4,1:4]  ## g1 g2 g3 g4 ## g1 0.000000 1.296588 1.0020614 1.4042011 ## g2 1.296588 0.000000 1.9166112 1.4512912 ## g3 1.002061 1.916611 0.0000000 0.2564108 ## g4 1.404201 1.451291 0.2564108 0.0000000  Hierarchical Clustering with hclust Hierarchical clustering with complete linkage and basic tree plotting\nhr \u003c- hclust(d, method = \"complete\", members=NULL) names(hr)  ## [1] \"merge\" \"height\" \"order\" \"labels\" \"method\" \"call\" ## [7] \"dist.method\"  par(mfrow = c(1, 2)); plot(hr, hang = 0.1); plot(hr, hang = -1)  Tree plotting I plot(as.dendrogram(hr), edgePar=list(col=3, lwd=4), horiz=T)  Tree plotting II The ape library provides more advanced features for tree plotting\nlibrary(ape) plot.phylo(as.phylo(hr), type=\"p\", edge.col=4, edge.width=2, show.node.label=TRUE, no.margin=TRUE)  Tree Cutting Accessing information in hclust objects\nhr  ## ## Call: ## hclust(d = d, method = \"complete\", members = NULL) ## ## Cluster method : complete ## Number of objects: 10  ## Print row labels in the order they appear in the tree hr$labels[hr$order]  ## [1] \"g10\" \"g3\" \"g4\" \"g2\" \"g9\" \"g6\" \"g7\" \"g1\" \"g5\" \"g8\"  Tree cutting with cutree\nmycl \u003c- cutree(hr, h=max(hr$height)/2) mycl[hr$labels[hr$order]]  ## g10 g3 g4 g2 g9 g6 g7 g1 g5 g8 ## 3 3 3 2 2 5 5 1 4 4  Heatmaps With heatmap.2 All in one step: clustering and heatmap plotting\nlibrary(gplots) heatmap.2(y, col=redgreen(75))  With pheatmap All in one step: clustering and heatmap plotting\nlibrary(pheatmap); library(\"RColorBrewer\") pheatmap(y, color=brewer.pal(9,\"Blues\"))  Customizing heatmaps Customizes row and column clustering and shows tree cutting result in row color bar. Additional color schemes can be found here.\nhc \u003c- hclust(as.dist(1-cor(y, method=\"spearman\")), method=\"complete\") mycol \u003c- colorpanel(40, \"darkblue\", \"yellow\", \"white\") heatmap.2(y, Rowv=as.dendrogram(hr), Colv=as.dendrogram(hc), col=mycol, scale=\"row\", density.info=\"none\", trace=\"none\", RowSideColors=as.character(mycl))  K-Means Clustering with PAM Runs K-means clustering with PAM (partitioning around medoids) algorithm and shows result in color bar of hierarchical clustering result from before.\nlibrary(cluster) pamy \u003c- pam(d, 4) (kmcol \u003c- pamy$clustering)  ## g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 ## 1 2 3 3 4 4 4 4 2 3  heatmap.2(y, Rowv=as.dendrogram(hr), Colv=as.dendrogram(hc), col=mycol, scale=\"row\", density.info=\"none\", trace=\"none\", RowSideColors=as.character(kmcol))  K-Means Fuzzy Clustering Performs k-means fuzzy clustering\nlibrary(cluster) fannyy \u003c- fanny(d, k=4, memb.exp = 1.5) round(fannyy$membership, 2)[1:4,]  ## [,1] [,2] [,3] [,4] ## g1 1.00 0.00 0.00 0.00 ## g2 0.00 0.99 0.00 0.00 ## g3 0.02 0.01 0.95 0.03 ## g4 0.00 0.00 0.99 0.01  fannyy$clustering  ## g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 ## 1 2 3 3 4 4 4 4 2 3  ## Returns multiple cluster memberships for coefficient above a certain ## value (here \u003e0.1) fannyyMA \u003c- round(fannyy$membership, 2) \u003e 0.10 apply(fannyyMA, 1, function(x) paste(which(x), collapse=\"_\"))  ## g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 ## \"1\" \"2\" \"3\" \"3\" \"4\" \"4\" \"4\" \"2_4\" \"2\" \"3\"  Multidimensional Scaling (MDS) Performs MDS analysis on the geographic distances between European cities\nloc \u003c- cmdscale(eurodist) ## Plots the MDS results in 2D plot. The minus is required in this example to ## flip the plotting orientation. plot(loc[,1], -loc[,2], type=\"n\", xlab=\"\", ylab=\"\", main=\"cmdscale(eurodist)\") text(loc[,1], -loc[,2], rownames(loc), cex=0.8)  Principal Component Analysis (PCA) Performs PCA analysis after scaling the data. It returns a list with class prcomp that contains five components: (1) the standard deviations (sdev) of the principal components, (2) the matrix of eigenvectors (rotation), (3) the principal component data (x), (4) the centering (center) and (5) scaling (scale) used.\nlibrary(scatterplot3d) pca \u003c- prcomp(y, scale=TRUE) names(pca)  ## [1] \"sdev\" \"rotation\" \"center\" \"scale\" \"x\"  summary(pca) # Prints variance summary for all principal components.  ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 ## Standard deviation 1.3611 1.1777 1.0420 0.69264 0.4416 ## Proportion of Variance 0.3705 0.2774 0.2172 0.09595 0.0390 ## Cumulative Proportion 0.3705 0.6479 0.8650 0.96100 1.0000  scatterplot3d(pca$x[,1:3], pch=20, color=\"blue\")  Additional Exercises See here\nVersion Information sessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] scatterplot3d_0.3-41 RColorBrewer_1.1-2 pheatmap_1.0.12 cluster_2.1.2 ## [5] gplots_3.1.1 ape_5.5 ggplot2_3.3.5 BiocStyle_2.22.0 ## ## loaded via a namespace (and not attached): ## [1] gtools_3.9.2 tidyselect_1.1.1 xfun_0.30 bslib_0.3.1 ## [5] purrr_0.3.4 lattice_0.20-45 colorspace_2.0-2 vctrs_0.3.8 ## [9] generics_0.1.1 htmltools_0.5.2 yaml_2.3.5 utf8_1.2.2 ## [13] rlang_1.0.2 jquerylib_0.1.4 pillar_1.6.4 glue_1.6.2 ## [17] withr_2.4.3 DBI_1.1.1 lifecycle_1.0.1 stringr_1.4.0 ## [21] munsell_0.5.0 blogdown_1.8.2 gtable_0.3.0 caTools_1.18.2 ## [25] codetools_0.2-18 evaluate_0.15 knitr_1.37 fastmap_1.1.0 ## [29] parallel_4.1.3 fansi_0.5.0 highr_0.9 Rcpp_1.0.8.2 ## [33] KernSmooth_2.23-20 scales_1.1.1 BiocManager_1.30.16 jsonlite_1.8.0 ## [37] digest_0.6.29 stringi_1.7.6 bookdown_0.24 dplyr_1.0.7 ## [41] grid_4.1.3 cli_3.1.0 tools_4.1.3 bitops_1.0-7 ## [45] magrittr_2.0.2 sass_0.4.0 tibble_3.1.6 crayon_1.4.2 ## [49] pkgconfig_2.0.3 ellipsis_0.3.2 assertthat_0.2.1 rmarkdown_2.13 ## [53] R6_2.5.1 nlme_3.1-155 compiler_4.1.3  References Hathaway, R J, J C Bezdek, and N R Pal. 1996. “Sequential Competitive Learning and the Fuzzy c-Means Clustering Algorithms.” Neural Netw. 9 (5): 787–96. http://www.hubmed.org/display.cgi?uids=12662563.\n  ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/rclustering/rclustering/","tags":"","title":"Cluster Analysis in R"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: .Rmd .R\n What is Shiny? Shiny is an R-based environment for building interactive web applications for data analysis and exploration (“Shiny - Tutorial,” n.d.). One can create a beautiful web app in Shiny with only knowledge of basic R. Shiny apps can be deployed on local computers or web servers including custom and cloud-based servers (e.g. AWS, GCP, shinyapp.io service). The basic structure of a Shiny app is an app.R script containing the user interface (UI) and server components:\n UI  ui \u003c- fluidPage()  Server as a function  server \u003c- function(input, output, session) {}  Statement to define the whole shiny app  shinyApp(ui = ui, server = server)  Get started The following introduces the basic development routines for Shiny apps using RStudio. Most other\nR IDEs can be used here as well. For beginners the interactive test and debugging functionalities of RStudio simplify the development process of Shiny apps.\nTo start, open an Rscript in Rstudio and load the shiny library\nlibrary(shiny)  When typing shiny in the code editor, the code/function suggestion dialog allows to select the corresponding code snippet. If the dialog does not show up automatically, one can press Ctrl + space to open it (on Mac use command instead of Ctrl).\nAfter selecting shinyapp in the dialog, the following code snippet will be inserted in the code editor.\nlibrary(shiny) ui \u003c- fluidPage( ) server \u003c- function(input, output, session) { } shinyApp(ui, server)  This is the basic structure of a Shiny app. Now, save it (Ctrl/Cmd + s) to your current working directory and name it “app.R.” You should see the script file has been recognized as a Shiny app file and it is runnable with a green Run App button on the top-right corner of the script file.\nIf you run current app, it should open up a tab in the browser or open the Viewer in Rstudio of the app. You can change where to open it with the down-arrow button next to the Run App button. Current app will give you an empty app because we have not filled it with any components.\nAdd UI Basic layout UI is the skeleton of the app. In Shiny, UI is usually defined by a page structure by calling the one of the page function, like a fluidPage, fixedPage, etc. Then, you can start to fill components within this structure. It is similar as writing HTML with R wrappers. It will be no problem if you do not know what is HTML, remember some of the following tags (functions) will be enough.\n h1-h6: Level 1 to level 6 headings, similar as # … ###### in Rmarkdown p (paragraph): basic text container fluidRow: create a row in the app (horizontal space) to hold children components column: divide a vertical space in the app. The width is 1-12. This relates to the HTML framework Bootstrap that Shiny is built on. You can divide the parent container vertically from 1-12, 1 is the minimum width and 12 is max. a: create a link img: add an image code: code blocks  Let us try to put things above to create an app:\nlibrary(shiny) ui \u003c- fluidPage( h1(\"My biggest title\"), column( width = 6, h2(\"section 1\"), p(\"some text blablablablablablablablablablablablablabla\"), fluidRow( column( 6, h4(\"sub section 1\"), p(\"some text blablablablablablablablablablablablablabla\"), code(\"1 + 1 = 2; mean(1, 2, 3)\") ), column( 6, h4(\"sub section 2\"), p(\"some text blablablablablablablablablablablablablabla\"), a(href = \"https://google.com\", \"Link to Google\") ) ) ), column( width = 6, h2(\"section 2\"), p(\"some text blablablablablablablablablablablablablabla\"), p(\"some other text blablablablablablablablablablablablablabla\"), img(src = \"https://raw.githubusercontent.com/systemPipeR/systemPipeR.github.io/main/static/images/systemPipeR.png\", style = \"height: 280px\") ) ) server \u003c- function(input, output, session) {} shinyApp(ui, server)  High-level layout functions There are some high-level functions in Shiny to help you create these layout easily:\n titlePanel, splitLayout, verticalLayout, sidebarLayout, tabsetPanel and more. Read more on Shiny reference manual.  Sidebar layout:\nui \u003c- fluidPage( titlePanel(\"Hello Shiny!\"), sidebarLayout( sidebarPanel( h3(\"Side bar content\"), p(\"...\"), p(\"...\"), p(\"...\"), p(\"...\") ), mainPanel( h3(\"Main panel content\"), p(\"...\"), p(\"...\"), p(\"...\"), p(\"...\") ) ) ) server \u003c- function(input, output, session) {} shinyApp(ui, server)  Tabset layout:\nui \u003c- fluidPage( tabsetPanel( tabPanel(\"panel1\", p(\"Panel 1 content\")), tabPanel(\"panel2\", p(\"Panel 2 content\")), tabPanel(\"panel3\", p(\"Panel 3 content\")) ) ) server \u003c- function(input, output, session) {} shinyApp(ui, server)  Interactive components The most important part that makes a Shiny app different from a static website is the interactivity. User interactions are achieved with interactive components (widgets) on the UI side. These components inlcude but not limited to, buttons, text inputs, select inputs, check boxes, numeric inputs, dates and more. Read more of these components on Shiny website.\nHere we demonstrate some of these components in addition to the sidebar layout.\nui \u003c- fluidPage( titlePanel(\"Shiny inputs\"), sidebarLayout( sidebarPanel( h3(\"Some input examples\"), actionButton(inputId = \"btn\", label = \"A button\", icon = icon(\"dna\")), selectInput( inputId = \"select\", label = \"Select input\", choices = c(\"apple\", \"banana\", \"orange\") ), sliderInput( inputId = \"slider\", label = \"Slider input\", min = 0, max = 100, value = 0 ), textInput( inputId = \"text\", label = \"Text input\", placeholder = \"Type some text\" ) ), mainPanel( h3(\"Main panel content\"), p(\"...\"), p(\"...\"), p(\"...\"), p(\"...\") ) ) ) server \u003c- function(input, output, session) {} shinyApp(ui, server)  Server So far we have learned how to create the Shiny UI and add some interactive components to it, but when we interact with these components, nothing will happen because we have not added any backend logic to tell the app what will happen when these components are interacted (clicked, selected, changed …). Here, the server comes in for the role. We define most interactive logic in the server (much of the logic can be done with Javascript too, but that is advanced, not be discussed here).\nServer function In Shiny, UI is defined as a list-like R object, but server is different, it is a function. From the different object type you would know UI is evaluated before app runs and done. Server is waiting to be evaluated after app run. It is subjected to accept different function inputs and to be re-evaluated many times.\nThree important arguments of the server function:\n input: list-like object with all user inputs from the UI (get value from UI) output: list-like object with data sending to the browser (set value to UI) session: R6-like class, containing all information of current app session (advanced use only).  Reactivity The entire Shiny server is built on a concept called reactivity. It defines how reactive source, usually UI interaction (button clicking) to trigger some server expressions to be evaluated and finally result some updates to the reactive endpoint, usually like sending data to the UI, making a new plot.\nWatch reactivity change Reactivity objects in Shiny (change of this object will trigger shiny to do the recalculation):\n input: each individual input object, like input$id1, input$id2, input$id3 reactive: expressions defined in reactive({...}) reactiveVal: a single object that will be changed at some point. reactiveValues: a list of objects that will be changed at some point.  There are a few different ways to watch for Shiny reactivity.\n  observe: this observer triggers if any reactive expression in is changed\nobserve({ input$btn1 input$btn2 # do some thing })  Both clicking on button 1 and button 2 will trigger this expression.\n  observeEvent: this observer triggers if any reactive expression in is changed\nobserveEvent(eventExpr = input$btn1, { input$btn2 # do some thing })  Only clicking on button 1 but not button 2 will trigger this expression.\n  reactive: all reactive expressions inside will trigger this reactive expression to be redefined. It is triggered similar to observe but will return a value to be used in other reactive expressions.\nmy_react \u003c- reactive({ input$btn1 input$btn2 # do some thing # return the final value })  Both clicking on button 1 and button 2 will trigger this expression.\n  All shiny render events: all reactive expressions inside shiny render events like renderPlot, renderText, … will trigger the rendering to be recalcualted. to be used in other reactive expressions.\noutput$myplot \u003c- renderPlot({ input$btn1 input$btn2 plot(1, 2) })  Both clicking on button 1 and button 2 will trigger this expression.\n  Use input to get values Here we use the sidebarLayout example and add some server code to demo how to get values from user interactions. Run this example and watch the output in your R console.\nlibrary(shiny) ui \u003c- fluidPage( titlePanel(\"Shiny inputs\"), sidebarLayout( sidebarPanel( h3(\"Some input examples\"), actionButton(inputId = \"btn\", label = \"A button\", icon = icon(\"dna\")), selectInput( inputId = \"select\", label = \"Select input\", choices = c(\"apple\", \"banana\", \"orange\") ), sliderInput( inputId = \"slider\", label = \"Slider input\", min = 0, max = 100, value = 0 ), textInput( inputId = \"text\", label = \"Text input\", placeholder = \"Type some text\" ) ), mainPanel( h3(\"Main panel content\") ) ) ) server \u003c- function(input, output, session) { observe({ cat(\"*****observe*****\\n\") cat(\"Button clicked\", input$btn, \"times\\n\") cat(\"Selected value is\", input$select, \"\\n\") cat(\"Slider value is\", input$slider, \"\\n\") cat(\"Typed balue is\", input$text, \"\\n\") }) # or use observeEvent, watch closely the difference of when the text will be printed on console # observeEvent(input$select, { # cat(\"*****observeEvent*****\\n\") # cat(\"Button clicked\", input$btn, \"times\\n\") # cat(\"Selected value is\", input$select, \"\\n\") # cat(\"Slider value is\", input$slider, \"\\n\") # cat(\"Typed balue is\", input$text, \"\\n\") # }) } shinyApp(ui, server)  Use output to update UI Instead of print the text to R console, we can use output to send text to users. Here we use output$text_out \u003c- renderPrint({...}) on server to create the expression. Also we need to add verbatimTextOutput(outputId = \"text_out\") inside mainPanel to indicate where we want to display the text on UI.\nIn shiny all render functions renderXxx are coupled with a xxxOutput function as container on UI. Read more of them on Shiny website.\nlibrary(shiny) ui \u003c- fluidPage( titlePanel(\"Shiny inputs\"), sidebarLayout( sidebarPanel( h3(\"Some input examples\"), actionButton(inputId = \"btn\", label = \"A button\", icon = icon(\"dna\")), selectInput( inputId = \"select\", label = \"Select input\", choices = c(\"apple\", \"banana\", \"orange\") ), sliderInput( inputId = \"slider\", label = \"Slider input\", min = 0, max = 100, value = 0 ), textInput( inputId = \"text\", label = \"Text input\", placeholder = \"Type some text\" ) ), mainPanel( h3(\"Main panel content\"), verbatimTextOutput(outputId = \"text_out\") ) ) ) server \u003c- function(input, output, session) { output$text_out \u003c- renderPrint({ cat(\"*****observe*****\\n\") cat(\"Button clicked\", input$btn, \"times\\n\") cat(\"Selected value is\", input$select, \"\\n\") cat(\"Slider value is\", input$slider, \"\\n\") cat(\"Typed balue is\", input$text, \"\\n\") }) } shinyApp(ui, server)  Example 2: create an interactive plot\nlibrary(shiny) library(ggplot2) ui \u003c- fluidPage( titlePanel(\"Plot control\"), sidebarLayout( sidebarPanel( h3(\"Some input examples\"), selectInput( inputId = \"select_col\", label = \"Choose a column\", choices = names(iris)[-length(names(iris))] # exlcude last column ), selectInput( inputId = \"select_fill\", label = \"Choose a fill color\", choices = c(\"white\", \"red\", \"blue\", \"orange\", \"yellow\") ), textInput( inputId = \"text\", label = \"Plot title\", value = \"Box plot of iris\" ) ), mainPanel( h3(\"Box plot of iris\"), plotOutput(\"myplot\") ) ) ) server \u003c- function(input, output, session) { output$myplot \u003c- renderPlot({ ggplot(iris) + geom_boxplot(aes_string(y = input$select_col), fill = input$select_fill) + ggtitle(paste0(input$text, \"-\", input$select_col)) }) } shinyApp(ui, server)  More examples There are many other basic examples that are provided by this package and have a taste what we can do with Shiny apps.\nTo list app examples\nrunExample()  ## Valid examples are \"01_hello\", \"02_text\", \"03_reactivity\", \"04_mpg\", \"05_sliders\", \"06_tabsets\", \"07_widgets\", \"08_html\", \"09_upload\", \"10_download\", \"11_timer\"  Choose one of the examples you would like, e.g.\nrunExample(\"01_hello\")  You can see it breaks into two section, left-side is the actual app, on the right-side is the code to build this app. When we build Shiny apps, The right-side section is usually not displayed. It is called the “showcase” mode which is good for teaching. When you interact with app, this mode will also highlight what part of the code has been re-evaluated by your interaction in order to update the UI, a powerful utility to learn how shiny backend work.\nHowever, to have a feel what a normal app would look like, use display.mode=\"normal\"\nrunExample(\"01_hello\", display.mode=\"normal\")  The following Shiny app is hosted on shinyapps.io and embedded into the markdown (or html) source of this page using the following iframe syntax:\n\u003ciframe src=\"https://tgirke.shinyapps.io/diamonds/\" style=\"border: none; width: 880px; height: 900px\"\u003e\u003c/iframe\u003e   Deploy This can be done on local or cloud systems. An easy solution is to get an account on shinyapps.io and then deploy Shiny apps there. Read the tutorial on Shiny website.\nCreate a Shiny app for your final project After we have familiarized with Shiny basics, we can try to create a Shiny app to to course project. Here is a template app file, source code stored in this Github repository.\nThis template helps readers to visualize your downstream results of workflows.\n A barplot of deferentially expressed genes (DEG) across different comparison groups from the RNAseq workflow. A barplot to show different gene ontology (GO) enriched results. This plot can be applied to both RNAseq and ChIPseq projects. Add you own additional plots.  The embedded app is below or to have a better visual, open it in a new tab with this link: https://tgirke.shinyapps.io/gen242_shiny_template/\n To use this template on your own dataset:\n  click the “Use this template” on the Github page to create your own repository.\n  Clone this newly generated repository to your local computer.\n  Install all packages that are required by the template:\ninstall.packages(c(\"shiny\", \"DT\", \"ggplot2\", \"plotly\", \"dplyr\"))    Do not modify anything of the template at this point. Directly open the ui.R file and click the Run App button to test run the app to see if you can run it with no problem.\n  Read deploy to set up your account on shinyapps.io and use the credentials to set up the account in your local Rstudio.\n  Click the deploy button to deploy to your account. The prompt may ask you to fill the credential for the first time.   Check all files and give your a title and publish it.\n  Once you have a successful template instance, you can start to modify to your own app.\n  Open up the ui.R file, change/add your own project information, title, name, description and other text.\n  Replace data sets in /data folder,\n degs.csv is for DEG plot, read “Plot caption” section on the app about the requirements of this file. go.csv is for GO enrichment plot, read “Plot caption” section on the app about the requirements of this file. If you are doing a ChIPseq project and do not have a DEG table. Delete the DEG section in UI and server files.    Test to run with your new datasets locally and re-deploy your instance.\n  Add your own components, plots and others to the app if you like.\n  More resources to learn Shiny Tutorial and books  Long video tutorials. Shiny official Lessons. Shiny official gallery and source code Advanced Shiny book - Mastering Shiny Advanced web application in R book - Javascript for R  Extension packages  Catalog of cool extension packages - Awesome Shiny shinyWidgets - UI components systemPipeShiny - A framework for workflow management and data visualization.  spsComps - UI components, animations, server components   shinyjs - server end JavaScript communications  Session Info sessionInfo()  ## R version 4.0.5 (2021-03-31) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] fgsea_1.16.0 ggplot2_3.3.2 BiocStyle_2.18.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.5 bslib_0.2.4 compiler_4.0.5 pillar_1.4.7 ## [5] BiocManager_1.30.10 jquerylib_0.1.3 tools_4.0.5 digest_0.6.27 ## [9] lattice_0.20-41 jsonlite_1.7.1 evaluate_0.14 lifecycle_0.2.0 ## [13] tibble_3.0.4 gtable_0.3.0 pkgconfig_2.0.3 rlang_0.4.8 ## [17] Matrix_1.3-2 fastmatch_1.1-0 parallel_4.0.5 yaml_2.2.1 ## [21] blogdown_1.2 xfun_0.22 gridExtra_2.3 withr_2.3.0 ## [25] stringr_1.4.0 dplyr_1.0.2 knitr_1.30 generics_0.1.0 ## [29] sass_0.3.1 vctrs_0.3.5 grid_4.0.5 tidyselect_1.1.0 ## [33] data.table_1.13.2 glue_1.4.2 R6_2.5.0 BiocParallel_1.24.1 ## [37] rmarkdown_2.7 bookdown_0.21 purrr_0.3.4 magrittr_2.0.1 ## [41] codetools_0.2-18 scales_1.1.1 htmltools_0.5.1.1 ellipsis_0.3.1 ## [45] colorspace_2.0-0 stringi_1.5.3 munsell_0.5.0 crayon_1.3.4  References “Shiny - Tutorial.” n.d. https://shiny.rstudio.com/tutorial/. https://shiny.rstudio.com/tutorial/.\n  ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/shinyapps/shinyapps/","tags":"","title":"Shiny Apps"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: [ Slides ] [ .Rmd ] [ .R ]\n Overview Motivation for building R packages  Organization  Consolidate functions with related utilties in single place Interdepencies among less complex functions make coding more efficient Minimizes duplications   Documentation  Help page infrastructure improves documentation of functions Big picture of utilties provided by package vignettes (manuals)   Sharability  Package can be easier shared with colleagues and public Increases code accessibilty for other users   Extendibility  Makes software more extentible and maintainable    Package development environments This page introduces two approaches for building R packages:\n R Base and related functionalities devtools and related packages (e.g. usethis, roxygen2 and sinew)  The sample code provided below creates for each of the two methods a simple test package that can be installed and loaded on a user’s system. The instructions for the second appoach are more detailed since it is likely to provide the most practical solution for newer users of R.\n1. R Base Approach R packages can be built with the package.skeleton function. The most comprehensive documentation on package development is provided by the Writing R Extensions page on CRAN. The basic workflow example below will create a directory named mypackage containing the skeleton of the package for all functions, methods and classes defined in the R script(s) passed on to the code_files argument. The basic structure of the package directory is described here. The package directory will also contain a file named Read-and-delete-me with instructions for completing the package:\n1.1 Create package skeleton ## Download R script (here pkg_build_fct.R) containing two sample functions download.file(\"https://raw.githubusercontent.com/tgirke/GEN242/main/content/en/tutorials/rpackages/helper_functions/pkg_build_fct.R\", \"pkg_build_fct.R\") ## Build package skeleton based on functions in pkg_build_fct.R package.skeleton(name=\"mypackage\", code_files=c(\"pkg_build_fct.R\"))  The given example will create a directory named mypackage containing the skeleton of the package for all functions, methods and classes defined in the R script(s) passed on to the code_files argument. The basic structure of the package directory is described here. The package directory will also contain a file named ‘Read-and-delete-me’ with the following instructions for completing the package:\n Edit the help file skeletons in man, possibly combining help files for multiple functions. Edit the exports in NAMESPACE, and add necessary imports. Put any C/C++/Fortran code in src. If you have compiled code, add a useDynLib() directive to NAMESPACE. Run R CMD build to build the package tarball. Run R CMD check to check the package tarball. Read Writing R Extensions for more information.  1.2 Build package Once a package skeleton is available one can build the package from the command-line (Linux/OS X), or from within R by executing the command-line calls with R’s system(\"...\") command. For instance, the command-line call R CMD build ... can be run from within R with system(\"R CMD build ...\"). The following examples refer to the R console.\nsystem(\"R CMD build mypackage\")  1.3 Check package This will create a tarball of the package with its version number encoded in the file name (here mypackage_1.0.tar.gz). Subsequently, the package tarball needs to be checked for errors with R CMD check.\nsystem(\"R CMD check mypackage_1.0.tar.gz\")  All issues in a package’s source code and documentation should be addressed until R CMD check returns no error or warning messages anymore.\n1.4 Install package Install package from source on Linux or OS X systems.\ninstall.packages(\"mypackage_1.0.tar.gz\", repos=NULL) # On OS X include the argument type=\"source\"  Windows requires a zip archive for installing R packages, which can be most conveniently created from the command-line (Linux/OS X) by installing the package in a local directory (here tempdir) and then creating a zip archive from the installed package directory:\n$ mkdir tempdir $ R CMD INSTALL -l tempdir mypackage_1.0.tar.gz $ cd tempdir $ zip -r mypackage mypackage ## The resulting mypackage.zip archive can be installed from R under Windows like this: install.packages(\"mypackage.zip\", repos=NULL)  This procedure only works for packages which do not rely on compiled code (C/C++). Instructions to fully build an R package under Windows can be found here and here.\n1.5 Maintain package Several useful helper utilities exist for maintaing and extending packages. Typical package development routines include:\n Adding new functions, methods and classes to the script files in the ./R directory in your package Adding their names to the NAMESPACE file of the package Additional .Rd help templates can be generated with the prompt() function family like this:  source(\"myscript.R\") # imports functions, methods and classes from myscript.R prompt(myfct) # writes help file myfct.Rd promptClass(\"myclass\") # writes file myclass-class.Rd promptMethods(\"mymeth\") # writes help file mymeth.Rd  The resulting .Rd help files can be edited in a text editor, and properly rendered and viewed from within R with help of the following functions.\nlibrary(tools) Rd2txt(\"./mypackage/man/myfct.Rd\") # renders *.Rd files as they look in final help pages checkRd(\"./mypackage/man/myfct.Rd\") # checks *.Rd help file for problems  1.6 Submit package to public repository The best way of sharing an R package with the community is to submit it to one of the main R package repositories, such as CRAN or Bioconductor. The details about the submission process are given on the corresponding repository submission pages:\n Submitting to Bioconductor Submitting to CRAN  2. R devtools Approach Several package develpment routines of the traditional method outlined above are manual, such as updating the NAMESPACE file and documenting functions in separate help (*.Rd) files. This process can be simplified and partially automated by taking advantage of a more recent R package development environment composed of several helper packages including devtools, usethis, roxygen2 and sinew (Wickham and Bryan, n.d.). Many books and web sites document this process in more detail. Here is a small selection of useful online documentation about R package development:\n Book: R Packages by Hadley Wickham and Jenny Bryan My First R Package by Fong Chun Chan How to Creat an R Package, Easy Mode by Amit Kohli Package Development Cheat Sheet Automating roxygen2 documentation with sinew by Jonathan Sidi: Blog and CRAN  Workflow for building R packages The following outlines the basic workflow for building, testing and extending R packages with the package development environment functionalities outlined above.\n2.1 Create package skeleton library(\"devtools\"); library(\"roxygen2\"); library(\"usethis\"); library(sinew) # If not availble install these packages with 'install.packages(...)' create(\"myfirstpkg\") # Creates package skeleton. The chosen name (here myfirstpkg) will be the name of the package. setwd(\"myfirstpkg\") # Set working directory of R session to package directory 'myfirstpkg' use_mit_license() # Add license information to description file (here MIT). To look up alternatives, do ?use_mit_license  2.2 Add R functions Next, R functions can be added to *.R file(s) under the R directory of the new package. Several functions can be organized in one *.R file, each in its own file or any combination. For demonstration purposes, the following will download an R file (pkg_build_fct.R from here) defining two functions (named:myMAcomp and talkToMe) and save it to the R directory of the package.\ndownload.file(\"https://raw.githubusercontent.com/tgirke/GEN242/main/content/en/tutorials/rpackages/helper_functions/pkg_build_fct.R\", \"R/pkg_build_fct.R\")  2.3 Auto-generate roxygen comment lines The makeOxygen function from the sinew package creates roxygen2 comment skeletons based on the information from each function (below for myMAcomp example). The roxygen comment lines need to be added above the code of each function. This can be done by copy and paste from the R console or by writing the output to a temporary file (below via writeLines). Alternatively, the makeOxyFile function can be used to create a roxygenized copy of an R source file, where the roxygen comment lines have been added above all functions automatically. Next, the default text in the comment lines needs to be replaced by meaningful text describing the utility and usage of each function. This editing process of documentation can be completed and/or revised any time.\nload_all() # Loads package in a simulated way without installing it. writeLines(makeOxygen(myMAcomp), \"myroxylines\") # This creates a 'myroxylines' file in current directory. Delete this file after adding its content to the corresponding functions.  2.4 Autogenerate help files The document function autogenerates for each function one *.Rd file in the man directory of the package. The content in the *.Rd help files is based on the information in the roxygen comment lines generated in the previous step. In addition, all relevant export/import instructions are added to the NAMESPACE file. Importantly, when using roxygen-based documentation in a package then the NAMESPACE and *.Rd files should not be manually edited since this information will be lost during the automation routines provided by roxygen2.\ndocument() # Auto-generates/updates *.Rd files under man directory (here: myMAcomp.Rd and talkToMe.Rd) tools::Rd2txt(\"man/myMAcomp.Rd\") # Renders Rd file from source tools::checkRd(\"man/myMAcomp.Rd\") # Checks Rd file for problems  2.5 Add a vignette A vignette template can be auto-generated with the use_vignette function from the usethis package. The *.Rmd source file of the vignette will be located under a new vignette directory. Additional vignettes can be manually added to this directory as needed.\nuse_vignette(\"introduction\", title=\"Introduction to this package\")  2.6 Check, install and build package Now the package can be checked for problems. All warnings and errors should be addressed prior to submission to a public repository. After this it can be installed on a user’s system with the install command. In addition, the build function allows to assemble the package in a *.tar.gz file. The latter is often important for sharing packages and/or submitting them to public repositories.\nsetwd(\"..\") # Redirect R session to parent directory check(\"myfirstpkg\") # Check package for problems, when in pkg dir one can just use check() # remove.packages(\"myfirstpkg\") # Optional. Removes test package if already installed install(\"myfirstpkg\", build_vignettes=TRUE) # Installs package build(\"myfirstpkg\") # Creates *.tar.gz file for package required to for submission to CRAN/Bioc  2.7 Using the new package After installing and loading the package its functions, help files and vignettes can be accessed as follows.\nlibrary(\"myfirstpkg\") library(help=\"myfirstpkg\") ?myMAcomp vignette(\"introduction\", \"myfirstpkg\")  Another very useful development function is test for evaluating the test code of a package.\n2.8 Share package on GitHub To host and share the new package myfirstpkg on GitHub, one can use the following steps:\n Create an empty target GitHub repos online (e.g. named mypkg_repos) as outlined here. Clone the new GitHub repos to local system with git clone https://github.com/\u003cgithub_username\u003e/\u003crepo name\u003e (here from command-line) Copy the root directory of the package into the downloaded repos with cp -r myfirstpkg mypkg_repos Next cd into mypkg_repos, and then add all files and directories of the package to the staging area with git add -A :/. Commit and push the changes to GitHub with: git commit -am \"first commit\"; git push. After this the package should be life on the corresponding GitHub web page. Assuming the package is public, it can be installed directly from GitHub by anyone as shown below (from within R). Installs of private packages require a personal access token (PAT) that needs to be assigned to the auth_token argument. PATs can be created here.  devtools::install_github(\"\u003cgithub_user_name\u003e/\u003cmypkg_repos\u003e\", subdir=\"myfirstpkg\") # If the package is in the root directory of the repos, then the 'subdir' argument can be dropped.  Session Info sessionInfo()  ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 11 (bullseye) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggplot2_3.3.6 limma_3.52.0 BiocStyle_2.24.0 ## ## loaded via a namespace (and not attached): ## [1] bslib_0.3.1 compiler_4.2.0 pillar_1.7.0 BiocManager_1.30.17 ## [5] jquerylib_0.1.4 tools_4.2.0 digest_0.6.29 jsonlite_1.8.0 ## [9] evaluate_0.15 lifecycle_1.0.1 tibble_3.1.7 gtable_0.3.0 ## [13] pkgconfig_2.0.3 rlang_1.0.2 DBI_1.1.2 cli_3.3.0 ## [17] yaml_2.3.5 blogdown_1.9 xfun_0.30 fastmap_1.1.0 ## [21] withr_2.5.0 dplyr_1.0.9 stringr_1.4.0 knitr_1.39 ## [25] generics_0.1.2 sass_0.4.1 vctrs_0.4.1 tidyselect_1.1.2 ## [29] grid_4.2.0 glue_1.6.2 R6_2.5.1 fansi_1.0.3 ## [33] rmarkdown_2.14 bookdown_0.26 purrr_0.3.4 magrittr_2.0.3 ## [37] codetools_0.2-18 scales_1.2.0 htmltools_0.5.2 ellipsis_0.3.2 ## [41] assertthat_0.2.1 colorspace_2.0-3 utf8_1.2.2 stringi_1.7.6 ## [45] munsell_0.5.0 crayon_1.5.1  References Wickham, Hadley, and Jennifer Bryan. n.d. “R Packages.” https://r-pkgs.org/index.html. https://r-pkgs.org/index.html.\n  ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/rpackages/rpackages/","tags":"","title":"Building R Packages"},{"body":"document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Source code downloads: [ Slides ] [ .Rmd ] [ .R ]\n Overview Modern object classes and methods for handling data.frame like structures are provided by the dplyr (tidyr) and data.table packages. A related example is Bioconductor’s DataTable object class (“Learn the tidyverse,” n.d.). This tutorial provide a short introduction to the usage and functionalities of the dplyr and related packages.\nRelated documentation More detailed tutorials on this topic can be found here:\n dplyr: A Grammar of Data Manipulation Introduction to dplyr Tutorial on dplyr Cheatsheet for Joins from Jenny Bryan Tibbles Intro to data.table package Big data with dplyr and data.table Fast lookups with dplyr and data.table  Installation The dplyr (tidyr) environment has evolved into an ecosystem of packages. To simplify package management, one can install and load the entire collection via the tidyverse package. For more details on tidyverse see here.\ninstall.packages(\"tidyverse\")  Construct a tibble (tibble) library(tidyverse) as_tibble(iris) # coerce data.frame to tibble tbl  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cfct\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows  Reading and writing tabular files While the base R read/write utilities can be used for data.frames, best time performance with the least amount of typing is achieved with the export/import functions from the readr package. For very large files the fread function from the data.table package achieves the best time performance.\nImport with readr Import functions provided by readr include:\n read_csv(): comma separated (CSV) files read_tsv(): tab separated files read_delim(): general delimited files read_fwf(): fixed width files read_table(): tabular files where colums are separated by white-space. read_log(): web log files  Create a sample tab delimited file for import\nwrite_tsv(iris, \"iris.txt\") # Creates sample file  Import with read_tsv\niris_df \u003c- read_tsv(\"iris.txt\") # Import with read_tbv from readr package iris_df  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows  To import Google Sheets directly into R, see here.\nFast table import with fread The fread function from the data.table package provides the best time performance for reading large tabular files into R.\nlibrary(data.table) iris_df \u003c- as_tibble(fread(\"iris.txt\")) # Import with fread and conversion to tibble iris_df  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows  Note: to ignore lines starting with comment signs, one can pass on to fread a shell command for preprocessing the file. The following example illustrates this option.\nfread(\"grep -v '^#' iris.txt\")  Export with readr Export function provided by readr inlcude\n write_delim(): general delimited files write_csv(): comma separated (CSV) files write_excel_csv(): excel style CSV files write_tsv(): tab separated files  For instance, the write_tsv function writes a data.frame or tibble to a tab delimited file with much nicer default settings than the base R write.table function.\nwrite_tsv(iris_df, \"iris.txt\")  Column and row binds The equivalents to base R’s rbind and cbind are bind_rows and bind_cols, respectively.\nbind_cols(iris_df, iris_df)  ## # A tibble: 150 × 10 ## Sepal.Length...1 Sepal.Width...2 Petal.Length...3 Petal.Width...4 Species...5 Sepal.Length...6 ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e \u003cdbl\u003e ## 1 5.1 3.5 1.4 0.2 setosa 5.1 ## 2 4.9 3 1.4 0.2 setosa 4.9 ## 3 4.7 3.2 1.3 0.2 setosa 4.7 ## 4 4.6 3.1 1.5 0.2 setosa 4.6 ## 5 5 3.6 1.4 0.2 setosa 5 ## 6 5.4 3.9 1.7 0.4 setosa 5.4 ## 7 4.6 3.4 1.4 0.3 setosa 4.6 ## 8 5 3.4 1.5 0.2 setosa 5 ## 9 4.4 2.9 1.4 0.2 setosa 4.4 ## 10 4.9 3.1 1.5 0.1 setosa 4.9 ## # … with 140 more rows, and 4 more variables: Sepal.Width...7 \u003cdbl\u003e, Petal.Length...8 \u003cdbl\u003e, ## # Petal.Width...9 \u003cdbl\u003e, Species...10 \u003cchr\u003e  bind_rows(iris_df, iris_df)  ## # A tibble: 300 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 290 more rows  Extract column as vector The subsetting operators [[ and $can be used to extract from a tibble single columns as vector.\niris_df[[5]][1:12]  ## [1] \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" ## [11] \"setosa\" \"setosa\"  iris_df$Species[1:12]  ## [1] \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" ## [11] \"setosa\" \"setosa\"  Important dplyr functions  filter() and slice() arrange() select() and rename() distinct() mutate() and transmute() summarise() sample_n() and sample_frac()  Slice and filter functions Filter function filter(iris_df, Sepal.Length \u003e 7.5, Species==\"virginica\")  ## # A tibble: 6 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 7.6 3 6.6 2.1 virginica ## 2 7.7 3.8 6.7 2.2 virginica ## 3 7.7 2.6 6.9 2.3 virginica ## 4 7.7 2.8 6.7 2 virginica ## 5 7.9 3.8 6.4 2 virginica ## 6 7.7 3 6.1 2.3 virginica  Base R code equivalent iris_df[iris_df[, \"Sepal.Length\"] \u003e 7.5 \u0026 iris_df[, \"Species\"]==\"virginica\", ]  ## # A tibble: 6 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 7.6 3 6.6 2.1 virginica ## 2 7.7 3.8 6.7 2.2 virginica ## 3 7.7 2.6 6.9 2.3 virginica ## 4 7.7 2.8 6.7 2 virginica ## 5 7.9 3.8 6.4 2 virginica ## 6 7.7 3 6.1 2.3 virginica  Including boolean operators filter(iris_df, Sepal.Length \u003e 7.5 | Sepal.Length \u003c 5.5, Species==\"virginica\")  ## # A tibble: 7 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 7.6 3 6.6 2.1 virginica ## 2 4.9 2.5 4.5 1.7 virginica ## 3 7.7 3.8 6.7 2.2 virginica ## 4 7.7 2.6 6.9 2.3 virginica ## 5 7.7 2.8 6.7 2 virginica ## 6 7.9 3.8 6.4 2 virginica ## 7 7.7 3 6.1 2.3 virginica  Subset rows by position dplyr approach\nslice(iris_df, 1:2)  ## # A tibble: 2 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa  Base R code equivalent\niris_df[1:2,]  ## # A tibble: 2 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa  Subset rows by names Since tibbles do not contain row names, row wise subsetting via the [,] operator cannot be used. However, the corresponding behavior can be achieved by passing to select a row position index obtained by basic R intersect utilities such as match.\nCreate a suitable test tibble\ndf1 \u003c- bind_cols(tibble(ids1=paste0(\"g\", 1:10)), as_tibble(matrix(1:40, 10, 4, dimnames=list(1:10, paste0(\"CA\", 1:4))))) df1  ## # A tibble: 10 × 5 ## ids1 CA1 CA2 CA3 CA4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g1 1 11 21 31 ## 2 g2 2 12 22 32 ## 3 g3 3 13 23 33 ## 4 g4 4 14 24 34 ## 5 g5 5 15 25 35 ## 6 g6 6 16 26 36 ## 7 g7 7 17 27 37 ## 8 g8 8 18 28 38 ## 9 g9 9 19 29 39 ## 10 g10 10 20 30 40  dplyr approach\nslice(df1, match(c(\"g10\", \"g4\", \"g4\"), ids1))  ## # A tibble: 3 × 5 ## ids1 CA1 CA2 CA3 CA4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g10 10 20 30 40 ## 2 g4 4 14 24 34 ## 3 g4 4 14 24 34  Base R equivalent\ndf1_old \u003c- as.data.frame(df1) rownames(df1_old) \u003c- df1_old[,1] df1_old[c(\"g10\", \"g4\", \"g4\"),]  ## ids1 CA1 CA2 CA3 CA4 ## g10 g10 10 20 30 40 ## g4 g4 4 14 24 34 ## g4.1 g4 4 14 24 34  Sorting with arrange Row-wise ordering based on specific columns\ndplyr approach\narrange(iris_df, Species, Sepal.Length, Sepal.Width)  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 4.3 3 1.1 0.1 setosa ## 2 4.4 2.9 1.4 0.2 setosa ## 3 4.4 3 1.3 0.2 setosa ## 4 4.4 3.2 1.3 0.2 setosa ## 5 4.5 2.3 1.3 0.3 setosa ## 6 4.6 3.1 1.5 0.2 setosa ## 7 4.6 3.2 1.4 0.2 setosa ## 8 4.6 3.4 1.4 0.3 setosa ## 9 4.6 3.6 1 0.2 setosa ## 10 4.7 3.2 1.3 0.2 setosa ## # … with 140 more rows  For ordering descendingly use desc() function\narrange(iris_df, desc(Species), Sepal.Length, Sepal.Width)  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 4.9 2.5 4.5 1.7 virginica ## 2 5.6 2.8 4.9 2 virginica ## 3 5.7 2.5 5 2 virginica ## 4 5.8 2.7 5.1 1.9 virginica ## 5 5.8 2.7 5.1 1.9 virginica ## 6 5.8 2.8 5.1 2.4 virginica ## 7 5.9 3 5.1 1.8 virginica ## 8 6 2.2 5 1.5 virginica ## 9 6 3 4.8 1.8 virginica ## 10 6.1 2.6 5.6 1.4 virginica ## # … with 140 more rows  Base R code equivalent\niris_df[order(iris_df$Species, iris_df$Sepal.Length, iris_df$Sepal.Width), ]  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 4.3 3 1.1 0.1 setosa ## 2 4.4 2.9 1.4 0.2 setosa ## 3 4.4 3 1.3 0.2 setosa ## 4 4.4 3.2 1.3 0.2 setosa ## 5 4.5 2.3 1.3 0.3 setosa ## 6 4.6 3.1 1.5 0.2 setosa ## 7 4.6 3.2 1.4 0.2 setosa ## 8 4.6 3.4 1.4 0.3 setosa ## 9 4.6 3.6 1 0.2 setosa ## 10 4.7 3.2 1.3 0.2 setosa ## # … with 140 more rows  iris_df[order(iris_df$Species, decreasing=TRUE), ]  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 6.3 3.3 6 2.5 virginica ## 2 5.8 2.7 5.1 1.9 virginica ## 3 7.1 3 5.9 2.1 virginica ## 4 6.3 2.9 5.6 1.8 virginica ## 5 6.5 3 5.8 2.2 virginica ## 6 7.6 3 6.6 2.1 virginica ## 7 4.9 2.5 4.5 1.7 virginica ## 8 7.3 2.9 6.3 1.8 virginica ## 9 6.7 2.5 5.8 1.8 virginica ## 10 7.2 3.6 6.1 2.5 virginica ## # … with 140 more rows  Select columns with select Select specific columns\nselect(iris_df, Species, Petal.Length, Sepal.Length)  ## # A tibble: 150 × 3 ## Species Petal.Length Sepal.Length ## \u003cchr\u003e \u003cdbl\u003e \u003cdbl\u003e ## 1 setosa 1.4 5.1 ## 2 setosa 1.4 4.9 ## 3 setosa 1.3 4.7 ## 4 setosa 1.5 4.6 ## 5 setosa 1.4 5 ## 6 setosa 1.7 5.4 ## 7 setosa 1.4 4.6 ## 8 setosa 1.5 5 ## 9 setosa 1.4 4.4 ## 10 setosa 1.5 4.9 ## # … with 140 more rows  Select range of columns by name\nselect(iris_df, Sepal.Length : Petal.Width)  ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # … with 140 more rows  Drop specific columns (here range)\nselect(iris_df, -(Sepal.Length : Petal.Width))  ## # A tibble: 150 × 1 ## Species ## \u003cchr\u003e ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa ## 7 setosa ## 8 setosa ## 9 setosa ## 10 setosa ## # … with 140 more rows  Renaming columns with rename dplyr approach\nrename(iris_df, new_col_name = Species)  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width new_col_name ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows  Base R code approach\ncolnames(iris_df)[colnames(iris_df)==\"Species\"] \u003c- \"new_col_names\"  Obtain unique rows with distinct dplyr approach\ndistinct(iris_df, Species, .keep_all=TRUE)  ## # A tibble: 3 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 7 3.2 4.7 1.4 versicolor ## 3 6.3 3.3 6 2.5 virginica  Base R code approach\niris_df[!duplicated(iris_df$Species),]  ## # A tibble: 3 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e ## 1 5.1 3.5 1.4 0.2 setosa ## 2 7 3.2 4.7 1.4 versicolor ## 3 6.3 3.3 6 2.5 virginica  Add columns mutate The mutate function allows to append columns to existing ones.\nmutate(iris_df, Ratio = Sepal.Length / Sepal.Width, Sum = Sepal.Length + Sepal.Width)  ## # A tibble: 150 × 7 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Ratio Sum ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e \u003cdbl\u003e \u003cdbl\u003e ## 1 5.1 3.5 1.4 0.2 setosa 1.46 8.6 ## 2 4.9 3 1.4 0.2 setosa 1.63 7.9 ## 3 4.7 3.2 1.3 0.2 setosa 1.47 7.9 ## 4 4.6 3.1 1.5 0.2 setosa 1.48 7.7 ## 5 5 3.6 1.4 0.2 setosa 1.39 8.6 ## 6 5.4 3.9 1.7 0.4 setosa 1.38 9.3 ## 7 4.6 3.4 1.4 0.3 setosa 1.35 8 ## 8 5 3.4 1.5 0.2 setosa 1.47 8.4 ## 9 4.4 2.9 1.4 0.2 setosa 1.52 7.3 ## 10 4.9 3.1 1.5 0.1 setosa 1.58 8 ## # … with 140 more rows  transmute The transmute function does the same as mutate but drops existing columns\ntransmute(iris_df, Ratio = Sepal.Length / Sepal.Width, Sum = Sepal.Length + Sepal.Width)  ## # A tibble: 150 × 2 ## Ratio Sum ## \u003cdbl\u003e \u003cdbl\u003e ## 1 1.46 8.6 ## 2 1.63 7.9 ## 3 1.47 7.9 ## 4 1.48 7.7 ## 5 1.39 8.6 ## 6 1.38 9.3 ## 7 1.35 8 ## 8 1.47 8.4 ## 9 1.52 7.3 ## 10 1.58 8 ## # … with 140 more rows  bind_cols The bind_cols function is the equivalent of cbind in base R. To add rows, use the corresponding bind_rows function.\nbind_cols(iris_df, iris_df)  ## # A tibble: 150 × 10 ## Sepal.Length...1 Sepal.Width...2 Petal.Length...3 Petal.Width...4 Species...5 Sepal.Length...6 ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e \u003cdbl\u003e ## 1 5.1 3.5 1.4 0.2 setosa 5.1 ## 2 4.9 3 1.4 0.2 setosa 4.9 ## 3 4.7 3.2 1.3 0.2 setosa 4.7 ## 4 4.6 3.1 1.5 0.2 setosa 4.6 ## 5 5 3.6 1.4 0.2 setosa 5 ## 6 5.4 3.9 1.7 0.4 setosa 5.4 ## 7 4.6 3.4 1.4 0.3 setosa 4.6 ## 8 5 3.4 1.5 0.2 setosa 5 ## 9 4.4 2.9 1.4 0.2 setosa 4.4 ## 10 4.9 3.1 1.5 0.1 setosa 4.9 ## # … with 140 more rows, and 4 more variables: Sepal.Width...7 \u003cdbl\u003e, Petal.Length...8 \u003cdbl\u003e, ## # Petal.Width...9 \u003cdbl\u003e, Species...10 \u003cchr\u003e  Summarize data Summary calculation on single column\nsummarize(iris_df, mean(Petal.Length))  ## # A tibble: 1 × 1 ## `mean(Petal.Length)` ## \u003cdbl\u003e ## 1 3.76  Summary calculation on many columns\nsummarize_all(iris_df[,1:4], mean)  ## # A tibble: 1 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e ## 1 5.84 3.06 3.76 1.20  Summarize by grouping column\nsummarize(group_by(iris_df, Species), mean(Petal.Length))  ## # A tibble: 3 × 2 ## Species `mean(Petal.Length)` ## \u003cchr\u003e \u003cdbl\u003e ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55  Aggregate summaries\nsummarize_all(group_by(iris_df, Species), mean)  ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## \u003cchr\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03  Note: group_by does the looping for the user similar to aggregate or tapply.\nMerging tibbles The dplyr package provides several join functions for merging tibbles by a common key column similar to the merge function in base R. These *_join functions include:\n inner_join(): returns join only for rows matching among both tibbles full_join(): returns join for all (matching and non-matching) rows of two tibbles left_join(): returns join for all rows in first tibble right_join(): returns join for all rows in second tibble anti_join(): returns for first tibble only those rows that have no match in the second one  Sample tibbles to illustrate *.join functions.\ndf1 \u003c- bind_cols(tibble(ids1=paste0(\"g\", 1:10)), as_tibble(matrix(1:40, 10, 4, dimnames=list(1:10, paste0(\"CA\", 1:4))))) df1  ## # A tibble: 10 × 5 ## ids1 CA1 CA2 CA3 CA4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g1 1 11 21 31 ## 2 g2 2 12 22 32 ## 3 g3 3 13 23 33 ## 4 g4 4 14 24 34 ## 5 g5 5 15 25 35 ## 6 g6 6 16 26 36 ## 7 g7 7 17 27 37 ## 8 g8 8 18 28 38 ## 9 g9 9 19 29 39 ## 10 g10 10 20 30 40  df2 \u003c- bind_cols(tibble(ids2=paste0(\"g\", c(2,5,11,12))), as_tibble(matrix(1:16, 4, 4, dimnames=list(1:4, paste0(\"CB\", 1:4))))) df2  ## # A tibble: 4 × 5 ## ids2 CB1 CB2 CB3 CB4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g2 1 5 9 13 ## 2 g5 2 6 10 14 ## 3 g11 3 7 11 15 ## 4 g12 4 8 12 16  Inner join inner_join(df1, df2, by=c(\"ids1\"=\"ids2\"))  ## # A tibble: 2 × 9 ## ids1 CA1 CA2 CA3 CA4 CB1 CB2 CB3 CB4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g2 2 12 22 32 1 5 9 13 ## 2 g5 5 15 25 35 2 6 10 14  Left join left_join(df1, df2, by=c(\"ids1\"=\"ids2\"))  ## # A tibble: 10 × 9 ## ids1 CA1 CA2 CA3 CA4 CB1 CB2 CB3 CB4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g1 1 11 21 31 NA NA NA NA ## 2 g2 2 12 22 32 1 5 9 13 ## 3 g3 3 13 23 33 NA NA NA NA ## 4 g4 4 14 24 34 NA NA NA NA ## 5 g5 5 15 25 35 2 6 10 14 ## 6 g6 6 16 26 36 NA NA NA NA ## 7 g7 7 17 27 37 NA NA NA NA ## 8 g8 8 18 28 38 NA NA NA NA ## 9 g9 9 19 29 39 NA NA NA NA ## 10 g10 10 20 30 40 NA NA NA NA  Right join right_join(df1, df2, by=c(\"ids1\"=\"ids2\"))  ## # A tibble: 4 × 9 ## ids1 CA1 CA2 CA3 CA4 CB1 CB2 CB3 CB4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g2 2 12 22 32 1 5 9 13 ## 2 g5 5 15 25 35 2 6 10 14 ## 3 g11 NA NA NA NA 3 7 11 15 ## 4 g12 NA NA NA NA 4 8 12 16  Full join full_join(df1, df2, by=c(\"ids1\"=\"ids2\"))  ## # A tibble: 12 × 9 ## ids1 CA1 CA2 CA3 CA4 CB1 CB2 CB3 CB4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g1 1 11 21 31 NA NA NA NA ## 2 g2 2 12 22 32 1 5 9 13 ## 3 g3 3 13 23 33 NA NA NA NA ## 4 g4 4 14 24 34 NA NA NA NA ## 5 g5 5 15 25 35 2 6 10 14 ## 6 g6 6 16 26 36 NA NA NA NA ## 7 g7 7 17 27 37 NA NA NA NA ## 8 g8 8 18 28 38 NA NA NA NA ## 9 g9 9 19 29 39 NA NA NA NA ## 10 g10 10 20 30 40 NA NA NA NA ## 11 g11 NA NA NA NA 3 7 11 15 ## 12 g12 NA NA NA NA 4 8 12 16  Anti join anti_join(df1, df2, by=c(\"ids1\"=\"ids2\"))  ## # A tibble: 8 × 5 ## ids1 CA1 CA2 CA3 CA4 ## \u003cchr\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e \u003cint\u003e ## 1 g1 1 11 21 31 ## 2 g3 3 13 23 33 ## 3 g4 4 14 24 34 ## 4 g6 6 16 26 36 ## 5 g7 7 17 27 37 ## 6 g8 8 18 28 38 ## 7 g9 9 19 29 39 ## 8 g10 10 20 30 40  For additional join options users want to cosult the *_join help pages.\nChaining To simplify chaining of serveral operations, dplyr provides the %\u003e% operator, where x %\u003e% f(y) turns into f(x, y). This way one can pipe together multiple operations by writing them from left-to-right or top-to-bottom. This makes for easy to type and readable code.\nExample 1 Series of data manipulations and export\nread_tsv(\"iris.txt\") %\u003e% # Import with read_tbv from readr package as_tibble() %\u003e% # Declare to use tibble select(Sepal.Length:Species) %\u003e% # Select columns filter(Species==\"setosa\") %\u003e% # Filter rows by some value arrange(Sepal.Length) %\u003e% # Sort by some column mutate(Subtract=Petal.Length - Petal.Width) # Calculate and append  ## # A tibble: 50 × 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Subtract ## \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cchr\u003e \u003cdbl\u003e ## 1 4.3 3 1.1 0.1 setosa 1 ## 2 4.4 2.9 1.4 0.2 setosa 1.2 ## 3 4.4 3 1.3 0.2 setosa 1.1 ## 4 4.4 3.2 1.3 0.2 setosa 1.1 ## 5 4.5 2.3 1.3 0.3 setosa 1 ## 6 4.6 3.1 1.5 0.2 setosa 1.3 ## 7 4.6 3.4 1.4 0.3 setosa 1.1 ## 8 4.6 3.6 1 0.2 setosa 0.8 ## 9 4.6 3.2 1.4 0.2 setosa 1.2 ## 10 4.7 3.2 1.3 0.2 setosa 1.1 ## # … with 40 more rows  # write_tsv(\"iris.txt\") # Export to file, omitted here to show result  Example 2 Series of summary calculations for grouped data (group_by)\niris_df %\u003e% # Declare tibble to use group_by(Species) %\u003e% # Group by species summarize(Mean_Sepal.Length=mean(Sepal.Length), Max_Sepal.Length=max(Sepal.Length), Min_Sepal.Length=min(Sepal.Length), SD_Sepal.Length=sd(Sepal.Length), Total=n())  ## # A tibble: 3 × 6 ## Species Mean_Sepal.Length Max_Sepal.Length Min_Sepal.Length SD_Sepal.Length Total ## \u003cchr\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cint\u003e ## 1 setosa 5.01 5.8 4.3 0.352 50 ## 2 versicolor 5.94 7 4.9 0.516 50 ## 3 virginica 6.59 7.9 4.9 0.636 50  Example 3 Combining dplyr chaining with ggplot\niris_df %\u003e% group_by(Species) %\u003e% summarize_all(mean) %\u003e% reshape2::melt(id.vars=c(\"Species\"), variable.name = \"Samples\", value.name=\"Values\") %\u003e% ggplot(aes(Samples, Values, fill = Species)) + geom_bar(position=\"dodge\", stat=\"identity\")  SQLite Databases SQLite is a lightweight relational database solution. The RSQLite package provides an easy to use interface to create, manage and query SQLite databases directly from R. Basic instructions for using SQLite from the command-line are available here. A short introduction to RSQLite is available here.\nLoading data into SQLite databases The following loads two data.frames derived from the iris data set (here mydf1 and mydf2) into an SQLite database (here test.db).\nlibrary(RSQLite) unlink(\"test.db\") # Delete any existing test.db mydb \u003c- dbConnect(SQLite(), \"test.db\") # Creates database file test.db mydf1 \u003c- data.frame(ids=paste0(\"id\", seq_along(iris[,1])), iris) mydf2 \u003c- mydf1[sample(seq_along(mydf1[,1]), 10),] dbWriteTable(mydb, \"mydf1\", mydf1) dbWriteTable(mydb, \"mydf2\", mydf2)  List names of tables in database dbListTables(mydb)  ## [1] \"mydf1\" \"mydf2\"  Import table into data.frame dbGetQuery(mydb, 'SELECT * FROM mydf2')  ## ids Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 id95 5.6 2.7 4.2 1.3 versicolor ## 2 id79 6.0 2.9 4.5 1.5 versicolor ## 3 id124 6.3 2.7 4.9 1.8 virginica ## 4 id33 5.2 4.1 1.5 0.1 setosa ## 5 id48 4.6 3.2 1.4 0.2 setosa ## 6 id122 5.6 2.8 4.9 2.0 virginica ## 7 id104 6.3 2.9 5.6 1.8 virginica ## 8 id56 5.7 2.8 4.5 1.3 versicolor ## 9 id105 6.5 3.0 5.8 2.2 virginica ## 10 id43 4.4 3.2 1.3 0.2 setosa  Query database dbGetQuery(mydb, 'SELECT * FROM mydf1 WHERE \"Sepal.Length\" \u003c 4.6')  ## ids Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 id9 4.4 2.9 1.4 0.2 setosa ## 2 id14 4.3 3.0 1.1 0.1 setosa ## 3 id39 4.4 3.0 1.3 0.2 setosa ## 4 id42 4.5 2.3 1.3 0.3 setosa ## 5 id43 4.4 3.2 1.3 0.2 setosa  Join tables The two tables can be joined on the shared ids column as follows.\ndbGetQuery(mydb, 'SELECT * FROM mydf1, mydf2 WHERE mydf1.ids = mydf2.ids')  ## ids Sepal.Length Sepal.Width Petal.Length Petal.Width Species ids Sepal.Length ## 1 id33 5.2 4.1 1.5 0.1 setosa id33 5.2 ## 2 id43 4.4 3.2 1.3 0.2 setosa id43 4.4 ## 3 id48 4.6 3.2 1.4 0.2 setosa id48 4.6 ## 4 id56 5.7 2.8 4.5 1.3 versicolor id56 5.7 ## 5 id79 6.0 2.9 4.5 1.5 versicolor id79 6.0 ## 6 id95 5.6 2.7 4.2 1.3 versicolor id95 5.6 ## 7 id104 6.3 2.9 5.6 1.8 virginica id104 6.3 ## 8 id105 6.5 3.0 5.8 2.2 virginica id105 6.5 ## 9 id122 5.6 2.8 4.9 2.0 virginica id122 5.6 ## 10 id124 6.3 2.7 4.9 1.8 virginica id124 6.3 ## Sepal.Width Petal.Length Petal.Width Species ## 1 4.1 1.5 0.1 setosa ## 2 3.2 1.3 0.2 setosa ## 3 3.2 1.4 0.2 setosa ## 4 2.8 4.5 1.3 versicolor ## 5 2.9 4.5 1.5 versicolor ## 6 2.7 4.2 1.3 versicolor ## 7 2.9 5.6 1.8 virginica ## 8 3.0 5.8 2.2 virginica ## 9 2.8 4.9 2.0 virginica ## 10 2.7 4.9 1.8 virginica  Session Info sessionInfo()  ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 11 (bullseye) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] RSQLite_2.2.14 data.table_1.14.2 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.9 ## [6] purrr_0.3.4 readr_2.1.2 tidyr_1.2.0 tibble_3.1.7 tidyverse_1.3.1 ## [11] ggplot2_3.3.6 limma_3.52.0 BiocStyle_2.24.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.8.3 lubridate_1.8.0 assertthat_0.2.1 digest_0.6.29 ## [5] utf8_1.2.2 plyr_1.8.7 R6_2.5.1 cellranger_1.1.0 ## [9] backports_1.4.1 reprex_2.0.1 evaluate_0.15 highr_0.9 ## [13] httr_1.4.3 pillar_1.7.0 rlang_1.0.2 readxl_1.4.0 ## [17] rstudioapi_0.13 blob_1.2.3 jquerylib_0.1.4 rmarkdown_2.14 ## [21] labeling_0.4.2 bit_4.0.4 munsell_0.5.0 broom_0.8.0 ## [25] compiler_4.2.0 modelr_0.1.8 xfun_0.30 pkgconfig_2.0.3 ## [29] htmltools_0.5.2 tidyselect_1.1.2 codetools_0.2-18 fansi_1.0.3 ## [33] crayon_1.5.1 tzdb_0.3.0 dbplyr_2.1.1 withr_2.5.0 ## [37] grid_4.2.0 jsonlite_1.8.0 gtable_0.3.0 lifecycle_1.0.1 ## [41] DBI_1.1.2 magrittr_2.0.3 scales_1.2.0 cachem_1.0.6 ## [45] cli_3.3.0 stringi_1.7.6 vroom_1.5.7 farver_2.1.0 ## [49] reshape2_1.4.4 fs_1.5.2 xml2_1.3.3 bslib_0.3.1 ## [53] ellipsis_0.3.2 generics_0.1.2 vctrs_0.4.1 tools_4.2.0 ## [57] bit64_4.0.5 glue_1.6.2 hms_1.1.1 parallel_4.2.0 ## [61] fastmap_1.1.0 yaml_2.3.5 colorspace_2.0-3 BiocManager_1.30.17 ## [65] rvest_1.0.2 memoise_2.0.1 knitr_1.39 haven_2.5.0 ## [69] sass_0.4.1  References “Learn the tidyverse.” n.d. https://www.tidyverse.org/learn/. https://www.tidyverse.org/learn/.\n  ","categories":"","description":"","excerpt":"document.addEventListener(\"DOMContentLoaded\", function() { …","ref":"/tutorials/dplyr/dplyr/","tags":"","title":"Environments dplyr, tidyr and some SQLite"},{"body":"Source code downloads: [ .Rmd ] [ .R ]\n Introduction A central concept for designing workflows within the systemPipeR environment is the usage of workflow management containers. For describing analysis workflows in a generic and flexible manner the Common Workflow Language (CWL) has been adopted throughout the environment including the workflow management containers (Amstutz et al. 2016). Using the CWL community standard in systemPipeR has many advantages. For instance, the integration of CWL allows running systemPipeR workflows from a single specification instance either entirely from within R, from various command line wrappers (e.g., cwl-runner) or from other languages (e.g., Bash or Python). An important feature of systemPipeR's CWL interface is that it provides two options to run command line tools and workflows based on CWL. First, one can run CWL in its native way via an R-based wrapper utility for cwl-runner or cwl-tools (CWL-based approach). Second, one can run workflows using CWL’s command line and workflow instructions from within R (R-based approach). In the latter case the same CWL workflow definition files (e.g. .cwl and .yml) are used but rendered and executed entirely with R functions defined by systemPipeR, and thus use CWL mainly as a command line and workflow definition format rather than execution software to run workflows. Moreover, systemPipeR provides several convenience functions that are useful for designing and debugging workflows, such as a command-line rendering function to retrieve the exact command-line strings for each data set and processing step prior to running a command-line.\nThis tutorial briefly introduces the basics how CWL defines command-line syntax. Next, it describes how to use CWL within systemPipeR for designing, modifying and running workflows.\nLoad package Recent versions of R (\u003e=4.0.0), Bioconductor (\u003e=3.14) and systemPipeR (\u003e=2.0.8) need to be used to gain access to the functions described in this tutorial.\nCWL command line specifications CWL command line specifications are written in YAML format.\nIn CWL, files with the extension .cwl define the parameters of a chosen command line step or workflow, while files with the extension .yml define the input variables of command line steps.\nThe following introduces first the basic structure of .cwl files.\ndir_path \u003c- system.file(\"extdata/cwl/example/\", package=\"systemPipeR\") cwl \u003c- yaml::read_yaml(file.path(dir_path, \"example.cwl\"))   The cwlVersion component specifies the version of CWL that is used here. The class component declares the usage of a command-line tool. Note, CWL has another class called Workflow. The latter defines one or more command-line tools, while CommandLineTool is limited one.  cwl[1:2]  ## $cwlVersion ## [1] \"v1.0\" ## ## $class ## [1] \"CommandLineTool\"   The baseCommand component contains the base name of the software to be executed.  cwl[3]  ## $baseCommand ## [1] \"echo\"   The inputs component provides the input information required for the command-line software. Important sub-components of this section are:  id: each input has an id assigning a name type: input type value (e.g. string, int, long, float, double, File, Directory or Any); inputBinding: optional component indicating if the input parameter should appear on the command line. If missing then the parameter will not appear in the command-line.    cwl[4]  ## $inputs ## $inputs$message ## $inputs$message$type ## [1] \"string\" ## ## $inputs$message$inputBinding ## $inputs$message$inputBinding$position ## [1] 1 ## ## ## ## $inputs$SampleName ## $inputs$SampleName$type ## [1] \"string\" ## ## ## $inputs$results_path ## $inputs$results_path$type ## [1] \"Directory\"   The outputs component should provide a list of the outputs expected after running a command-line tools. Important sub-components of this section are:  id: each output has an id assigning a name type: output type value (e.g. string, int, long, float, double, File, Directory, Any or stdout) outputBinding: defines how to set the outputs values. The glob component will define the name of the output value.    cwl[5]  ## $outputs ## $outputs$string ## $outputs$string$type ## [1] \"stdout\"   stdout: specifies a filename for capturing standard output. Note here we are using a syntax that takes advantage of the inputs section, using results_path parameter and also the SampleName to construct the filename of the output.  cwl[6]  ## $stdout ## [1] \"$(inputs.results_path.basename)/$(inputs.SampleName).txt\"  Next, the structure and content of the .yml files will be introduced. The .yml file provides the parameter values for the .cwl components described above.\nThe following example defines three parameters.\nyaml::read_yaml(file.path(dir_path, \"example_single.yml\"))  ## $message ## [1] \"Hello World!\" ## ## $SampleName ## [1] \"M1\" ## ## $results_path ## $results_path$class ## [1] \"Directory\" ## ## $results_path$path ## [1] \"./results\"  Importantly, if an input component is defined in the corresponding .cwl file, then the required value needs to be provided by the corresponding component of the .yml file.\nHow to connect CWL description files within systemPipeR A SYSargsList container stores several SYSargs2 instances in a list-like object containing all instructions required for processing a set of input files with a single or many command-line steps within a workflow (i.e. several tools of one software or several independent software tools). A single SYSargs2 object is created and fully populated with the constructor functions loadWF and renderWF.\nThe following imports a .cwl file (here example.cwl) for running a simple echo Hello World example where a string Hello World will be printed to stdout and redirected to a file named M1.txt located under a subdirectory named results.\nHW \u003c- loadWF(wf_file=\"example.cwl\", input_file=\"example_single.yml\", dir_path = dir_path) HW \u003c- renderWF(HW) HW  ## Instance of 'SYSargs2': ## Slot names/accessors: ## targets: 0 (...), targetsheader: 0 (lines) ## modules: 0 ## wf: 0, clt: 1, yamlinput: 3 (inputs) ## input: 1, output: 1 ## cmdlist: 1 ## Sub Steps: ## 1. example (rendered: TRUE)  cmdlist(HW)  ## $defaultid ## $defaultid$example ## [1] \"echo Hello World! \u003e results/M1.txt\"  The above example is limited to running only one command-line call, corresponding to one input file, e.g. representing a single experimental sample. To scale to many command-line calls, e.g. when processing many input samples, a simple solution offered by systemPipeR is to use variables, one for each parameter with many inputs.\nThe following gives a simple example for defining and processing many inputs.\nyml \u003c- yaml::read_yaml(file.path(dir_path, \"example.yml\")) yml  ## $message ## [1] \"_STRING_\" ## ## $SampleName ## [1] \"_SAMPLE_\" ## ## $results_path ## $results_path$class ## [1] \"Directory\" ## ## $results_path$path ## [1] \"./results\"  Under the message and SampleName parameters, variables are used for that will be populated by values provided by a third file called targets.\nThe following shows the structure of a simple targets file.\ntargetspath \u003c- system.file(\"extdata/cwl/example/targets_example.txt\", package=\"systemPipeR\") read.delim(targetspath, comment.char = \"#\")  ## Message SampleName ## 1 Hello World! M1 ## 2 Hello USA! M2 ## 3 Hello Bioconductor! M3  With help of a targets file, one can define all input files, sample ids and experimental variables relevant for an analysis workflow. In the above example, strings defined under the Message column will be passed on to the echo command-line tool. In addition, each command-line will be assigned a label or id specified under SampleName column. Any number of additional columns can be added as needed.\nUsers should note here, the usage of targets files is optional when using systemPipeR's CWL interface. Since targets files are very efficient for organizing experimental variables, their usage is highly encouraged and well supported in systemPipeR.\nConnect parameter and targets files The constructor functions construct an SYSargs2 instance from three input files:\n- `.cwl` file path assigned to `wf_file` argument - `.yml` file path assigned to `input_file` argument - `target` file assigned to `targets` argument  As mentioned above, the latter targets file is optional. The connection between input variables (here defined by input_file argument) and the targets file are defined under the inputvars argument. A named vector is required, where each element name needs to match the column names in the targets file, and the value must match the names of the .yml variables. This is used to replace the CWL variable and construct the command-lines, usually one for each input sample.\nFor consistency the pattern _XXXX_ is used for variable naming in the .yml file, where the name matches the corresponding column name in the targets file. This pattern is recommended for easy identification but not enforced.\nThe following imports a .cwl file (same example as above) for running the echo example. However, now several command-line calls are constructed with the information provided under the Message column of the targets file that is passed on to matching component in the .yml file.\nHW_mul \u003c- loadWorkflow(targets = targetspath, wf_file=\"example.cwl\", input_file=\"example.yml\", dir_path = dir_path) HW_mul \u003c- renderWF(HW_mul, inputvars = c(Message = \"_STRING_\", SampleName = \"_SAMPLE_\")) HW_mul  ## Instance of 'SYSargs2': ## Slot names/accessors: ## targets: 3 (M1...M3), targetsheader: 1 (lines) ## modules: 0 ## wf: 0, clt: 1, yamlinput: 3 (inputs) ## input: 3, output: 3 ## cmdlist: 3 ## Sub Steps: ## 1. example (rendered: TRUE)  cmdlist(HW_mul)  ## $M1 ## $M1$example ## [1] \"echo Hello World! \u003e results/M1.txt\" ## ## ## $M2 ## $M2$example ## [1] \"echo Hello USA! \u003e results/M2.txt\" ## ## ## $M3 ## $M3$example ## [1] \"echo Hello Bioconductor! \u003e results/M3.txt\"     Figure 1: Connectivity between CWL param files and targets files.\n Auto-creation of CWL param files from command-line Users can define the command-line in a pseudo-bash script format. The following used the the command-line for HISAT2 as example.\ncommand \u003c- \" hisat2 \\ -S \u003cF, out: ./results/M1A.sam\u003e \\ -x \u003cF: ./data/tair10.fasta\u003e \\ -k \u003cint: 1\u003e \\ -min-intronlen \u003cint: 30\u003e \\ -max-intronlen \u003cint: 3000\u003e \\ -threads \u003cint: 4\u003e \\ -U \u003cF: ./data/SRR446027_1.fastq.gz\u003e \"  Define prefix and defaults   First line is the base command. Each line is an argument with its default value.\n  All following lines specify arguments. Lines starting with a - or -- followed by a non-space delimited letter/word will be interpreted as a prefix, e.g. -S or --min. Lines without this prefix will be rendered as non-prefix arguments.\n  All default settings are placed inside \u003c...\u003e. Omit for arguments without values such as --verbose.\n  First argument is the type of the input. F for “File”, “int” and “string” are unchanged.\n  Optional: keyword out followed the type. Separation by , (comma) indicates whether this argument is also a CWL output.\n  Use : to separate keywords and default values. Any non-space separated value after the : will be treated as the default value.\n  createParamFiles Function The createParamFiles function accepts as input a command-line provided in above string syntax. The function returns a cwl with the following components:\n BaseCommand: Specifies the program to execute Inputs: Defines the input parameters of the process Outputs: Defines the parameters representing the output of the process  The fourth component is the original command-line provided as input.\nIn interactive mode, the function will verify if everything is correct and ask the user to proceed. The user can answer “no” and provide more information at the string input level. Another question is whether to save the generated CWL results to the corresponding .cwl and .yml files. When running the function in non-interactive mode, the results will be returned without asking for confirmation by the user.\ncmd \u003c- createParamFiles(command, writeParamFiles = FALSE)  ## *****BaseCommand***** ## hisat2 ## *****Inputs***** ## S: ## type: File ## preF: -S ## yml: ./results/M1A.sam ## x: ## type: File ## preF: -x ## yml: ./data/tair10.fasta ## k: ## type: int ## preF: -k ## yml: 1 ## min-intronlen: ## type: int ## preF: -min-intronlen ## yml: 30 ## max-intronlen: ## type: int ## preF: -max-intronlen ## yml: 3000 ## threads: ## type: int ## preF: -threads ## yml: 4 ## U: ## type: File ## preF: -U ## yml: ./data/SRR446027_1.fastq.gz ## *****Outputs***** ## output1: ## type: File ## value: ./results/M1A.sam ## *****Parsed raw command line***** ## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz  If the user chooses not to save the param files in the createParamFiles call directly, then the writeParamFiles function allows to do this in a separate step.\nwriteParamFiles(cmd, overwrite = TRUE)  ## Written content of 'commandLine' to file: ## param/cwl/hisat2/hisat2.cwl ## Written content of 'commandLine' to file: ## param/cwl/hisat2/hisat2.yml  Accessor functions Print components Note, the results of createParamFiles are stored in a SYSargs2 container. The individual components can be accessed as follows.\nprintParam(cmd, position = \"baseCommand\") ## Print a baseCommand section  ## *****BaseCommand***** ## hisat2  printParam(cmd, position = \"outputs\")  ## *****Outputs***** ## output1: ## type: File ## value: ./results/M1A.sam  printParam(cmd, position = \"inputs\", index = 1:2) ## Print by index  ## *****Inputs***** ## S: ## type: File ## preF: -S ## yml: ./results/M1A.sam ## x: ## type: File ## preF: -x ## yml: ./data/tair10.fasta  printParam(cmd, position = \"inputs\", index = -1:-2) ## Negative indexing printing to exclude certain indices in a position  ## *****Inputs***** ## k: ## type: int ## preF: -k ## yml: 1 ## min-intronlen: ## type: int ## preF: -min-intronlen ## yml: 30 ## max-intronlen: ## type: int ## preF: -max-intronlen ## yml: 3000 ## threads: ## type: int ## preF: -threads ## yml: 4 ## U: ## type: File ## preF: -U ## yml: ./data/SRR446027_1.fastq.gz  cmdlist(cmd)  ## $defaultid ## $defaultid$hisat2 ## [1] \"hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz\"  Subsetting the command-line cmd2 \u003c- subsetParam(cmd, position = \"inputs\", index = 1:2, trim = TRUE)  ## *****Inputs***** ## S: ## type: File ## preF: -S ## yml: ./results/M1A.sam ## x: ## type: File ## preF: -x ## yml: ./data/tair10.fasta ## *****Parsed raw command line***** ## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta  cmdlist(cmd2)  ## $defaultid ## $defaultid$hisat2 ## [1] \"hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta\"  cmd2 \u003c- subsetParam(cmd, position = \"inputs\", index = c(\"S\", \"x\"), trim = TRUE)  ## *****Inputs***** ## S: ## type: File ## preF: -S ## yml: ./results/M1A.sam ## x: ## type: File ## preF: -x ## yml: ./data/tair10.fasta ## *****Parsed raw command line***** ## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta  cmdlist(cmd2)  ## $defaultid ## $defaultid$hisat2 ## [1] \"hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta\"  Replacing existing argument cmd3 \u003c- replaceParam(cmd, \"base\", index = 1, replace = list(baseCommand = \"bwa\"))  ## Replacing baseCommand ## *****BaseCommand***** ## bwa ## *****Parsed raw command line***** ## bwa -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz  cmdlist(cmd3)  ## $defaultid ## $defaultid$hisat2 ## [1] \"bwa -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz\"  new_inputs \u003c- new_inputs \u003c- list( \"new_input1\" = list(type = \"File\", preF=\"-b\", yml =\"myfile\"), \"new_input2\" = \"-L \u003cint: 4\u003e\" ) cmd4 \u003c- replaceParam(cmd, \"inputs\", index = 1:2, replace = new_inputs)  ## Replacing inputs ## *****Inputs***** ## new_input1: ## type: File ## preF: -b ## yml: myfile ## new_input2: ## type: int ## preF: -L ## yml: 4 ## k: ## type: int ## preF: -k ## yml: 1 ## min-intronlen: ## type: int ## preF: -min-intronlen ## yml: 30 ## max-intronlen: ## type: int ## preF: -max-intronlen ## yml: 3000 ## threads: ## type: int ## preF: -threads ## yml: 4 ## U: ## type: File ## preF: -U ## yml: ./data/SRR446027_1.fastq.gz ## *****Parsed raw command line***** ## hisat2 -b myfile -L 4 -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz  cmdlist(cmd4)  ## $defaultid ## $defaultid$hisat2 ## [1] \"hisat2 -b myfile -L 4 -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz\"  Adding new arguments newIn \u003c- new_inputs \u003c- list( \"new_input1\" = list(type = \"File\", preF=\"-b1\", yml =\"myfile1\"), \"new_input2\" = list(type = \"File\", preF=\"-b2\", yml =\"myfile2\"), \"new_input3\" = \"-b3 \u003cF: myfile3\u003e\" ) cmd5 \u003c- appendParam(cmd, \"inputs\", index = 1:2, append = new_inputs)  ## Replacing inputs ## *****Inputs***** ## S: ## type: File ## preF: -S ## yml: ./results/M1A.sam ## x: ## type: File ## preF: -x ## yml: ./data/tair10.fasta ## k: ## type: int ## preF: -k ## yml: 1 ## min-intronlen: ## type: int ## preF: -min-intronlen ## yml: 30 ## max-intronlen: ## type: int ## preF: -max-intronlen ## yml: 3000 ## threads: ## type: int ## preF: -threads ## yml: 4 ## U: ## type: File ## preF: -U ## yml: ./data/SRR446027_1.fastq.gz ## new_input1: ## type: File ## preF: -b1 ## yml: myfile1 ## new_input2: ## type: File ## preF: -b2 ## yml: myfile2 ## new_input3: ## type: File ## preF: -b3 ## yml: myfile3 ## *****Parsed raw command line***** ## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz -b1 myfile1 -b2 myfile2 -b3 myfile3  cmdlist(cmd5)  ## $defaultid ## $defaultid$hisat2 ## [1] \"hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz -b1 myfile1 -b2 myfile2 -b3 myfile3\"  cmd6 \u003c- appendParam(cmd, \"inputs\", index = 1:2, after=0, append = new_inputs)  ## Replacing inputs ## *****Inputs***** ## new_input1: ## type: File ## preF: -b1 ## yml: myfile1 ## new_input2: ## type: File ## preF: -b2 ## yml: myfile2 ## new_input3: ## type: File ## preF: -b3 ## yml: myfile3 ## S: ## type: File ## preF: -S ## yml: ./results/M1A.sam ## x: ## type: File ## preF: -x ## yml: ./data/tair10.fasta ## k: ## type: int ## preF: -k ## yml: 1 ## min-intronlen: ## type: int ## preF: -min-intronlen ## yml: 30 ## max-intronlen: ## type: int ## preF: -max-intronlen ## yml: 3000 ## threads: ## type: int ## preF: -threads ## yml: 4 ## U: ## type: File ## preF: -U ## yml: ./data/SRR446027_1.fastq.gz ## *****Parsed raw command line***** ## hisat2 -b1 myfile1 -b2 myfile2 -b3 myfile3 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz  cmdlist(cmd6)  ## $defaultid ## $defaultid$hisat2 ## [1] \"hisat2 -b1 myfile1 -b2 myfile2 -b3 myfile3 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz\"  Editing output param new_outs \u003c- list( \"sam_out\" = \"\u003cF: $(inputs.results_path)/test.sam\u003e\" ) cmd7 \u003c- replaceParam(cmd, \"outputs\", index = 1, replace = new_outs)  ## Replacing outputs ## *****Outputs***** ## sam_out: ## type: File ## value: $(inputs.results_path)/test.sam ## *****Parsed raw command line***** ## hisat2 -S ./results/M1A.sam -x ./data/tair10.fasta -k 1 -min-intronlen 30 -max-intronlen 3000 -threads 4 -U ./data/SRR446027_1.fastq.gz  output(cmd7)  ## $defaultid ## $defaultid$hisat2 ## [1] \"./results/test.sam\"  Version information sessionInfo()  ## R version 4.2.0 (2022-04-22) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 11 (bullseye) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] systemPipeR_2.2.2 ShortRead_1.54.0 ## [3] GenomicAlignments_1.32.0 SummarizedExperiment_1.26.1 ## [5] Biobase_2.56.0 MatrixGenerics_1.8.0 ## [7] matrixStats_0.62.0 BiocParallel_1.30.0 ## [9] Rsamtools_2.12.0 Biostrings_2.64.0 ## [11] XVector_0.36.0 GenomicRanges_1.48.0 ## [13] GenomeInfoDb_1.32.1 IRanges_2.30.0 ## [15] S4Vectors_0.34.0 BiocGenerics_0.42.0 ## ## loaded via a namespace (and not attached): ## [1] lattice_0.20-45 png_0.1-7 assertthat_0.2.1 ## [4] digest_0.6.29 utf8_1.2.2 R6_2.5.1 ## [7] evaluate_0.15 ggplot2_3.3.6 blogdown_1.9 ## [10] pillar_1.7.0 zlibbioc_1.42.0 rlang_1.0.2 ## [13] jquerylib_0.1.4 Matrix_1.4-1 rmarkdown_2.14 ## [16] stringr_1.4.0 htmlwidgets_1.5.4 RCurl_1.98-1.6 ## [19] munsell_0.5.0 DelayedArray_0.22.0 compiler_4.2.0 ## [22] xfun_0.30 pkgconfig_2.0.3 htmltools_0.5.2 ## [25] tidyselect_1.1.2 tibble_3.1.7 GenomeInfoDbData_1.2.8 ## [28] bookdown_0.26 fansi_1.0.3 crayon_1.5.1 ## [31] dplyr_1.0.9 bitops_1.0-7 grid_4.2.0 ## [34] DBI_1.1.2 jsonlite_1.8.0 gtable_0.3.0 ## [37] lifecycle_1.0.1 magrittr_2.0.3 scales_1.2.0 ## [40] cli_3.3.0 stringi_1.7.6 hwriter_1.3.2.1 ## [43] latticeExtra_0.6-29 bslib_0.3.1 generics_0.1.2 ## [46] ellipsis_0.3.2 vctrs_0.4.1 RColorBrewer_1.1-3 ## [49] tools_4.2.0 glue_1.6.2 purrr_0.3.4 ## [52] jpeg_0.1-9 parallel_4.2.0 fastmap_1.1.0 ## [55] yaml_2.3.5 colorspace_2.0-3 knitr_1.39 ## [58] sass_0.4.1  Funding This project is funded by NSF award ABI-1661152.\nReferences Amstutz, Peter, Michael R Crusoe, Nebojša Tijanić, Brad Chapman, John Chilton, Michael Heuer, Andrey Kartashov, et al. 2016. “Common Workflow Language, V1.0,” July. https://doi.org/10.6084/m9.figshare.3115156.v2.\n  ","categories":"","description":"","excerpt":"Source code downloads: [ .Rmd ] [ .R ]\n Introduction A central concept …","ref":"/tutorials/cmdtocwl/cmdtocwl/","tags":"","title":"Automate Creation of CWL Instructions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/news/","tags":"","title":"News"},{"body":"\n\n Teaching material will be posted one day before each class meeting.\n [ Download ]\n ","categories":"","description":"","excerpt":"\n\n Teaching material will be posted one day before each class meeting. …","ref":"/slides/slides_01/","tags":"","title":"Course Introduction"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_02/","tags":"","title":"Genome Basics"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_03/","tags":"","title":"Databases and Software"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_04/","tags":"","title":"Sequencing Technologies"},{"body":"\n\n[ Download ]\n Nvim-R: Terminal-based R IDE [ View Slides in Separate Browser Tab ]\n  ","categories":"","description":"","excerpt":"\n\n[ Download ]\n Nvim-R: Terminal-based R IDE [ View Slides in Separate …","ref":"/slides/slides_05/","tags":"","title":"Introduction to R"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_06/","tags":"","title":"Sequence Alignments and Similarity Searching"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_07/","tags":"","title":"Programming in R"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_08/","tags":"","title":"Multiple Alignments"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_09/","tags":"","title":"Short Read Alignments"},{"body":"\n\n[ View Slides in Separate Browser Tab ]\n  ","categories":"","description":"","excerpt":"\n\n[ View Slides in Separate Browser Tab ]\n  ","ref":"/slides/slides_10/","tags":"","title":"R on HPC Systems"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_11/","tags":"","title":"NGS Analysis Basics"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_12/","tags":"","title":"Analysis of Gene Expression Data"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_13/","tags":"","title":"Introduction to NGS Workflows"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_14/","tags":"","title":"ChIP-Seq Overview"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_15/","tags":"","title":"Functional Enrichment Analysis"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_16/","tags":"","title":"Cluster Analysis"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_17/","tags":"","title":"Project Data Management"},{"body":"\n\n[ View Slides in Separate Browser Tab ]\n  ","categories":"","description":"","excerpt":"\n\n[ View Slides in Separate Browser Tab ]\n  ","ref":"/slides/slides_18/","tags":"","title":"Graphics and Data Visualization in R"},{"body":"\n\n[ View Slides in Separate Browser Tab ]\n  ","categories":"","description":"","excerpt":"\n\n[ View Slides in Separate Browser Tab ]\n  ","ref":"/slides/slides_19/","tags":"","title":"Building R packages"},{"body":"\n\n[ View Slides in Separate Browser Tab ]\n  ","categories":"","description":"","excerpt":"\n\n[ View Slides in Separate Browser Tab ]\n  ","ref":"/slides/slides_20/","tags":"","title":"Environments dplyr, tidyr and some SQLite"},{"body":"\n\n[ Download ]\n ","categories":"","description":"","excerpt":"\n\n[ Download ]\n ","ref":"/slides/slides_21/","tags":"","title":"Review of Data Analysis Workflows"},{"body":"\n\nThe following provides links to slide shows covering topics that where not included in this year’s offering of GEN242. These topics were excluded because they operlapped with similar material covered by other courses students in the class had taken already, a strong interest in learning more about advanced data analysis programming, as well as time limitations.\n De Novo Assembly of Genomes and Transcriptomes Profile HMMs for Sequence Family Modeling Phylogenetics Cheminformatics for Drug Discovery  ","categories":"","description":"","excerpt":"\n\nThe following provides links to slide shows covering topics that …","ref":"/slides/slides_22/","tags":"","title":"Additional Slide Shows"},{"body":"The homework assignments will be posted in this section.\n","categories":"","description":"","excerpt":"The homework assignments will be posted in this section.\n","ref":"/assignments/homework/","tags":"","title":"Homework Assignments"},{"body":"A. Online Excercise: Databases and Software Tools This is an easy warm-up homework exposing students to a variety of online databases and software tools.\n Go to http://www.ncbi.nlm.nih.gov, select Protein database in dropdown, and then run query: P450 \u0026 hydroxylase \u0026 human [organism], select under Source databases UniProtKB/Swiss-Prot  Report final query syntax from Search Details field.    \nSave GIs of the final query result to a file. For this select under Send to dropdown GI List format.  Report the number of retrieved GIs.    \nRetrieve the corresponding sequences through Batch-Entrez using GI list file as query input -\u003e save sequences in FASTA format  \nGenerate multiple alignment and tree of these sequences using MultAalin  Save multiple alignment and tree to file Identify putative heme binding cysteine in multiple alignment    \nOpen corresponding UniProt page and search for first P450 sequence in your list.  Compare putative heme binding cysteine with consensus pattern from Prosite database (Syntax) Report corresponding Pfam ID    \nBLASTP against PDF database (use again first P450 in your list); on result page click first entry in BLAST hit list (here 3K9V_A); then select ‘Identify Conserved Domains’ on side bar; click grey bar labelled ‘p450’; then select ‘Interactive View’ under ‘Structure’ menu which will download a file named ‘pfam00067.cn3’.  Compare resulting alignment with result from MultAlin View 3D structure (pfam00067.cn3) in Cn3D*, save structure (screen shot) and highlight heme binding cysteine. Note, Cn3D* can be downloaded from here.    *If there are problems in the last step (6.2) with the install of Cn3D, then please use this online only alternative: (i) click in the 3K9V_A page ‘Protein 3D Structure’ instead of ‘Identify Conserved Domains’; (ii) choose one of the two structure entries provided on the subsequent page; (iii) select option “full-featured 3D viewer” in the bottom right corner of the structure image; (iv) choose the ‘Details’ tab on the right; (v) after this the structure of the protein is shown on the left and the underlying protein sequence on the right; (vi) highlight the heme binding cysteine in the structure by selecting it in the sequence; and (vii) then save the structure view to a PNG file or take a screenshot.\nB. Homework Submission to a Private GitHub Repository Please assemble the results of this homework in one PDF file and upload it to your private course GitHub repository under Homework/HW1/HW1.pdf.\nDue date Most homework will be due one week after they are assigned. This one is due on Thu, April 7th at 6:00 PM. You have unlimited attempts. Students can edit and re-upload files anytime before the deadline.\nHomework solution A solution for this homework is not required since the tasks are identical to the steps described above under sections HW1A-B.\n","categories":"","description":"","excerpt":"A. Online Excercise: Databases and Software Tools This is an easy …","ref":"/assignments/homework/hw01/hw01/","tags":"","title":"HW1 - Online Exercise and Basic GitHub Usage"},{"body":"Topic: Linux Basics   Log into your user account on the HPCC cluster, and from there into a compute node with srun.\nsrun --x11 --partition=short --mem=2gb --cpus-per-task 4 --ntasks 1 --time 1:00:00 --pty bash -l    Download code from this page\nwget https://cluster.hpcc.ucr.edu/~tgirke/Linux.sh --no-check-certificate    Download Halobacterium proteome and inspect it\nwget https://ftp.ncbi.nlm.nih.gov/genomes/genbank/archaea/Halobacterium_salinarum/representative/GCA_004799605.1_ASM479960v1/GCA_004799605.1_ASM479960v1_protein.faa.gz gunzip GCA_004799605.1_ASM479960v1_protein.faa.gz mv GCA_004799605.1_ASM479960v1_protein.faa halobacterium.faa less halobacterium.faa # press q to quit    How many protein sequences are stored in the downloaded file?\ngrep '\u003e' halobacterium.faa | wc grep '^\u003e' halobacterium.faa --count    How many proteins contain the pattern WxHxxH or WxHxxHH?\negrep 'W.H..H{1,2}' halobacterium.faa --count    Use less to find IDs for pattern matches or use awk\nawk --posix -v RS='\u003e' '/W.H..(H){1,2}/ { print \"\u003e\" $0;}' halobacterium.faa | less awk --posix -v RS='\u003e' '/W.H..(H){1,2}/ { print \"\u003e\" $0;}' halobacterium.faa | grep '^\u003e' | cut -c 2- | cut -f 1 -d\\ \u003e myIDs    Create a BLASTable database with formatdb\nmodule load ncbi-blast/2.2.31+ makeblastdb -in halobacterium.faa -out halobacterium.faa -dbtype prot -hash_index -parse_seqids    Query BLASTable database by IDs stored in a file (e.g. myIDs)\nblastdbcmd -db halobacterium.faa -dbtype prot -entry_batch myIDs -get_dups -out myseq.fasta    Run BLAST search for sequences stored in myseq.fasta\nblastp -query myseq.fasta -db halobacterium.faa -outfmt 0 -evalue 1e-6 -out blastp.out blastp -query myseq.fasta -db halobacterium.faa -outfmt 6 -evalue 1e-6 -out blastp.tab    Return system time and host name\ndate hostname    Additional exercise material in Linux Manual\nHomework assignment Perform above analysis on the protein sequences from E. coli. A right click on the link will allow you to copy the URL so that it can be used together with wget. Record result from final BLAST command (with outfmt 6) in text file.\nHomework submission Upload result file to your private course GitHub repository under Homework/HW2/HW2.txt.\nDue date Most homeworks will be due one week after they are assigned. This one is due on Thu, April 7th at 6:00 PM.\nHomework solution See here.\n","categories":"","description":"","excerpt":"Topic: Linux Basics   Log into your user account on the HPCC cluster, …","ref":"/assignments/homework/hw02/hw02/","tags":"","title":"HW2 - Introduction to Biocluster and Linux"},{"body":"A. Object Subsetting, Import and Export  Task 1: Sort the rows of the iris data frame by its first column and sort its columns alphabetically by column names. Task 2: Subset the first 12 rows, export the result to a text file and view it in a spreadsheet program like Excel or Google Sheets. Task 3: Change some column titles in your spreadsheet program, save the result to a tab delimited text file and import it back into R. Note, for this task you only want to include the read.table command in the homework result (here R script).  Before you start it can be helpful to evaluate the structure of the iris data set with the following commands:\nclass(iris) dim(iris) colnames(iris)  B. Scatter Plots  Task 1: Generate a scatter plot for the first two columns of the iris data frame and color the dots by the Species column. Task 2: Use the xlim/ylim arguments to set limits on the x- and y-axes so that all data points are restricted to the bottom left quadrant of the plot.  Again before you start, evaluate the structure of iris data set. The following commands are useful:\niris[1:4,] table(iris$Species)  C. Bar Plots  Task 1: Calculate the mean values for the Species components of the first four columns in the iris data frame. Organize the results in a matrix where the row names are the unique values from the iris Species column and the column names are the names of the first four iris columns. Task 2: Generate two bar plots for the matrix generated in the previous step: one with stacked bars and one with horizontally arranged bars.  D-H. Analysis Worflow The instructions for these homework assignments are here.\nHomework submission Assemble the code from the homework assignments A-H in a single R script (HW3.R) and upload it to your private GitHub repository under Homework/HW3/HW3.R.\nDue date This homework is due on Thu, April 14th at 6:00 PM.\nHomework Solutions See here.\n","categories":"","description":"","excerpt":"A. Object Subsetting, Import and Export  Task 1: Sort the rows of the …","ref":"/assignments/homework/hw03/hw03/","tags":"","title":"HW3 - Introduction to R"},{"body":"\n\nA. Choice of Sequence Type  Task 1: Which sequence type - amino acid or nucleotide - is more appropriate to search databases for remotely related sequences? Provide at least three reasons for your decision.  B. Dynamic Programming for Pairwise Alignments  Task 2: Create manually (or write an R script for it) one global and one local alignment for the following two protein sequences using the Needleman-Wusch and Smith-Waterman algorithms, respectively:  O15528: PFGFGKRSCMGRRLA P98187: FIPFSAGPRNCIGQK  Use in each case BLOSUM50 as substitution matrix and 8 as gap extension penalty (no extra penalty for gap opening). Note, here is some helper code in R to create the initial matrix programmatically for upload to a spreadsheet program. Alternatively, solve the entire homework by writing an R script. Your answers should contain the following components:\n Manually populated dynamic programming matrices The optimal pairwise alignments created by traceback The final scores of the alignments  C. Alignments with Different Substitution Matrices  Task 1: Load the Biostrings package in R, import the following two cytochrome P450 sequences O15528 and P98187 from NCBI (save as myseq.fasta), and create a global alignment with the pairwiseAlignment function from Biostrings as follows:  library(Biostrings) myseq \u003c- readAAStringSet(\"myseq.fasta\", \"fasta\") (p \u003c- pairwiseAlignment(myseq[[1]], myseq[[2]], type=\"global\", substitutionMatrix=\"BLOSUM50\")) writePairwiseAlignments(p)  Your answers should address the following:\n Record the scores for the scoring matrices BLOSUM50, BLOSUM62 and BLOSUM80. How and why do the scores differ for the three scoring matrices?  Homework submission Assemble the results from this homework in one PDF file (HW4.pdf) and upload it to your private GitHub repository under Homework/HW4/HW4.pdf.\nDue date This homework is due in two weeks on Thu, April 21 at 6:00 PM.\nHomework Solutions See here.\n","categories":"","description":"","excerpt":"\n\nA. Choice of Sequence Type  Task 1: Which sequence type - amino acid …","ref":"/assignments/homework/hw04/hw04/","tags":"","title":"HW4: Pairwise Alignments"},{"body":"  Source code downloads: [ .Rmd ] [ pairwiseAlign_Fct.R ]\n Rendering Instructions To render this R Markdown document, one needs to download the following files to the same directory.\n HW4_key.Rmd: Rmd source file for this document pairwiseAlign_Fct.R: R script defining pairwise alignment functions bibtex.bib: references cited in the text in BibTeX format  Next, one can render the report to HTML, PDF and other formats following the instructions below. Both the HTML and PDF versions are linked here:\n HTML: this report in HTML format PDF: corresponding PDF version  The HTML report can be rendered with rmarkdown::render() as follows.\nrmarkdown::render('HW4_key.Rmd') # From R Rscript -e \"rmarkdown::render('HW4_key.Rmd')\" # From command-line  To render a PDF file instead of HTML, one can instruct the rendering function to do so like this: rmarkdown::render('HW4_key.Rmd', c('pdf_document'). To render to several formats with a single command, one can concatenate the formatting values with c('html_document', 'pdf_document').\nA. Choice of Sequence Type Task 1: Which sequence type - amino acid or nucleotide - is more appropriate to search databases for remotely related sequences? Provide at least three reasons for your decision.\nAnswer: When coding sequences are expected to have weak similarities then one should use protein sequences rather than DNA sequences for database searching, because of (1) their higher information content (20 versus 4 letter alphabet), as well as (2) the better scoring and (3) functional classification systems available for amino acids.\nB. Dynamic Programming for Pairwise Alignments Task 2: Create manually (or write an R script for it) one global and one local alignment for the following two protein sequences using the Needleman-Wusch and Smith-Waterman algorithms, respectively (Smith and Waterman 1981; Needleman and Wunsch 1970).\nO15528: PFGFGKRSCMGRRLA P98187: FIPFSAGPRNCIGQK  Source functions All alignment functions used in the following sections are defined in the downloaded R script file that is named pairwiseAlign_Fct.R. These functions are loaded with the source() command below.\nsource(\"pairwiseAlign_Fct.R\")  Input sequences Define within R or import them (here former).\nS1 \u003c- \"PFGFGKRSCMGRRLA\" S2 \u003c- \"FIPFSAGPRNCIGQK\"  Additional test sequences\n# S1 \u003c- \"HEAGAWGHEE\" # S2 \u003c- \"PAWHEAE\"  Global alignment The alignment type choice is passed on to all following functions.\nalign_type \u003c- \"global\" # align_type \u003c- \"local\"  Dynamic programming matrices dynMA \u003c- dynProgMatrix(S1, S2, align_method=align_type, gap_penalty=8, substitutionMA=\"BLOSUM50\")  The matrices are stored in a list and returned below. The path is indicated by three numbers in the glob_ma_path matrix. Their meaning is:\n 1: diagonal 2: vertical (up) 3: horizontal (left)  dynMA  ## $glob_ma ## gp P F G F G K R S C M G R R L A ## gp 0 -8 -16 -24 -32 -40 -48 -56 -64 -72 -80 -88 -96 -104 -112 -120 ## F -8 -4 0 -8 -16 -24 -32 -40 -48 -56 -64 -72 -80 -88 -96 -104 ## I -16 -11 -4 -4 -8 -16 -24 -32 -40 -48 -54 -62 -70 -78 -86 -94 ## P -24 -6 -12 -6 -8 -10 -17 -25 -33 -41 -49 -56 -64 -72 -80 -87 ## F -32 -14 2 -6 2 -6 -14 -20 -28 -35 -41 -49 -57 -65 -71 -79 ## S -40 -22 -6 2 -6 2 -6 -14 -15 -23 -31 -39 -47 -55 -63 -70 ## A -48 -30 -14 -6 -1 -6 1 -7 -13 -16 -24 -31 -39 -47 -55 -58 ## G -56 -38 -22 -6 -9 7 -1 -2 -7 -15 -19 -16 -24 -32 -40 -48 ## P -64 -46 -30 -14 -10 -1 6 -2 -3 -11 -18 -21 -19 -27 -35 -41 ## R -72 -54 -38 -22 -17 -9 2 13 5 -3 -11 -19 -14 -12 -20 -28 ## N -80 -62 -46 -30 -25 -17 -6 5 14 6 -2 -10 -18 -15 -16 -21 ## C -88 -70 -54 -38 -32 -25 -14 -3 6 27 19 11 3 -5 -13 -17 ## I -96 -78 -62 -46 -38 -33 -22 -11 -2 19 29 21 13 5 -3 -11 ## G -104 -86 -70 -54 -46 -30 -30 -19 -10 11 21 37 29 21 13 5 ## Q -112 -94 -78 -62 -54 -38 -28 -27 -18 3 13 29 38 30 22 14 ## K -120 -102 -86 -70 -62 -46 -32 -25 -26 -5 5 21 32 41 33 25 ## ## $glob_ma_path ## gp P F G F G K R S C M G R R L A ## gp 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## F 2 1 1 3 1 3 3 3 3 3 3 3 3 3 3 3 ## I 2 1 1 1 1 3 3 3 3 3 1 3 3 3 1 3 ## P 2 1 2 1 1 1 1 3 1 3 3 1 3 3 3 1 ## F 2 2 1 3 1 3 1 1 1 1 1 3 3 3 1 3 ## S 2 2 2 1 2 1 1 3 1 3 3 3 3 3 3 1 ## A 2 2 2 1 1 1 1 3 1 1 1 1 3 3 3 1 ## G 2 2 2 1 2 1 3 1 1 3 1 1 3 3 3 3 ## P 2 1 2 2 1 2 1 3 1 1 1 1 1 1 3 1 ## R 2 2 2 2 1 2 1 1 3 3 3 3 1 1 3 3 ## N 2 2 2 2 2 1 2 2 1 3 3 3 3 1 1 1 ## C 2 2 2 2 1 2 2 2 2 1 3 3 3 3 3 1 ## I 2 2 2 2 1 2 2 2 2 2 1 3 3 3 1 3 ## G 2 2 2 1 2 1 2 2 2 2 2 1 3 3 3 3 ## Q 2 2 2 2 2 2 1 2 2 2 2 2 1 1 3 3 ## K 2 2 2 2 2 2 1 1 2 2 2 2 1 1 3 3  Compute alignment The following alignList stores all relevant results in a list, including dynamic programming matrices, as well as the coordinates (named path_coor) to highlight path in dynamic progamming matrix (see below).\nalignList \u003c- alignmentTraceback(ma=dynMA[[1]], ma_path=dynMA[[2]], align_method=align_type) names(alignList)  ## [1] \"ma\" \"ma_path\" \"path_coor\" \"as1\" \"consensus\" \"as2\" ## [7] \"score\"  # alignList$ma # dyn ma with scores # alignList$ma_path # dyn ma with path # alignList$path_coor # coordinates for path to auto highlight path in HTML/PDF table  Return results Traceback in matrix The following prints the fully populated dynamic programming matrix where the traceback path is highlighted in color.\nprintColMa(alignList)     gp\n P\n F\n G\n F\n G\n K\n R\n S\n C\n M\n G\n R\n R\n L\n A\n     gp\n 0\n -8\n -16\n -24\n -32\n -40\n -48\n -56\n -64\n -72\n -80\n -88\n -96\n -104\n -112\n -120\n   F\n -8\n -4\n 0\n -8\n -16\n -24\n -32\n -40\n -48\n -56\n -64\n -72\n -80\n -88\n -96\n -104\n   I\n -16\n -11\n -4\n -4\n -8\n -16\n -24\n -32\n -40\n -48\n -54\n -62\n -70\n -78\n -86\n -94\n   P\n -24\n -6\n -12\n -6\n -8\n -10\n -17\n -25\n -33\n -41\n -49\n -56\n -64\n -72\n -80\n -87\n   F\n -32\n -14\n 2\n -6\n 2\n -6\n -14\n -20\n -28\n -35\n -41\n -49\n -57\n -65\n -71\n -79\n   S\n -40\n -22\n -6\n 2\n -6\n 2\n -6\n -14\n -15\n -23\n -31\n -39\n -47\n -55\n -63\n -70\n   A\n -48\n -30\n -14\n -6\n -1\n -6\n 1\n -7\n -13\n -16\n -24\n -31\n -39\n -47\n -55\n -58\n   G\n -56\n -38\n -22\n -6\n -9\n 7\n -1\n -2\n -7\n -15\n -19\n -16\n -24\n -32\n -40\n -48\n   P\n -64\n -46\n -30\n -14\n -10\n -1\n 6\n -2\n -3\n -11\n -18\n -21\n -19\n -27\n -35\n -41\n   R\n -72\n -54\n -38\n -22\n -17\n -9\n 2\n 13\n 5\n -3\n -11\n -19\n -14\n -12\n -20\n -28\n   N\n -80\n -62\n -46\n -30\n -25\n -17\n -6\n 5\n 14\n 6\n -2\n -10\n -18\n -15\n -16\n -21\n   C\n -88\n -70\n -54\n -38\n -32\n -25\n -14\n -3\n 6\n 27\n 19\n 11\n 3\n -5\n -13\n -17\n   I\n -96\n -78\n -62\n -46\n -38\n -33\n -22\n -11\n -2\n 19\n 29\n 21\n 13\n 5\n -3\n -11\n   G\n -104\n -86\n -70\n -54\n -46\n -30\n -30\n -19\n -10\n 11\n 21\n 37\n 29\n 21\n 13\n 5\n   Q\n -112\n -94\n -78\n -62\n -54\n -38\n -28\n -27\n -18\n 3\n 13\n 29\n 38\n 30\n 22\n 14\n   K\n -120\n -102\n -86\n -70\n -62\n -46\n -32\n -25\n -26\n -5\n 5\n 21\n 32\n 41\n 33\n 25\n    Alignment and score printAlign(x=alignList)  ## ## S1: --PFGFGKRSCMGRRLA ## || | | | | ## S2: FIPFSAGPRNCIGQK-- ## ## Score of alignment: 25  Local alignment The alignment type choice is passed on to all following functions.\n# align_type \u003c- \"global\" align_type \u003c- \"local\"  Dynamic programming matrices dynMA \u003c- dynProgMatrix(S1, S2, align_method=align_type, gap_penalty=8, substitutionMA=\"BLOSUM50\")  The matrices are stored in a list and returned below.\ndynMA  ## $loc_ma ## gp P F G F G K R S C M G R R L A ## gp 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## F 0 0 8 0 8 0 0 0 0 0 0 0 0 0 1 0 ## I 0 0 0 4 0 4 0 0 0 0 2 0 0 0 2 0 ## P 0 10 2 0 0 0 3 0 0 0 0 0 0 0 0 1 ## F 0 2 18 10 8 0 0 0 0 0 0 0 0 0 1 0 ## S 0 0 10 18 10 8 0 0 5 0 0 0 0 0 0 2 ## A 0 0 2 10 15 10 7 0 1 4 0 0 0 0 0 5 ## G 0 0 0 10 7 23 15 7 0 0 1 8 0 0 0 0 ## P 0 10 2 2 6 15 22 14 6 0 0 0 5 0 0 0 ## R 0 2 7 0 0 7 18 29 21 13 5 0 7 12 4 0 ## N 0 0 0 7 0 0 10 21 30 22 14 6 0 6 8 3 ## C 0 0 0 0 5 0 2 13 22 43 35 27 19 11 4 7 ## I 0 0 0 0 0 1 0 5 14 35 45 37 29 21 13 5 ## G 0 0 0 8 0 8 0 0 6 27 37 53 45 37 29 21 ## Q 0 0 0 0 4 0 10 2 0 19 29 45 54 46 38 30 ## K 0 0 0 0 0 2 6 13 5 11 21 37 48 57 49 41 ## ## $loc_ma_path ## gp P F G F G K R S C M G R R L A ## gp 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## F 0 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 ## I 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## P 0 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 ## F 0 2 1 3 1 3 1 1 1 1 1 1 1 1 1 1 ## S 0 1 2 1 3 1 1 1 1 1 1 1 1 1 1 1 ## A 0 1 2 1 1 1 1 3 1 1 1 1 1 1 1 1 ## G 0 1 1 1 2 1 3 3 1 1 1 1 3 1 1 1 ## P 0 1 3 2 1 2 1 3 1 3 1 2 1 1 1 1 ## R 0 2 1 1 1 2 1 1 3 3 3 1 1 1 3 1 ## N 0 1 2 1 3 1 2 2 1 3 3 3 1 1 1 1 ## C 0 1 1 2 1 1 2 2 2 1 3 3 3 3 1 1 ## I 0 1 1 1 1 1 1 2 2 2 1 3 3 3 1 3 ## G 0 1 1 1 3 1 3 1 2 2 2 1 3 3 3 3 ## Q 0 1 1 2 1 2 1 3 1 2 2 2 1 1 3 3 ## K 0 1 1 1 1 1 1 1 3 2 2 2 1 1 3 3  Compute alignment Note: alignList stores all relevant results in a list, including dynamic programming matrices, as well as the coordinates (named path_coor) to highlight the path in the dynamic progamming matrix. This way one can easily generate a single dynamic programming matrix with the traceback path highlighted by colors or arrows in an HTML or PDF document (see below).\nalignList \u003c- alignmentTraceback(ma=dynMA[[1]], ma_path=dynMA[[2]], align_method=align_type) names(alignList)  ## [1] \"ma\" \"ma_path\" \"path_coor\" \"as1\" \"consensus\" \"as2\" ## [7] \"score\"  # alignList$ma # dyn ma with scores # alignList$ma_path # dyn ma with path # alignList$path_coor # coordinates for path to auto highlight path in HTML/PDF table  Return results Traceback in matrix The following prints the fully populated dynamic programming matrix where the traceback path is highlighted in color.\nprintColMa(alignList)     gp\n P\n F\n G\n F\n G\n K\n R\n S\n C\n M\n G\n R\n R\n L\n A\n     gp\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n   F\n 0\n 0\n 8\n 0\n 8\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 1\n 0\n   I\n 0\n 0\n 0\n 4\n 0\n 4\n 0\n 0\n 0\n 0\n 2\n 0\n 0\n 0\n 2\n 0\n   P\n 0\n 10\n 2\n 0\n 0\n 0\n 3\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 1\n   F\n 0\n 2\n 18\n 10\n 8\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 1\n 0\n   S\n 0\n 0\n 10\n 18\n 10\n 8\n 0\n 0\n 5\n 0\n 0\n 0\n 0\n 0\n 0\n 2\n   A\n 0\n 0\n 2\n 10\n 15\n 10\n 7\n 0\n 1\n 4\n 0\n 0\n 0\n 0\n 0\n 5\n   G\n 0\n 0\n 0\n 10\n 7\n 23\n 15\n 7\n 0\n 0\n 1\n 8\n 0\n 0\n 0\n 0\n   P\n 0\n 10\n 2\n 2\n 6\n 15\n 22\n 14\n 6\n 0\n 0\n 0\n 5\n 0\n 0\n 0\n   R\n 0\n 2\n 7\n 0\n 0\n 7\n 18\n 29\n 21\n 13\n 5\n 0\n 7\n 12\n 4\n 0\n   N\n 0\n 0\n 0\n 7\n 0\n 0\n 10\n 21\n 30\n 22\n 14\n 6\n 0\n 6\n 8\n 3\n   C\n 0\n 0\n 0\n 0\n 5\n 0\n 2\n 13\n 22\n 43\n 35\n 27\n 19\n 11\n 4\n 7\n   I\n 0\n 0\n 0\n 0\n 0\n 1\n 0\n 5\n 14\n 35\n 45\n 37\n 29\n 21\n 13\n 5\n   G\n 0\n 0\n 0\n 8\n 0\n 8\n 0\n 0\n 6\n 27\n 37\n 53\n 45\n 37\n 29\n 21\n   Q\n 0\n 0\n 0\n 0\n 4\n 0\n 10\n 2\n 0\n 19\n 29\n 45\n 54\n 46\n 38\n 30\n   K\n 0\n 0\n 0\n 0\n 0\n 2\n 6\n 13\n 5\n 11\n 21\n 37\n 48\n 57\n 49\n 41\n    Alignment and score printAlign(x=alignList)  ## ## S1: PFGFGKRSCMGRR ## || | | | | ## S2: PFSAGPRNCIGQK ## ## Score of alignment: 57  C. Different Substitution Matrices Task 1: Load the Biostrings package in R, import the following two cytochrome P450 sequences O15528 and P98187 from NCBI (save as myseq.fasta), and create a global alignment with the pairwiseAlignment function from Biostrings as follows.\nlibrary(Biostrings) myseq \u003c- readAAStringSet(\"myseq.fasta\", \"fasta\") (p \u003c- pairwiseAlignment(myseq[[1]], myseq[[2]], type=\"global\", substitutionMatrix=\"BLOSUM50\")) writePairwiseAlignments(p)  Your answers should address the following items:\nRecord the scores for the scoring matrices BLOSUM50, BLOSUM62 and BLOSUM80. How and why do the scores differ for the three scoring matrices?\nAnswer 1: The scores for the three BLOSUM substitutions matrices are:\n BLOSUM50: 227 BLOSUM62: 54 BLOSUM80: -52  Answer 2: Since the two sequences are relatively dissimilar (as determined by alignment view from writePairwiseAlignments(p)) it is expected that the BLOSUM matrices trained on less similar sequences (e.g. BLOSUM50) result in higher scores than those trained on more similar sequences (e.g. BLOSUM80).\nSession Info sessionInfo()  ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Debian GNU/Linux 10 (buster) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats4 stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] kableExtra_1.3.4 Biostrings_2.62.0 GenomeInfoDb_1.30.0 ## [4] XVector_0.34.0 IRanges_2.28.0 S4Vectors_0.32.3 ## [7] BiocGenerics_0.40.0 ## ## loaded via a namespace (and not attached): ## [1] bslib_0.3.1 compiler_4.1.3 jquerylib_0.1.4 ## [4] bitops_1.0-7 tools_4.1.3 zlibbioc_1.40.0 ## [7] digest_0.6.29 viridisLite_0.4.0 jsonlite_1.8.0 ## [10] evaluate_0.15 lifecycle_1.0.1 rlang_1.0.2 ## [13] cli_3.1.0 rstudioapi_0.13 yaml_2.3.5 ## [16] blogdown_1.8.2 xfun_0.30 fastmap_1.1.0 ## [19] GenomeInfoDbData_1.2.7 stringr_1.4.0 httr_1.4.2 ## [22] knitr_1.37 xml2_1.3.3 systemfonts_1.0.4 ## [25] sass_0.4.0 webshot_0.5.3 svglite_2.1.0 ## [28] glue_1.6.2 R6_2.5.1 rmarkdown_2.13 ## [31] bookdown_0.24 magrittr_2.0.2 scales_1.1.1 ## [34] htmltools_0.5.2 rvest_1.0.2 colorspace_2.0-2 ## [37] stringi_1.7.6 munsell_0.5.0 RCurl_1.98-1.5 ## [40] crayon_1.4.2  References Needleman, S B, and C D Wunsch. 1970. “A general method applicable to the search for similarities in the amino acid sequence of two proteins.” J. Mol. Biol. 48 (3): 443–53. https://doi.org/10.1016/0022-2836(70)90057-4.\n Smith, T F, and M S Waterman. 1981. “Identification of common molecular subsequences.” J. Mol. Biol. 147 (1): 195–97. http://www.ncbi.nlm.nih.gov/pubmed/7265238.\n  ","categories":"","description":"","excerpt":"  Source code downloads: [ .Rmd ] [ pairwiseAlign_Fct.R ]\n Rendering …","ref":"/assignments/homework/hw04/hw4_solution/hw4_key/","tags":"","title":"HW4: Pairwise Alignment Solutions"},{"body":"\n\nSource code downloads:   [ .R ]  A. Reverse and complement of DNA Task 1: Write a RevComp function that returns the reverse and complement of a DNA sequence string. Include an argument that will allow to return only (i) the reversed sequence, (ii) the complemented sequence, or (iii) the reversed and complemented sequence. The following R functions will be useful for the implementation:\nGenerate a short test DNA sequence\nx \u003c- c(\"ATGCATTGGACGTTAG\") x  ## [1] \"ATGCATTGGACGTTAG\"  Vectorize sequence\nx \u003c- substring(x, 1:nchar(x), 1:nchar(x)) x  ## [1] \"A\" \"T\" \"G\" \"C\" \"A\" \"T\" \"T\" \"G\" \"G\" \"A\" \"C\" \"G\" \"T\" \"T\" \"A\" \"G\"  Reverse sequence\nx \u003c- rev(x) x  ## [1] \"G\" \"A\" \"T\" \"T\" \"G\" \"C\" \"A\" \"G\" \"G\" \"T\" \"T\" \"A\" \"C\" \"G\" \"T\" \"A\"  Collapse sequence back to character string\nx \u003c- paste(x, collapse=\"\") x  ## [1] \"GATTGCAGGTTACGTA\"  Form complement of sequence\nchartr(\"ATGC\", \"TACG\", x)  ## [1] \"CTAACGTCCAATGCAT\"  Task 2: Write a function that applies the RevComp function to many sequences stored in a vector.\nB. Translate DNA into Protein Task 3: Write a function that will translate one or many DNA sequences in all three reading frames into proteins. The following commands will simplify this task:\nImport lookup table of genetic code\nAAdf \u003c- read.table(file=\"http://faculty.ucr.edu/~tgirke/Documents/R_BioCond/My_R_Scripts/AA.txt\", header=TRUE, sep=\"\\t\") AAdf[1:4,]  ## Codon AA_1 AA_3 AA_Full AntiCodon ## 1 TCA S Ser Serine TGA ## 2 TCG S Ser Serine CGA ## 3 TCC S Ser Serine GGA ## 4 TCT S Ser Serine AGA  Generated named vector of relevant components\nAAv \u003c- as.character(AAdf[,2]) names(AAv) \u003c- AAdf[,1] AAv  ## TCA TCG TCC TCT TTT TTC TTA TTG TAT TAC TAA TAG TGT TGC TGA TGG CTA CTG CTC CTT CCA CCG CCC CCT CAT ## \"S\" \"S\" \"S\" \"S\" \"F\" \"F\" \"L\" \"L\" \"Y\" \"Y\" \"*\" \"*\" \"C\" \"C\" \"*\" \"W\" \"L\" \"L\" \"L\" \"L\" \"P\" \"P\" \"P\" \"P\" \"H\" ## CAC CAA CAG CGA CGG CGC CGT ATT ATC ATA ATG ACA ACG ACC ACT AAT AAC AAA AAG AGT AGC AGA AGG GTA GTG ## \"H\" \"Q\" \"Q\" \"R\" \"R\" \"R\" \"R\" \"I\" \"I\" \"I\" \"M\" \"T\" \"T\" \"T\" \"T\" \"N\" \"N\" \"K\" \"K\" \"S\" \"S\" \"R\" \"R\" \"V\" \"V\" ## GTC GTT GCA GCG GCC GCT GAT GAC GAA GAG GGA GGG GGC GGT ## \"V\" \"V\" \"A\" \"A\" \"A\" \"A\" \"D\" \"D\" \"E\" \"E\" \"G\" \"G\" \"G\" \"G\"  Tripletize sequence and translate by name subsetting/sorting of AAv\ny \u003c- gsub(\"(...)\", \"\\\\1_\", x) y \u003c- unlist(strsplit(y, \"_\")) y \u003c- y[grep(\"^...$\", y)] AAv[y]  ## GAT TGC AGG TTA CGT ## \"D\" \"C\" \"R\" \"L\" \"R\"  Homework submission Submit the 3 functions in one well structured and annotated R script to your private GitHub repository under Homework/HW5/HW5.R. The script should include instructions on how to use the functions.\nDue date This homework is due on Thu, April 21 at 6:00 PM.\nHomework Solutions See here.\n","categories":"","description":"","excerpt":"\n\nSource code downloads:   [ .R ]  A. Reverse and complement of DNA …","ref":"/assignments/homework/hw05/hw05/","tags":"","title":"HW5 - Programming in R"},{"body":"\n\nSource code downloads:   [ .R ]  A. Demultiplexing Write a demultiplexing function that accepts any number of barcodes and splits a FASTQ file into as many subfiles as there are barcodes. At the same time the function should remove low quality tails from the reads. The following function accomplishes the first step. Expand this function so that it performs the second step as well. As test data set one can use the FASTQ test files downloaded in the corresponding tutorial section here.\ndemultiplex \u003c- function(x, barcode, nreads) { f \u003c- FastqStreamer(x, nreads) while(length(fq \u003c- yield(f))) { for(i in barcode) { pattern \u003c- paste(\"^\", i, sep=\"\") fqsub \u003c- fq[grepl(pattern, sread(fq))] if(length(fqsub) \u003e 0) { writeFastq(fqsub, paste(x, i, sep=\"_\"), mode=\"a\", compress=FALSE) } } } close(f) } demultiplex(x=fastq[1], barcode=c(\"TT\", \"AA\", \"GG\"), nreads=50)  B. Sequence Parsing  Download GFF from Halobacterium sp here Download genome sequence from Halobacterium sp here Task 1 Extract gene ranges, parse their sequences from genome and translate them into proteins Task 2 Reduce overlapping genes and parse their sequences from genome Task 3 Generate intergenic ranges and parse their sequences from genome  Useful commands\ndownload.file(\"https://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Bacteria/Halobacterium_sp_uid217/AE004437.gff\", \"data/AE004437.gff\") download.file(\"https://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Bacteria/Halobacterium_sp_uid217/AE004437.fna\", \"data/AE004437.fna\") chr \u003c- readDNAStringSet(\"data/AE004437.fna\") gff \u003c- import(\"data/AE004437.gff\") gffgene \u003c- gff[values(gff)[,\"type\"]==\"gene\"] gene \u003c- DNAStringSet(Views(chr[[1]], IRanges(start(gffgene), end(gffgene)))) names(gene) \u003c- values(gffgene)[,\"locus_tag\"] pos \u003c- values(gffgene[strand(gffgene) == \"+\"])[,\"locus_tag\"] p1 \u003c- translate(gene[names(gene) %in% pos]) names(p1) \u003c- names(gene[names(gene) %in% pos]) neg \u003c- values(gffgene[strand(gffgene) == \"-\"])[,\"locus_tag\"] p2 \u003c- translate(reverseComplement(gene[names(gene) %in% neg])) names(p2) \u003c- names(gene[names(gene) %in% neg]) writeXStringSet(c(p1, p2), \"./data/mypep.fasta\")  Homework submission Please submit the homework results in one well structured and annotated R script to your private GitHub repository under Homework/HW6/HW6.R. The script should include instructions on how to use the functions.\nDue date This homework is due on Thu, April 28 at 6:00 PM.\nHomework Solutions See here.\n","categories":"","description":"","excerpt":"\n\nSource code downloads:   [ .R ]  A. Demultiplexing Write a …","ref":"/assignments/homework/hw06/hw06/","tags":"","title":"HW6 - NGS Analysis Basics"},{"body":"\n\nSource code downloads:   [ .R ]  A. Unstranded and strand-specific read counting   Task 1: Rerun the RNA-Seq workflow with the toy data sets up to the read quantification step here. Note, the toy data set gets automatically loaded when intializing a workflow environment (directory structure) with the genWorkenvir function (see tutorial here). In the read quantification step with summarizeOverlaps generate count tables for exons by genes (eByg) of the following three strand modes:\n Unstranded Strand-specific for positive (sense) strand Strand-specific for negative (antisense) strand  The solution for generating the unstranded read counts is given below. Note, the upstream steps of the RNA-Seq workflow only need to be rerun to generate the proper inputs for the read counting. Thus, they are not required to be included in the homework results (see HW7.R below).\n  summarizeOverlaps(eByg, bfl, mode=\"Union\", ignore.strand=TRUE, # preprocess.reads=invertStrand, inter.feature=FALSE, singleEnd=FALSE)  Before attempting to solve this homework task please read the vignette Counting reads with summarizeOverlaps (here) from the GenomicAlignments package that defines the summarizeOverlap function. In addition, the help file for ?summarizeOverlaps provides useful information.\n  Task 2: Provide R code that demonstrates that the two strand-specific count tables sum up to very similar values as the unstranded count table.\n  Task 3: Explain the utility (biological relevance) of the different strand counting modes used under Task 1. Include your explanation as comment text in your homework script (see HW7.R below).\n  Note, for Tasks 1-3 only the code and/or text needs to be included in the homework submission (no data/result files). For details see below.\nB. Read counting for different feature types   Task 4: Compute strand-specific count tables for the positive (sense) strand of the following feature types. The help files of ?exonsBy and ?transcripts provide useful information for solving these tasks.\n Genes Exons Exons by genes Introns by transcripts 5’-UTRs by transcripts    Note, for Tasks 4 only include the code and/or text in your homework submission (no data/result files).\nC. DEG analysis   Task 5: Perform the DEG analysis with edgeR as outlined under section 6 of the RNA-Seq workflow here. Use in one case for the DEG analysis the unstranded count table as input (from Task 1.1) and in another the sense strand count table (from Task 1.2). Compare the DEG result of the two methods in two separate 4-way Venn diagrams for the same sample comparisons used in the workflow example here.\n 4-way Venn diagram for unstranded count table 4-way Venn diagram for sense strand count table    Note, for Tasks 5 include both the code and the resulting images in your homework submission.\nHomework submission Please submit the homework results in one well structured and annotated R script to your private GitHub repository under Homework/HW7/HW7.R. Instead of an R script the homework can be submitted in form of an R Markdown (*Rmd) file.\nDue date This homework is due on Thu, May 5th at 6:00 PM.\nHomework Solutions See here.\n","categories":"","description":"","excerpt":"\n\nSource code downloads:   [ .R ]  A. Unstranded and strand-specific …","ref":"/assignments/homework/hw07/hw07/","tags":"","title":"HW7 - RNA-Seq Analysis"},{"body":"\n\nLearning R Markdown To get started with the following homework tasks, log in to your HPCC account and then cd into /bigdata/gen242/\u003cuser_name\u003e. Then clone your new project repos linked under your name in Colum J (2nd sheet) of the Course Planning Sheet linked from Canvas here. After this, cd into your project’s repos and create as subdirectory called HW8. Subsequentially, download into the HW8 directory with wget the sample.Rmd and bibtex.bib files linked from the R Markdown Tutorial here. Next, make the following changes (Tasks 1.-8.) to the downloaded R Markdown (Rmd) file. After this render the modified Rmd to HTML format, and then submit both files to your project repos on GitHub. The changes to include in the R Markdown are:\nHomework Tasks\n At the beginning of the R Markdown report, add a short section describing the analysis steps you have chosen to perform as part of your challenge project. In the challenge project section, cite the reference(s) of the paper(s) you have chosen to present in class as part of your course project. For this add the reference in BibTeX format to the downloaded bibtex.tex file, and then cite it in the text of the Rmd file so that the properly rendered citation shows up in the text and the corresponding reference is automatically added to the reference list at the end of the R Markdown when running rmarkdown::render. Note, references in BibTeX format can be obtained from Google Scholar, Paperpile or most other reference management software. More detailed information about managing references in R Markdown files is here. Add a mathematical equation in LaTeX format to the challenge project section and make sure it renders properly when generating the HTML report. Evaluate an R expression in the text as inline R code (see here) of your challenge project, e.g. mathematical or number from an existing R oject such as mean value of the first column of the iris data.frame. Add a code chunk that auto-generates the barplot for HW3C. This barplot should be embedded in the rendered report without saving it intermediately to a file. Insert the targets file of your workflow (default toy data is sufficient) as an interactive table using the DT package. An example is given in the table section of the R Markdown manual here. Use the rmarkdown::render() function to render the report to an HTML file (details are here). Important: for this assignment it is not relevant to evaluate the code chunks for the actual analysis steps of the analysis workflow. Only the chunks required for the above tasks need to be evaluated. Also, when embedding the targets file under step 6 into the report, the interactive table generated by the DT package can only be included in the HTML version of the report. To including this table in the PDF version of the report, use the static knitr::kable option instead as outlined here. Submit the Rmd and HTML files for the report to the corresponding project repos on GitHub. Remember the details for adding, committing and pushing new files to a GitHub respos are given in the GitHub tutorial of GEN242 here.  Homework submission Please submit both the final Rmd and HTML files, where the above Tasks 1-8 have been fully addressed, to the HW8 subdirectory of your project repos on GitHub.\nDue date This homework is due in one week on Thu, May 19th at 6:00 PM.\nHomework Solution All solutions for HW08 can be looked up in the R Markdown tutorial of this class. The source code is available in the Rmd file and the rendered result in the corresponding HTML file. As in all tutorial pages of this site, the Rmd files can also be accessed via a link located in the header section of each tutorial page.\n","categories":"","description":"","excerpt":"\n\nLearning R Markdown To get started with the following homework …","ref":"/assignments/homework/hw08/hw08/","tags":"","title":"HW8 - R Markdown Exercises"},{"body":"Projects will be posted here.\n","categories":"","description":"","excerpt":"Projects will be posted here.\n","ref":"/assignments/projects/","tags":"","title":"Projects"},{"body":"\n\nIntroduction During the tutorial sessions of this class all students will perform the basic data analysis of at least two NGS Workflows including RNA-Seq and ChIP-Seq. In addition, every student will work on a Challenge Project addressing a specific data analysis task within one of the general NGS Workflows. Students will also present a scientific paper closely related to their challenge topic (see here). To facilitate teamwork and communication with instructors, each course project will be assigned a private GitHub repository.\nThe results of the Challenge Projects will be presented by each student during the last week of the course (see Slideshow Template here). In addition, each student will write a detailed analysis report for the assigned course project. This report needs to include all analysis steps of the corresponding NGS Workflow (e.g. full RNA-Seq analysis) as well as the code and results of the Challenge Project. The final project reports will be written in R Markdown. A basic tutorial on R Markdown is available here. Both the R Markdown script (.Rmd) along with the rendered HTML or PDF report will be submitted to each student’s private project GitHub repository. All helper code used for the challenge project needs to be organized as well documented R functions in each project’s *_Fct.R script. The expected structure of the final project report is outlined below.\nThe reports should be submitted to each student’s private project GitHub repository. For the report each student should create in this repository a new directory named after their workflow project and include in it the following files:\n .Rmd source script of project report Report rendered from .Rmd source in HTML or PDF format ._Fct.R file containing all helper functions written for challenge project Submission Deadline for reports: 6:00 PM, June 7th, 2022  Structure of final project report  Abstract Introduction Methods  Short description of methods used by NGS workflow Detailed description of methods used for challenge project   Results and Discussion  Includes all components of NGS workflow as well as challenge project   Conclusions Acknowledgments References Supplement (optional)  ","categories":"","description":"","excerpt":"\n\nIntroduction During the tutorial sessions of this class all students …","ref":"/assignments/projects/project_overview/","tags":"","title":"Overview of Course Projects"},{"body":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming Map reads against reference genome Perform read counting for required ranges (e.g. exonic gene ranges) Normalization of read counts Identification of differentially expressed genes (DEGs) Clustering of gene expression profiles Gene set enrichment analysis  Challenge Project: Comparison of RNA-Seq Aligners  Run workflow from start to finish (steps 1-7) on RNA-Seq data set from Howard et al. (2013) Challenge project tasks  Compare the RNA-Seq aligner HISAT2 with at least 1-2 other aligners, such as Rsubread, Star or Kallisto. Evaluate the impact of the aligner on the downstream analysis results including:  Read counts Differentially expressed genes (DEGs) Generate plots to compare the results efficiently      References  Bray NL, Pimentel H, Melsted P, Pachter L (2016) Near-optimal probabilistic RNA-seq quantification. Nat Biotechnol. doi: 10.1038/nbt.3519 PubMed Howard, B.E. et al., 2013. High-throughput RNA sequencing of pseudomonas-infected Arabidopsis reveals hidden transcriptome complexity and novel splice variants. PloS one, 8(10), p.e74183. PubMed Kim D, Pertea G, Trapnell C, Pimentel H, Kelley R, Salzberg SL (2013) TopHat2: accurate alignment of transcriptomes in the presence of insertions, deletions and gene fusions. Genome Biol. doi: 10.1186/gb-2013-14-4-r36 PubMed Kim D, Langmead B, Salzberg SL (2015) HISAT: a fast spliced aligner with low memory requirements. Nat Methods 12: 357–360 PubMed Liao Y, Smyth GK, Shi W (2013) The Subread aligner: fast, accurate and scalable read mapping by seed-and-vote. Nucleic Acids Res 41: e108 PubMed  ","categories":"","description":"","excerpt":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/01_rnaseq_aligners/","tags":"","title":"RNA-Seq - NGS Aligners"},{"body":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming Map reads against reference genome Perform read counting for required ranges (e.g. exonic gene ranges) Normalization of read counts Identification of differentially expressed genes (DEGs) Clustering of gene expression profiles Gene set enrichment analysis  Challenge Projects 1. Comparison of DEG analysis methods  Run workflow from start to finish (steps 1-7) on RNA-Seq data set from Howard et al. (2013) Challenge project tasks  Compare the DEG analysis method chosen for paper presentation with at least 1-2 additional methods (e.g. one student compares edgeR vs. baySeq, and other student DESeq2 vs. limma/voom). Assess the results as follows:  Analyze the similarities and differences in the DEG lists obtained from the two methods using intersect matrices, venn diagrams and/or upset plots. Assess the impact of the DEG method on the downstream gene set enrichment analysis? Plot the performance of the DEG methods in form of ROC curves and/or record their AUC values. A consensus DEG set or the one from the Howard et al. (2013) paper could be used as a ‘pseudo’ ground truth result.      2. Comparison of DEG analysis methods  Similar as above but with different combination of DEG methods and/or performance testing approach.  References  Howard, B.E. et al., 2013. High-throughput RNA sequencing of pseudomonas-infected Arabidopsis reveals hidden transcriptome complexity and novel splice variants. PloS one, 8(10), p.e74183. PubMed Guo Y, Li C-I, Ye F, Shyr Y (2013) Evaluation of read count based RNAseq analysis methods. BMC Genomics 14 Suppl 8: S2 PubMed Hardcastle TJ, Kelly KA (2010) baySeq: empirical Bayesian methods for identifying differential expression in sequence count data. BMC Bioinformatics 11: 422 PubMed Liu R, Holik AZ, Su S, Jansz N, Chen K, Leong HS, Blewitt ME, Asselin-Labat M-L, Smyth GK, Ritchie ME (2015) Why weight? Modelling sample and observational level variability improves power in RNA-seq analyses. Nucleic Acids Res. doi: 10.1093/nar/gkv412. PubMed Love MI, Huber W, Anders S (2014) Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol 15: 550 PubMed Zhou X, Lindsay H, Robinson MD (2014) Robustly detecting differential expression in RNA sequencing data using observation weights. Nucleic Acids Res 42: e91 PubMed  ","categories":"","description":"","excerpt":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/02_rnaseq_deg/","tags":"","title":"RNA-Seq - DEG Analysis Methods"},{"body":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming Map reads against reference genome Perform read counting for required ranges (e.g. exonic gene ranges) Normalization of read counts Identification of differentially expressed genes (DEGs) Clustering of gene expression profiles Gene set enrichment analysis  Challenge Projects 1. Cluster and network analysis methods  Run workflow from start to finish (steps 1-7) on RNA-Seq data set from Howard et al. (2013) Challenge project tasks  Compare at least 2-3 cluster analysis methods (e.g. Clust, hierarchical, k-means, Fuzzy C-Means, WGCNA, other) and assess the performance differences as follows:  Analyze the similarities and differences in the cluster groupings obtained from the two methods. Do the differences affect the results of the downstream functional enrichment analysis? Plot the performance of the clustering methods in form of ROC curves and/or record their AUC values. Functional annotations (e.g. GO, KEGG, Pfam) could be used as a benchmark for defining true results.      2. Cluster and network analysis methods  Similar as above but with different combination of clustering methods and/or performance testing approach.  References  Abu-Jamous B, Kelly S (2018) Clust: automatic extraction of optimal co-expressed gene clusters from gene expression data. Genome Biol 19: 172 PubMed Howard, B.E. et al., 2013. High-throughput RNA sequencing of pseudomonas-infected Arabidopsis reveals hidden transcriptome complexity and novel splice variants. PloS one, 8(10), p.e74183. PubMed Langfelder P, Luo R, Oldham MC, Horvath S (2011) Is my network module preserved and reproducible? PLoS Comput Biol 7: e1001057. PubMed Langfelder P, Horvath S (2008) WGCNA: an R package for weighted correlation network analysis. BMC Bioinformatics 9: 559–559. PubMed Rodriguez MZ, Comin CH, Casanova D, Bruno OM, Amancio DR, Costa L da F, Rodrigues FA (2019) Clustering algorithms: A comparative approach. PLoS One 14: e0210236. PubMed  ","categories":"","description":"","excerpt":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/03_cluster_analysis/","tags":"","title":"Cluster and Network Analysis Methods"},{"body":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming Map reads against reference genome Perform read counting for required ranges (e.g. exonic gene ranges) Normalization of read counts Identification of differentially expressed genes (DEGs) Clustering of gene expression profiles Gene set enrichment analysis  Challenge Projects Differential exon usage analysis with DEXseq  Run workflow from start to finish (steps 1-7) on RNA-Seq data set from Howard et al. (2013) Challenge project tasks  Perform differential exon analysis with DEXseq. Assess the results as follows:  Identify genes that show differential exon usage according to DEXseq. Optionally, perform functional gene set enrichment analysis on the obained gene set. Compare the results with the findings of the splice variant analysis reported by Howard et al (2013).      References  Anders S, Reyes A, Huber W (2012) Detecting differential usage of exons from RNA-seq data. Genome Res 22: 2008–2017 PubMed Howard, B.E. et al., 2013. High-throughput RNA sequencing of pseudomonas-infected Arabidopsis reveals hidden transcriptome complexity and novel splice variants. PloS one, 8(10), p.e74183. PubMed Guo Y, Li C-I, Ye F, Shyr Y (2013) Evaluation of read count based RNAseq analysis methods. BMC Genomics 14 Suppl 8: S2 PubMed Hardcastle TJ, Kelly KA (2010) baySeq: empirical Bayesian methods for identifying differential expression in sequence count data. BMC Bioinformatics 11: 422 PubMed Liu R, Holik AZ, Su S, Jansz N, Chen K, Leong HS, Blewitt ME, Asselin-Labat M-L, Smyth GK, Ritchie ME (2015) Why weight? Modelling sample and observational level variability improves power in RNA-seq analyses. Nucleic Acids Res. doi: 10.1093/nar/gkv412. PubMed Love MI, Huber W, Anders S (2014) Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol 15: 550 PubMed Zhou X, Lindsay H, Robinson MD (2014) Robustly detecting differential expression in RNA sequencing data using observation weights. Nucleic Acids Res 42: e91 PubMed  ","categories":"","description":"","excerpt":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/02_rnaseq_dex/","tags":"","title":"RNA-Seq - Differential Exon Usage Analysis"},{"body":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming Map reads against reference genome Perform read counting for required ranges (e.g. exonic gene ranges) Normalization of read counts Identification of differentially expressed genes (DEGs) Clustering of gene expression profiles Gene set enrichment analysis  Challenge Projects 1. Embedding Methods for scRNA-Seq  Run workflow from start to finish (steps 1-7) on RNA-Seq data set from Howard et al. (2013) Challenge project tasks  Compare the partition performance of at least 3 embedding methods for high-dimensional gene expression data using single cell RNA-Seq data. The dimensionality reduction methods can include PCA, MDS, SC3, isomap, t-SNE, FIt-SNE, UMAP, runUMAP in scater Bioc package, etc. To obtain meaningful test results, choose an scRNA-Seq data set (here pre-processed count data) where the correct cell clustering is known (ground truth). For simplicity the data could be obtained from the scRNAseq package (Risso and Cole, 2020) or loaded from GEO (e.g. Shulse et al., 2019). For learning purposes, organize the data in a SingleCellExperiment object. How to work with SingleCellExperiment objects with embedding methods like t-SNE, the tutorial (here) of the scran package provides an excellent introduction. Optional: plot the (partitioning) performance in the form of ROC curves and/or record their AUC values. Compare your test results with published performance test results, e.g. Sun et al. (2019) or Duò et al. (2018).    2. Embedding Methods for scRNA-Seq  Similar as above but with different combination of embedding methods and/or performance testing approach.  3. Embedding Methods for scRNA-Seq  Similar as above but with different combination of embedding methods and/or performance testing approach.  References  Duò A, Robinson MD, Soneson C (2018) A systematic performance evaluation of clustering methods for single-cell RNA-seq data. F1000Res 7: 1141. PubMed Howard, B.E. et al., 2013. High-throughput RNA sequencing of pseudomonas-infected Arabidopsis reveals hidden transcriptome complexity and novel splice variants. PloS one, 8(10), p.e74183. PubMed Kiselev VY, Kirschner K, Schaub MT, Andrews T, Yiu A, Chandra T, Natarajan KN, Reik W, Barahona M, Green AR, et al (2017) SC3: consensus clustering of single-cell RNA-seq data. Nat Methods 14: 483–486. PubMed L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9 (Nov) : 2579-2605, 2008. Linderman GC, Rachh M, Hoskins JG, Steinerberger S, Kluger Y (2019) Fast interpolation-based t-SNE for improved visualization of single-cell RNA-seq data. Nat Methods 16: 243–245 PubMed (Note: this could be used as a more recent pub on t-SNE; the speed improved version is also available for R with a C) McInnes L, Healy J, Melville J (2018) UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv Risso D, Cole M (2020). scRNAseq: Collection of Public Single-Cell RNA-Seq Datasets. R package version 2.4.0. -\u003e Choose one scRNA-Seq data set from this Bioc data package for testing embedding methods. URL Senabouth A, Lukowski SW, Hernandez JA, Andersen SB, Mei X, Nguyen QH, Powell JE (2019) ascend: R package for analysis of single-cell RNA-seq data. Gigascience. doi: 10.1093/gigascience/giz087. PubMed Shulse CN, Cole BJ, Ciobanu D, Lin J, Yoshinaga Y, Gouran M, Turco GM, Zhu Y, O’Malley RC, Brady SM, et al (2019) High-Throughput Single-Cell Transcriptome Profiling of Plant Cell Types. Cell Rep 27: 2241–2247.e4 PubMed Sun S, Zhu J, Ma Y, Zhou X (2019) Accuracy, robustness and scalability of dimensionality reduction methods for single-cell RNA-seq analysis. Genome Biol 20: 269. PubMed Sun S, Zhu J, Zhou X (2020) Statistical analysis of spatial expression patterns for spatially resolved transcriptomic studies. Nat Methods. doi: 10.1038/s41592-019-0701-7. PubMed  ","categories":"","description":"","excerpt":"\n\nRNA-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/04_scrnaseq_embedding/","tags":"","title":"Embedding Methods for scRNA-Seq"},{"body":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming Align reads to reference genome Compute read coverage across genome Peak calling with different methods and consensus peak identification Annotate peaks Differential binding analysis Gene set enrichment analysis Motif prediction to identify putative TF binding sites  Challenge Projects 1. Comparison of peak calling methods  Run workflow from start to finish (steps 1-8) on ChIP-Seq data set from Kaufman et al. (2010) Challenge project tasks  Call peaks with at least 2-3 software tools, such as MACS2, slice coverage calling (Bioc), PeakSeq, F-Seq, Homer, ChIPseqR, or CSAR. Compare the results with peaks identified by Kaufmann et al (2010) Report unique and common peaks among three methods and plot the results as venn diagrams Plot the performance of the peak callers in form of ROC plots. As true result set one can use the intersect of the peaks identified by all methods.    2. Comparison of peak calling methods  Similar as above but with different combination of peak calling methods and/or performance testing approach.  References  Feng J, Liu T, Qin B, Zhang Y, Liu XS (2012) Identifying ChIP-seq enrichment using MACS. Nat Protoc 7: 1728–1740. PubMed Kaufmann, K, F Wellmer, J M Muiño, T Ferrier, S E Wuest, V Kumar, A Serrano-Mislata, et al. 2010. “Orchestration of Floral Initiation by APETALA1.” Science 328 (5974): 85–89. PubMed Landt SG, Marinov GK, Kundaje A, Kheradpour P, Pauli F, Batzoglou S, Bernstein BE, Bickel P, Brown JB, Cayting P, et al (2012) ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia. Genome Res 22: 1813–1831. PubMed Lun ATL, Smyth GK (2014) De novo detection of differentially bound regions for ChIP-seq data using peaks and windows: controlling error rates correctly. Nucleic Acids Res 42: e95. PubMed Muiño JM, Kaufmann K, van Ham RC, Angenent GC, Krajewski P (2011) ChIP-seq Analysis in R (CSAR): An R package for the statistical detection of protein-bound genomic regions. Plant Methods 7: 11. PubMed Wilbanks EG, Facciotti MT (2010) Evaluation of algorithm performance in ChIP-seq peak detection. PLoS One. doi: 10.1371/journal.pone.0011471. PubMed Zhang Y, Liu T, Meyer CA, Eeckhoute J, Johnson DS, Bernstein BE, Nussbaum C, Myers RM, Brown M, Li W, et al (2008) Model-based analysis of ChIP-Seq (MACS). Genome Biol. doi: 10.1186/gb-2008-9-9-r137. PubMed  ","categories":"","description":"","excerpt":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/05_chipseq_peakcaller/","tags":"","title":"ChIP-Seq Peak Callers"},{"body":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming Align reads to reference genome Compute read coverage across genome Peak calling with different methods and consensus peak identification Annotate peaks Differential binding analysis Gene set enrichment analysis Motif prediction to identify putative TF binding sites  Challenge Project: Functional enrichment analysis (FEA)  Run workflow from start to finish (steps 1-8) on ChIP-Seq data set from Kaufman et al. (2010) Challenge project tasks  Perform functional enrichment analysis on the genes overlapping or downstream of the peak ranges discovered by the ChIP-Seq workflow. Compare at least 2 functional enrichment methods (e.g. GOCluster_Report, fgsea, chipenrich, goseq, GOstats) using KEGG/Reactome or Gene Ontology as functional annotation systems. Among the FEA methods include one based on the hypergeometric distribution (ORA) and one on the Gene Set Enrichment Analysis (GSEA) algorithm. Assess the results as follows:  Quantify the rank-based similarities of the functional categories among the chosen enrichment methods. Determine whether the enrichment results match the biological expectations of the experiment (e.g. are certain biological processes enriched)? Optional: visualize the results with one of the pathway or GO graph viewing tools.      References  Kaufmann, K, F Wellmer, J M Muiño, T Ferrier, S E Wuest, V Kumar, A Serrano-Mislata, et al. 2010. “Orchestration of Floral Initiation by APETALA1.” Science 328 (5974): 85–89. PubMed Sergushichev A (2016) An algorithm for fast preranked gene set enrichment analysis using cumulative statistic calculation. bioRxiv 060012 Subramanian A, Tamayo P, Mootha VK, Mukherjee S, Ebert BL, Gillette MA, Paulovich A, Pomeroy SL, Golub TR, Lander ES, et al (2005) Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc Natl Acad Sci U S A 102: 15545–15550. PubMed Welch RP, Lee C, Imbriano PM, Patil S, Weymouth TE, Smith RA, Scott LJ, Sartor MA (2014) ChIP-Enrich: gene set enrichment testing for ChIP-seq data. Nucleic Acids Res 42: e105. PubMed Young MD, Wakefield MJ, Smyth GK, Oshlack A (2010) Gene ontology analysis for RNA-seq: accounting for selection bias. Genome Biol 11: R14. PubMed  ","categories":"","description":"","excerpt":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/06_functional_enrichment/","tags":"","title":"Functional enrichment analysis (FEA)"},{"body":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming Align reads to reference genome Compute read coverage across genome Peak calling with different methods and consensus peak identification Annotate peaks Differential binding analysis Gene set enrichment analysis Motif prediction to identify putative TF binding sites  Challenge Projects 1. Functional enrichment analysis (FEA)  Run workflow from start to finish (steps 1-8) on ChIP-Seq data set from Kaufman et al. (2010) Challenge project tasks  Prioritize/rank peaks by FDR from differential binding analysis Parse peak sequences from genome Determine which motifs in the Jaspar database (motifDB) show the highest enrichment in the peak sequences. The motif enrichment tests can be performed with the PWMEnrich package. Basic starter code for accomplishing these tasks is provided here. The motif mapping can be performed with matchPWM or motifmatcher, and motif identification in databases can be performed with MotIV. To have distinct challenge project aspects for each of the two students in this project, one could use different peak ranking approaches, e.g. one ranks by FDR of differential binding analysis, and the other by coverage or p-values of peak caller.    2. Functional enrichment analysis (FEA)  Similar as above but with different combination of enrichment and/or testing methods.  References  Frith, Martin C., Yutao Fu, Liqun Yu, Jiang‐fan Chen, Ulla Hansen, and Zhiping Weng. 2004. “Detection of Functional DNA Motifs via Statistical Over‐representation.” Nucleic Acids Research 32 (4): 1372–81. PubMed Kaufmann, K, F Wellmer, J M Muiño, T Ferrier, S E Wuest, V Kumar, A Serrano-Mislata, et al. 2010. “Orchestration of Floral Initiation by APETALA1.” Science 328 (5974): 85–89. PubMed Machanick P, Bailey TL (2011) MEME-ChIP: motif analysis of large DNA datasets. Bioinformatics 27: 1696–1697. PubMed McLeay, Robert C, and Timothy L Bailey. 2010. “Motif Enrichment Analysis: A Unified Framework and an Evaluation on ChIP Data.” BMC Bioinformatics 11: 165. PubMed Tompa, M, N Li, T L Bailey, G M Church, B De Moor, E Eskin, A V Favorov, et al. 2005. “Assessing Computational Tools for the Discovery of Transcription Factor Binding Sites.” Nature Biotechnology 23 (1): 137–44. PubMed Alipanahi B, Delong A, Weirauch MT, Frey BJ (2015) Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning. Nat Biotechnol 33: 831–838. PubMed  ","categories":"","description":"","excerpt":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/07_motif_enrichment/","tags":"","title":"Motif Enrichment Analysis (MEA)"},{"body":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming Align reads to reference genome Compute read coverage across genome Peak calling with different methods and consensus peak identification Annotate peaks Differential binding analysis Gene set enrichment analysis Motif prediction to identify putative TF binding sites  Challenge Project: Drug-target analysis of proteins encoded by genes in peak regions  Run workflow from start to finish (steps 1-8) on ChIP-Seq data set from Kaufman et al. (2010) Challenge project tasks  Identify protein coding genes in peak regions Identify corresponding human orthologs Perform drug-target annotation analysis, e.g. with drugTargetInteractions package Identify similar drugs with two different structural similarity search algorithms (e.g. 2 fingerprint methods) Challenge question: which of the two structural similarity search tools identifies more similar small molecules that have annotated protein targets in ChEMBL (DrugBank). Explore options on how to visualize the performance results.    References  Chen X, Reynolds CH (2002) Performance of similarity measures in 2D fragment-based similarity searching: comparison of structural descriptors and similarity coefficients. J Chem Inf Comput Sci 42: 1407–1414 PubMed Kaufmann, K, F Wellmer, J M Muiño, T Ferrier, S E Wuest, V Kumar, A Serrano-Mislata, et al. 2010. “Orchestration of Floral Initiation by APETALA1.” Science 328 (5974): 85–89. PubMed  ","categories":"","description":"","excerpt":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/09_drug_target_analysis/","tags":"","title":"Drug-target analysis"},{"body":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming Align reads to reference genome Compute read coverage across genome Peak calling with different methods and consensus peak identification Annotate peaks Differential binding analysis Gene set enrichment analysis Motif prediction to identify putative TF binding sites  Challenge Project: Programmable graphics for visualizing genomic features  Run workflow from start to finish (steps 1-8) on ChIP-Seq data set from Kaufman et al. (2010) Challenge project tasks  This project focuses on the visualization of patterns in NGS experiments (e.g. consensus motifs in ChIP-Seq peaks) to discover novel features in genomes. The visualization backend should be based on one of the programmable and extendable R/Bioconductor environments such as ggplot2 (ggplotly), ggbio, Gviz, RCircos, etc. For instance, this could include:  The generation of motif logos (e.g. for ChIP-Seq peaks) for any number of sequence ranges of interest. Integration of the results with functional annotation information (e.g. protein families from Pfam, exonic regions coding for disordered structures), pathways and/or GO. Incorporation of quantitative information such as relative or differential abundance information obtained from the corresponding NGS profiling technology. If there is interest, a Shiny App could be included to run the developed R functions interactively from a web browser.      References  Hahne F, Ivanek R (2016). “Statistical Genomics: Methods and Protocols.” In Mathé E, Davis S (eds.), chapter Visualizing Genomic Data Using Gviz and Bioconductor, 335–351. Springer New York, New York, NY. ISBN 978-1-4939-3578-9, doi: 10.1007/978-1-4939-3578-9_16. PubMed Kaufmann, K, F Wellmer, J M Muiño, T Ferrier, S E Wuest, V Kumar, A Serrano-Mislata, et al. 2010. “Orchestration of Floral Initiation by APETALA1.” Science 328 (5974): 85–89. PubMed Yin T, Cook D, Lawrence M (2012). “ggbio: an R package for extending the grammar of graphics for genomic data.” Genome Biology, 13(8), R77. PubMed Zhang H, Meltzer P, Davis S (2013) RCircos: an R package for Circos 2D track plots. BMC Bioinformatics 14: 244–244. PubMed  ","categories":"","description":"","excerpt":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/08_genome_graphics/","tags":"","title":"Genome Summary Graphics"},{"body":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming Align reads to reference genome Compute read coverage across genome Peak calling with different methods and consensus peak identification Annotate peaks Differential binding analysis Gene set enrichment analysis Motif prediction to identify putative TF binding sites  Challenge Project: Functional enrichment analysis (FEA)  Run workflow from start to finish (steps 1-8) on ChIP-Seq data set from Kaufman et al. (2010) Challenge project tasks  Parses DNA sequences of identified peak footprints Identify in the identified peak sequences 1-2 of the following feature types:  Long non-coding RNAs (lncRNAs; Han et al., 2019; Hu et al., 2017) Open reading frames (ORFs) miRNAs Repeats      References  Kaufmann, K, F Wellmer, J M Muiño, T Ferrier, S E Wuest, V Kumar, A Serrano-Mislata, et al. 2010. “Orchestration of Floral Initiation by APETALA1.” Science 328 (5974): 85–89. PubMed Han S, Liang Y, Ma Q, Xu Y, Zhang Y, Du W, Wang C, Li Y (2019) LncFinder: an integrated platform for long non-coding RNA identification utilizing sequence intrinsic composition, structural information and physicochemical property. Brief Bioinform 20: 2009–2027 PubMed Hu L, Xu Z, Hu B, Lu ZJ (2017) COME: a robust coding potential calculation tool for lncRNA identification and characterization based on multiple features. Nucleic Acids Res 45: e2 PubMed  ","categories":"","description":"","excerpt":"\n\nChIP-Seq Workflow  Read quality assessment, filtering and trimming …","ref":"/assignments/projects/10_lncrna_orf_discovery/","tags":"","title":"lncRNAs and other features"},{"body":"\n\nBig data space on HPCC All larger data sets of the course projects will be organized in a big data space under /bigdata/gen242/\u003cuser_name\u003e. Within this space, each student will work in a subdirectory named after their project:\n /bigdata/gen242/\u003cuser_name\u003e/\u003cgithub_user_name\u003e_project  Project GitHub repositories Students will work on their course projects within GitHub repositories, one for each student. These project repositories are private and have been shared with each student. To populate a course project with an initial project workflow, please follow the instructions given in the following section.\nGenerate workflow environment with real project data  Log in to the HPCC cluster and set your working directory to bigdata or (/bigdata/gen242/\u003cuser_name\u003e) Clone the GitHub repository for your project with git clone ... (URLs listed here) and then cd into this directory. As mentioned above, the project GitHub repos follow this naming convention: \u003cgithub_user_name\u003e_project. Generate the workflow environment for your project on the HPCC cluster with genWorkenvir from systemPipeRdata. Next, cd into the directory of your workflow, delete its default data and results directories, and then substitute them with empty directories outside of your project GitHub repos as follows: mkdir ../../\u003cworkflow\u003e_data mkdir ../../\u003cworkflow\u003e_results   Within your workflow directory create symbolic links pointing to the new directories created in the previous step. For instance, the projects using the RNA-Seq workflow should create the symbolic links for their data and results directories like this: ln -s /bigdata/gen242/\u003cuser_name\u003e/rnaseq_data data ln -s /bigdata/gen242/\u003cuser_name\u003e/rnaseq_results results   Add the workflow directory to the GitHub repository of your project with git add -A and then run commit and push as outlined in the GitHub instructions of this course here. After this check whether the workflow directory and its content shows up on your project’s online repos on GitHub. Very important: make sure that the data and results are empty at this point. If not investigate why and fix the problem in the corresponding step above. Download the FASTQ files of your project with getSRAfastq (see below) to the data directory of your project. Generate a proper targets file for your project where the first column(s) point(s) to the downloaded FASTQ files. In addition, provide sample names matching the experimental design (columns: SampleNames and Factor). More details about the structure of targets files are provided here. Ready to use targets files for both the RNA-Seq and ChIP-Seq project can be downloaded as tab separated (TSV) files from here. Alternatively, one can download the corresponding Google Sheets with the read_sheet function from the googlesheets4 package (RNA-Seq GSheet and ChIP-Seq GSheet). Inspect and adjust the .param files you will be using. For instance, make sure the software modules you are loading and the path to the reference genome are correct. Every time you start working on your project you cd into the directory of the repository and then run git pull to get the latest change. When you are done, you commit and push your changes back to GitHub with git commit -am \"some edits\"; git push -u origin main.  Download of project data After logging in to one of the computer nodes via srun, open R from within the GitHub repository of your project and then run the following code section, but only those that apply to your project.\nFASTQ files from SRA Choose FASTQ data for your project  The FASTQ files for the ChIP-Seq project are from SRA study SRP002174 (Kaufman et al. 2010)  sraidv \u003c- paste(\"SRR0388\", 45:51, sep=\"\")   The FASTQ files for the RNA-Seq project are from SRA study SRP010938 (Howard et al. 2013)  sraidv \u003c- paste(\"SRR4460\", 27:44, sep=\"\")  Load libraries and modules library(systemPipeR) moduleload(\"sratoolkit/3.0.0\") system(\"vdb-config --prefetch-to-cwd\") # sets download default to current directory # system('prefetch --help') # helps to speed up fastq-dump # system('vdb-config -i') # allows to change SRA Toolkit configuration; instructions are here: https://bit.ly/3lzfU4P # system('fastq-dump --help') # below uses this one for backwards compatibility # system('fasterq-dump --help') # faster than fastq-dump  Define download function The following function downloads and extracts the FASTQ files for each project from SRA. Internally, it uses the prefetch and fastq-dump utilities of the SRA Toolkit from NCBI. The faster fasterq-dump alternative (see comment line below) is not used here for historical reasons.\ngetSRAfastq \u003c- function(sraid, threads=1) { system(paste(\"prefetch\", sraid)) # makes download faster system(paste(\"fastq-dump --split-files --gzip\", sraid)) # gzip option makes it slower but saves storage space # system(paste(\"fasterq-dump --threads 4 --split-files --progress \", sraid, \"--outdir .\")) # Faster alternative to fastq-dump unlink(x=sraid, recursive = TRUE, force = TRUE) # deletes sra download directory }  Run download Note the following performs the download in serialized mode for the chosen data set and saves the extracted FASTQ files to the current working directory.\nmydir \u003c- getwd(); setwd(\"data\") for(i in sraidv) getSRAfastq(sraid=i) setwd(mydir)  Alternatively, the download can be performed in parallelized mode with BiocParallel. Please run this version only on one of the compute nodes.\nmydir \u003c- getwd(); setwd(\"data\") # bplapply(sraidv, getSRAfastq, BPPARAM = MulticoreParam(workers=4)) setwd(mydir)  Download reference genome and annotation The following downloadRefs function downloads the Arabidopsis thaliana genome sequence and GFF file from the TAIR FTP site. It also assigns consistent chromosome identifiers to make them the same among both the genome sequence and the GFF file. This is important for many analysis routines such as the read counting in the RNA-Seq workflow.\ndownloadRefs \u003c- function(rerun=FALSE) { if(rerun==TRUE) { library(Biostrings) download.file(\"https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_chromosome_files/TAIR10_chr_all.fas\", \"./data/tair10.fasta\") dna \u003c- readDNAStringSet(\"./data/tair10.fasta\") names(dna) \u003c- paste(rep(\"Chr\", 7), c(1:5, \"M\", \"C\"), sep=\"\") # Fixes chromomse ids writeXStringSet(dna, \"./data/tair10.fasta\") download.file(\"https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_gff3/TAIR10_GFF3_genes.gff\", \"./data/tair10.gff\") download.file(\"https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_functional_descriptions\", \"./data/tair10_functional_descriptions\") } }  After importing/sourcing the above function, execute it as follows:\ndownloadRefs(rerun=TRUE)  Workflow Rmd file To run the actual data analysis workflows, the RNA-Seq project can use the systemPipeRNAseq.Rmd file obtained from the genWorkenvir(workflow='rnaseq') call directly. However, the ChIP-Seq group should use the Rmd linked on top right corner of the ChIP-Seq tutorial here and then name the downloaded file systemPipeChIPseq.Rmd. To simplify this, the ChIP-Seq group members can run from the command-line within their chipseq workflow directory the following download command.\nwget https://raw.githubusercontent.com/tgirke/GEN242/main/static/custom/spWFtemplates/spchipseq.Rmd -O systemPipeChIPseq.Rmd  This will assign the proper file name and overwrite the preloaded version of this file that has the same name.\nRecommendations for running workflows Run instructions The following provides recommendations and additional options to consider for running and modifying workflows. This also includes parallelization settings for the specific data used by the class projects. Note, additional details can be found in this and other sections of the workflow introduction tutorial here.\nlibrary(systemPipeR) sal \u003c- SPRproject() # when running a WF for first time sal sal \u003c- importWF(sal, file_path = \"systemPipeRNAseq.Rmd\") # populates sal with WF steps defined in Rmd sal sal \u003c- SPRproject(resume=TRUE) # when restarting a WF, skip above steps and resume WF with this command getRversion() # should be 4.1.2 or 4.2.0. Note, R version can be changed with `module load ...` system(\"hostname\") # should return number of a compute node; if not close Nvim-R session, log in to a compute node with srun and then restart Nvim-R session # sal \u003c- runWF(sal) # runs WF serialized. Not recommended since this will take much longer than parallel mode introduced below by taking advantage of resource allocation resources \u003c- list(conffile=\".batchtools.conf.R\", template=\"batchtools.slurm.tmpl\", Njobs=18, # chipseq should use here number of fastq files (7 or 8) walltime=180, ## minutes ntasks=1, ncpus=4, memory=4096, ## Mb partition = \"gen242\" ) ## For RNA-Seq project use: sal \u003c- addResources(sal, step = c(\"preprocessing\", \"trimming\", \"hisat2_mapping\"), resources = resources) # parallelizes time consuming computations assigned to `step` argument ## For ChIP-Seq project use: sal \u003c- addResources(sal, step = c(\"preprocessing\", \"bowtie2_alignment\"), resources = resources) sal \u003c- runWF(sal) # runs entire workflow; specific steps can be executed by assigning their corresponding position numbers within the workflow to the `steps` argument (see ?runWF) sal \u003c- renderReport(sal) # after workflow has completed render Rmd to HTML report (default name is SPR_Report.html) and view it via web browser which requires symbolic link in your ~/.html folder. rmarkdown::render(\"systemPipeRNAseq.Rmd\", clean=TRUE, output_format=\"BiocStyle::html_document\") # Alternative approach for rendering report from Rmd file instead of sal object  Modify a workflow If needed one can modify existing workflow steps in a pre-populated SYSargsList object, and potentially already executed WF, with the replaceStep(sal) \u003c- replacement function. The following gives an example where step number 3 in a SYSargsList (sal) object will be updated with modified or new code. Note, this is a generalized example where the user needs to insert the code lines and also adjust the values assigned to the arguments: step_name and dependency. Additional details on this topic are available in the corresponding section of systemPipeR’s introductory tutorial here.\nreplaceStep(sal, step=3) \u003c- LineWise( code = { \u003c\u003c my modified code lines \u003e\u003e }, step_name = \u003c\u003c \"my_step_name\" \u003e\u003e, dependency = \u003c\u003c \"my_dependency\" \u003e\u003e)  Subsequently, one can rerun the corresponding step (here 3) as follows:\nrunWF(sal, steps=3)  Note, any step in a workflow can only be run in isolation if its expected input exists (see dependency).\nAdding steps to a workflow New steps can be added to the Rmd file of a workflow by inserting new R Markdown code chunks starting and ending with the usual appendStep\u003c- syntax and then creating a new SYSargsList instance with importWF that contain the new step(s). To add steps to a pre-populated SYSargsList object, one can use the after argument of the appendStep\u003c- function. The following example will add a new step after position 3 to the corresponding sal object. This can be useful if a longer workflow has already been completed and one only wants to make some refinements without re-running the entire workflow.\nappendStep(sal, after=3) \u003c- \u003c\u003c my_step_code \u003e\u003e  Submit workflow from command-line to cluster In addition to running workflows within interactive R sessions, after logging in to a computer node with srun, one can execute them entirely from the command-line by including the relevant workflow run instructions in an R script. The R script can then be submitted via a Slurm submission script to the cluster. The following gives an example for the RNA-Seq workflow (ChIP-Seq version requires only minor adjustments). Additional details on this topic are available in the corresponding section of systemPipeR’s introductory tutorial here.\n R script: wf_run_script.R Slurm submission script: wf_run_script.sh  To test this out, users can generate in their user account of the cluster a workflow environment populated with the toy data as outlined here). After this, it is best to create within the workflow directory a subdirectory, e.g. called cl_sbatch_run, and then save the above two files to this subdirectory. Next, the parameters in both files need to be adjusted to match the type of workflow and the required computing resources. This includes the name of the Rmd file and scheduler resource settings such as: partition, Njobs, walltime, memory, etc. After all relevant settings have been set correctly, one can execute the workflow with sbatch within the cl_sbatch_run directory as follows:\nsbatch wf_run_script.sh  After the submission to the cluster, one usually should check its status and progress with squeue -u \u003cusername\u003e as well a by monitoring the content of the slurm-\u003cjobid\u003e.out file generated by the scheduler in the same directory. This file contains most of the STDOUT and STDERROR generated by a cluster job. Once everything is working on the toy data, users can run the workflow on the real data the same way.\n","categories":"","description":"","excerpt":"\n\nBig data space on HPCC All larger data sets of the course projects …","ref":"/assignments/projects/project_data/","tags":"","title":"Project Data Management and Run Instructions"},{"body":"Presentation will be posted here.\n","categories":"","description":"","excerpt":"Presentation will be posted here.\n","ref":"/assignments/presentations/","tags":"","title":"Project and Paper Presentations"},{"body":"Overview Each student has been assigned one journal publication to present in class. The expected structure of the paper presentations is outlined in this Slideshow Template. A detailed presentation schedule is available on the internal Course Schedule. The following lists the assigned papers organized by course project topics.\nPublications organized by course projects All references in Paperpile\nRNA-Seq Aligners  Kim D, Langmead B, Salzberg SL (2015) HISAT: a fast spliced aligner with low memory requirements. Nat Methods 12: 357–360. PubMed  DEG Methods Zhou X, Lindsay H, Robinson MD (2014) Robustly detecting differential expression in RNA sequencing data using observation weights. Nucleic Acids Res 42: e91. PubMed Love MI, Huber W, Anders S (2014) Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol 15: 550. PubMed  Differential Exon Analysis Anders S, Reyes A, Huber W (2012) Detecting differential usage of exons from RNA-seq data. Genome Res 22: 2008–2017. PubMed  Sequence Classification Han S, Liang Y, Ma Q, Xu Y, Zhang Y, Du W, Wang C, Li Y (2019) LncFinder: an integrated platform for long non-coding RNA identification utilizing sequence intrinsic composition, structural information and physicochemical property. Brief Bioinform 20: 2009–2027. PubMed  Clustering and Network Analysis Rodriguez MZ, Comin CH, Casanova D, Bruno OM, Amancio DR, Costa L da F, Rodrigues FA (2019) Clustering algorithms: A comparative approach. PLoS One 14: e0210236. PubMed Abu-Jamous B, Kelly S (2018) Clust: automatic extraction of optimal co-expressed gene clusters from genhttps://pubmed.ncbi.nlm.nih.gov/30359297/e expression data. Genome Biol 19: 172. PubMed  Embedding of High-dimensional scRNA-Seq Data Sun S, Zhu J, Ma Y, Zhou X (2019) Accuracy, robustness and scalability of dimensionality reduction methods for single-cell RNA-seq analysis. Genome Biol 20: 269. PubMed  ChIP-Seq Peak Callers Feng J, Liu T, Qin B, Zhang Y, Liu XS (2012) Identifying ChIP-seq enrichment using MACS. Nat Protoc 7: 1728–1740. PubMed  Functional Enrichment Analysis Subramanian A, Tamayo P, Mootha VK, Mukherjee S, Ebert BL, Gillette MA, Paulovich A, Pomeroy SL, Golub TR, Lander ES, et al (2005) Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc Natl Acad Sci U S A 102: 15545–15550. PubMed  Drug-Target Analysis Chen X, Reynolds CH (2002) Performance of similarity measures in 2D fragment-based similarity searching: comparison of structural descriptors and similarity coefficients. J Chem Inf Comput Sci 42: 1407–1414. PubMed  ","categories":"","description":"","excerpt":"Overview Each student has been assigned one journal publication to …","ref":"/assignments/presentations/paper_presentations/","tags":"","title":"Student Paper Presentations"},{"body":"Suggestions  Single cell profiling (e.g. scRNA-Seq) Multi-Omics analysis Comparative genomics (e.g. ortholog assignments and/or assembly) Tool development (e.g. package development)  ","categories":"","description":"...","excerpt":"...","ref":"/blog/2022/03/22/special-topics/","tags":"","title":"Special Topics"},{"body":"Welcome to GEN242 - Spring 2022  This class will be instructed entirely online via Zoom. The Zoom URLs for lectures, discussion sections and office hours will be provided shortly before the class starts. First Lecture: 02:00-03:20 PM, Tue, March 29, 2022  ","categories":"","description":"...","excerpt":"...","ref":"/blog/2022/03/22/first-day-of-instructions/","tags":"","title":"First Day of Instructions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/about/","tags":"","title":"About GEN242"},{"body":"","categories":"","description":"","excerpt":"","ref":"/assignments/","tags":"","title":"Assignments"},{"body":"  #td-cover-block-0 { background-image: url(\"background.jpg\") }  Data Analysis in Genome Biology  --  About Course  Piazza   Instructors     GEN242 is a graduate class taught at the University of California, Riverside         Overview This course introduces algorithms, statistical methods and data analysis programming routines relevant for genome biology. It consists of three main components: lectures, hands-on practicals and student course projects. The lecture topics cover databases, sequence (NGS) analysis, phylogenetics, comparative genomics, genome-wide profiling methods, network biology and more. The hands-on practicals include homework assignments and course projects focusing on data analysis programming of next generation genome data using command-line tools on a computer cluster and the programming environment R. Depending on student interests, one or more specialty topics may be included, such as the analysis of single cell (e.g. scRNA-Seq) experiments, multi-omics data, or the development of web-based analysis tools (e.g. Shiny Apps).  Who should take this class? Students with a strong interest and motivation in acquiring the skills required for mastering the computational aspects of modern genome research. The class is mainly targeting graduate students but senior undergraduate students are welcome to enroll as well. The main audience of this class usually includes students from bioscience, biomedical and bioengineering programs as well as CS and statistics students with interest in computational biology.  Can I audit this class? It is possible to audit this class. However, due to the emphasis on active participation in practicals and course projects, students usually learn much more if they enroll into the class rather than auditing it in a passive manner.      University of California, Riverside    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: url(\"background.jpg\") }  Data …","ref":"/","tags":"","title":"GEN242"},{"body":"","categories":"","description":"The pages under this _Internal Section_ provide information about internal resources that are mainly relevant for the instructor(s) of this class.","excerpt":"The pages under this _Internal Section_ provide information about …","ref":"/about/internal/","tags":"","title":"Internal Resources"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases. …","ref":"/blog/","tags":"","title":"GEN242 News"},{"body":"This page provides URLs to external resources  Piazza GitHub Repo Bioconductor Hugo, Docsy and R  ","categories":"","description":"","excerpt":"This page provides URLs to external resources  Piazza GitHub Repo …","ref":"/external_resources/","tags":"","title":"Links"},{"body":"pre code { white-space: pre !important; overflow-x: scroll !important; word-break: keep-all !important; word-wrap: initial !important; }  document.addEventListener(\"DOMContentLoaded\", function() { document.querySelector(\"h1\").className = \"title\"; });  document.addEventListener(\"DOMContentLoaded\", function() { var links = document.links; for (var i = 0, linksLength = links.length; i Introduction Users want to provide here background information about the design of their RNA-Seq project.\nSamples and environment settings Environment settings and input data systemPipeRdata package is a helper package to generate a fully populated systemPipeR workflow environment in the current working directory with a single command. All the instruction for generating the workflow are provide in the systemPipeRdata vignette here.\nsystemPipeRdata::genWorkenvir(workflow = \"rnaseq\", mydirname = \"rnaseq\") setwd(\"rnaseq\")  Typically, the user wants to record here the sources and versions of the reference genome sequence along with the corresponding annotations. In the provided sample data set all data inputs are stored in a data subdirectory and all results will be written to a separate results directory, while the systemPipeRNAseq.Rmd workflow and the targets file are expected to be located in the parent directory.\nThe chosen data set used by this report SRP010938 contains 18 paired-end (PE) read sets from Arabidposis thaliana (Howard et al. 2013). To minimize processing time during testing, each FASTQ file has been subsetted to 90,000-100,000 randomly sampled PE reads that map to the first 100,000 nucleotides of each chromosome of the A. thaliana genome. The corresponding reference genome sequence (FASTA) and its GFF annotation files have been truncated accordingly. This way the entire test sample data set is less than 200MB in storage space. A PE read set has been chosen for this test data set for flexibility, because it can be used for testing both types of analysis routines requiring either SE (single end) reads or PE reads.\nTo work with real data, users want to organize their own data similarly and substitute all test data for their own data. To rerun an established workflow on new data, the initial targets file along with the corresponding FASTQ files are usually the only inputs the user needs to provide.\nFor more details, please consult the documentation here. More information about the targets files from systemPipeR can be found here.\nExperiment definition provided by targets file The targets file defines all FASTQ files and sample comparisons of the analysis workflow.\ntargetspath \u003c- system.file(\"extdata\", \"targetsPE.txt\", package = \"systemPipeR\") targets \u003c- read.delim(targetspath, comment.char = \"#\") targets[1:4, -c(5, 6)]  ## FileName1 FileName2 ## 1 ./data/SRR446027_1.fastq.gz ./data/SRR446027_2.fastq.gz ## 2 ./data/SRR446028_1.fastq.gz ./data/SRR446028_2.fastq.gz ## 3 ./data/SRR446029_1.fastq.gz ./data/SRR446029_2.fastq.gz ## 4 ./data/SRR446030_1.fastq.gz ./data/SRR446030_2.fastq.gz ## SampleName Factor Date ## 1 M1A M1 23-Mar-2012 ## 2 M1B M1 23-Mar-2012 ## 3 A1A A1 23-Mar-2012 ## 4 A1B A1 23-Mar-2012  To work with custom data, users need to generate a targets file containing the paths to their own FASTQ files.\nWorkflow environment systemPipeR workflows can be designed and built from start to finish with a single command, importing from an R Markdown file or stepwise in interactive mode from the R console.\nThis tutorial will demonstrate how to build the workflow in an interactive mode, appending each step. The workflow is constructed by connecting each step via appendStep method. Each SYSargsList instance contains instructions for processing a set of input files with a specific command-line or R software and the paths to the corresponding outfiles generated by a particular command-line software/step.\nTo create a workflow within systemPipeR, we can start by defining an empty container and checking the directory structure:\nlibrary(systemPipeR) sal \u003c- SPRproject() sal  Required packages and resources The systemPipeR package needs to be loaded (H Backman and Girke 2016).\nappendStep(sal) \u003c- LineWise(code = { library(systemPipeR) }, step_name = \"load_SPR\")  Read preprocessing Preprocessing with preprocessReads function The function preprocessReads allows to apply predefined or custom read preprocessing functions to all FASTQ files referenced in a SYSargsList container, such as quality filtering or adapter trimming routines. Internally, preprocessReads uses the FastqStreamer function from the ShortRead package to stream through large FASTQ files in a memory-efficient manner. The following example performs adapter trimming with the trimLRPatterns function from the Biostrings package.\nHere, we are appending this step to the SYSargsList object created previously. All the parameters are defined on the preprocessReads/preprocessReads-pe.yml file.\nappendStep(sal) \u003c- SYSargsList(step_name = \"preprocessing\", targets = \"targetsPE.txt\", dir = TRUE, wf_file = \"preprocessReads/preprocessReads-pe.cwl\", input_file = \"preprocessReads/preprocessReads-pe.yml\", dir_path = system.file(\"extdata/cwl\", package = \"systemPipeR\"), inputvars = c(FileName1 = \"_FASTQ_PATH1_\", FileName2 = \"_FASTQ_PATH2_\", SampleName = \"_SampleName_\"), dependency = c(\"load_SPR\"))  After the preprocessing step, the outfiles files can be used to generate the new targets files containing the paths to the trimmed FASTQ files. The new targets information can be used for the next workflow step instance, e.g. running the NGS alignments with the trimmed FASTQ files. The appendStep function is automatically handling this connectivity between steps. Please check the next step for more details.\nThe following example shows how one can design a custom read ‘preprocessReads’ function using utilities provided by the ShortRead package, and then run it in batch mode with the ‘preprocessReads’ function. Here, it is possible to replace the function used on the preprocessing step and modify the sal object. Because it is a custom function, it is necessary to save the part in the R object, and internally the preprocessReads.doc.R is loading the custom function. If the R object is saved with a different name (here \"param/customFCT.RData\"), please replace that accordingly in the preprocessReads.doc.R.\nPlease, note that this step is not added to the workflow, here just for demonstration.\nFirst, we defined the custom function in the workflow:\nappendStep(sal) \u003c- LineWise(code = { filterFct \u003c- function(fq, cutoff = 20, Nexceptions = 0) { qcount \u003c- rowSums(as(quality(fq), \"matrix\") \u003c= cutoff, na.rm = TRUE) # Retains reads where Phred scores are \u003e= cutoff # with N exceptions fq[qcount \u003c= Nexceptions] } save(list = ls(), file = \"param/customFCT.RData\") }, step_name = \"custom_preprocessing_function\", dependency = \"preprocessing\")  After, we can edit the input parameter:\nyamlinput(sal, \"preprocessing\")$Fct yamlinput(sal, \"preprocessing\", \"Fct\") \u003c- \"'filterFct(fq, cutoff=20, Nexceptions=0)'\" yamlinput(sal, \"preprocessing\")$Fct ## check the new function cmdlist(sal, \"preprocessing\", targets = 1) ## check if the command line was updated with success  Read trimming with Trimmomatic Trimmomatic software (Bolger, Lohse, and Usadel 2014) performs a variety of useful trimming tasks for Illumina paired-end and single ended data. Here, an example of how to perform this task using parameters template files for trimming FASTQ files.\nThis step is optional.\nappendStep(sal) \u003c- SYSargsList(step_name = \"trimming\", targets = \"targetsPE.txt\", wf_file = \"trimmomatic/trimmomatic-pe.cwl\", input_file = \"trimmomatic/trimmomatic-pe.yml\", dir_path = system.file(\"extdata/cwl\", package = \"systemPipeR\"), inputvars = c(FileName1 = \"_FASTQ_PATH1_\", FileName2 = \"_FASTQ_PATH2_\", SampleName = \"_SampleName_\"), dependency = \"load_SPR\", run_step = \"optional\")  FASTQ quality report The following seeFastq and seeFastqPlot functions generate and plot a series of useful quality statistics for a set of FASTQ files, including per cycle quality box plots, base proportions, base-level quality trends, relative k-mer diversity, length, and occurrence distribution of reads, number of reads above quality cutoffs and mean quality distribution. The results are written to a PDF file named fastqReport.pdf.\nappendStep(sal) \u003c- LineWise(code = { fastq \u003c- getColumn(sal, step = \"preprocessing\", \"targetsWF\", column = 1) fqlist \u003c- seeFastq(fastq = fastq, batchsize = 10000, klength = 8) pdf(\"./results/fastqReport.pdf\", height = 18, width = 4 * length(fqlist)) seeFastqPlot(fqlist) dev.off() }, step_name = \"fastq_report\", dependency = \"preprocessing\")  Figure 1: FASTQ quality report for 18 samples\n  Alignments Read mapping with HISAT2 The following steps will demonstrate how to use the short read aligner Hisat2 (Kim, Langmead, and Salzberg 2015). First, the Hisat2 index needs to be created.\nappendStep(sal) \u003c- SYSargsList(step_name = \"hisat2_index\", dir = FALSE, targets = NULL, wf_file = \"hisat2/hisat2-index.cwl\", input_file = \"hisat2/hisat2-index.yml\", dir_path = \"param/cwl\", dependency = \"load_SPR\")  HISAT2 mapping The parameter settings of the aligner are defined in the workflow_hisat2-pe.cwl and workflow_hisat2-pe.yml files. The following shows how to construct the corresponding SYSargsList object.\nappendStep(sal) \u003c- SYSargsList(step_name = \"hisat2_mapping\", dir = TRUE, targets = \"preprocessing\", wf_file = \"workflow-hisat2/workflow_hisat2-pe.cwl\", input_file = \"workflow-hisat2/workflow_hisat2-pe.yml\", dir_path = \"param/cwl\", inputvars = c(preprocessReads_1 = \"_FASTQ_PATH1_\", preprocessReads_2 = \"_FASTQ_PATH2_\", SampleName = \"_SampleName_\"), rm_targets_col = c(\"FileName1\", \"FileName2\"), dependency = c(\"preprocessing\", \"hisat2_index\"))  To double-check the command line for each sample, please use the following:\ncmdlist(sal, step = \"hisat2_mapping\", targets = 1)  Read and alignment stats The following provides an overview of the number of reads in each sample and how many of them aligned to the reference.\nappendStep(sal) \u003c- LineWise(code = { fqpaths \u003c- getColumn(sal, step = \"preprocessing\", \"targetsWF\", column = \"FileName1\") bampaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") read_statsDF \u003c- alignStats(args = bampaths, fqpaths = fqpaths, pairEnd = TRUE) write.table(read_statsDF, \"results/alignStats.xls\", row.names = FALSE, quote = FALSE, sep = \"\\t\") }, step_name = \"align_stats\", dependency = \"hisat2_mapping\")  Create symbolic links for viewing BAM files in IGV The symLink2bam function creates symbolic links to view the BAM alignment files in a genome browser such as IGV without moving these large files to a local system. The corresponding URLs are written to a file with a path specified under urlfile, here IGVurl.txt. Please replace the directory and the user name.\nappendStep(sal) \u003c- LineWise(code = { bampaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") symLink2bam(sysargs = bampaths, htmldir = c(\"~/.html/\", \"somedir/\"), urlbase = \"http://cluster.hpcc.ucr.edu/~tgirke/\", urlfile = \"./results/IGVurl.txt\") }, step_name = \"bam_IGV\", dependency = \"hisat2_mapping\", run_step = \"optional\")  Read quantification Reads overlapping with annotation ranges of interest are counted for each sample using the summarizeOverlaps function (Lawrence et al. 2013). The read counting is preformed for exon gene regions in a non-strand-specific manner while ignoring overlaps among different genes. Subsequently, the expression count values are normalized by reads per kp per million mapped reads (RPKM). The raw read count table (countDFeByg.xls) and the corresponding RPKM table (rpkmDFeByg.xls) are written to separate files in the directory of this project. Parallelization is achieved with the BiocParallel package, here using 4 CPU cores.\nCreate a database for gene annotation appendStep(sal) \u003c- LineWise(code = { library(GenomicFeatures) txdb \u003c- suppressWarnings(makeTxDbFromGFF(file = \"data/tair10.gff\", format = \"gff\", dataSource = \"TAIR\", organism = \"Arabidopsis thaliana\")) saveDb(txdb, file = \"./data/tair10.sqlite\") }, step_name = \"create_db\", dependency = \"hisat2_mapping\")  Read counting with summarizeOverlaps in parallel mode using multiple cores appendStep(sal) \u003c- LineWise(code = { library(GenomicFeatures) library(BiocParallel) txdb \u003c- loadDb(\"./data/tair10.sqlite\") outpaths \u003c- getColumn(sal, step = \"hisat2_mapping\", \"outfiles\", column = \"samtools_sort_bam\") eByg \u003c- exonsBy(txdb, by = c(\"gene\")) bfl \u003c- BamFileList(outpaths, yieldSize = 50000, index = character()) multicoreParam \u003c- MulticoreParam(workers = 4) register(multicoreParam) registered() counteByg \u003c- bplapply(bfl, function(x) summarizeOverlaps(eByg, x, mode = \"Union\", ignore.strand = TRUE, inter.feature = FALSE, singleEnd = FALSE, BPPARAM = multicoreParam)) countDFeByg \u003c- sapply(seq(along = counteByg), function(x) assays(counteByg[[x]])$counts) rownames(countDFeByg) \u003c- names(rowRanges(counteByg[[1]])) colnames(countDFeByg) \u003c- names(bfl) rpkmDFeByg \u003c- apply(countDFeByg, 2, function(x) returnRPKM(counts = x, ranges = eByg)) write.table(countDFeByg, \"results/countDFeByg.xls\", col.names = NA, quote = FALSE, sep = \"\\t\") write.table(rpkmDFeByg, \"results/rpkmDFeByg.xls\", col.names = NA, quote = FALSE, sep = \"\\t\") ## Creating a SummarizedExperiment object colData \u003c- data.frame(row.names = SampleName(sal, \"hisat2_mapping\"), condition = getColumn(sal, \"hisat2_mapping\", position = \"targetsWF\", column = \"Factor\")) colData$condition \u003c- factor(colData$condition) countDF_se \u003c- SummarizedExperiment::SummarizedExperiment(assays = countDFeByg, colData = colData) ## Add results as SummarizedExperiment to the workflow ## object SE(sal, \"read_counting\") \u003c- countDF_se }, step_name = \"read_counting\", dependency = \"create_db\")  When providing a BamFileList as in the example above, summarizeOverlaps methods use by default bplapply and use the register interface from BiocParallel package. If the number of workers is not set, MulticoreParam will use the number of cores returned by parallel::detectCores(). For more information, please check help(\"summarizeOverlaps\") documentation.\nNote, for most statistical differential expression or abundance analysis methods, such as edgeR or DESeq2, the raw count values should be used as input. The usage of RPKM values should be restricted to specialty applications required by some users, e.g. manually comparing the expression levels among different genes or features.\nSample-wise correlation analysis The following computes the sample-wise Spearman correlation coefficients from the rlog transformed expression values generated with the DESeq2 package. After transformation to a distance matrix, hierarchical clustering is performed with the hclust function and the result is plotted as a dendrogram (also see file sample_tree.pdf).\nappendStep(sal) \u003c- LineWise(code = { library(DESeq2, quietly = TRUE) library(ape, warn.conflicts = FALSE) ## Extracting SummarizedExperiment object se \u003c- SE(sal, \"read_counting\") dds \u003c- DESeqDataSet(se, design = ~condition) d \u003c- cor(assay(rlog(dds)), method = \"spearman\") hc \u003c- hclust(dist(1 - d)) pdf(\"results/sample_tree.pdf\") plot.phylo(as.phylo(hc), type = \"p\", edge.col = \"blue\", edge.width = 2, show.node.label = TRUE, no.margin = TRUE) dev.off() }, step_name = \"sample_tree\", dependency = \"read_counting\")  Figure 2: Correlation dendrogram of samples\n  Analysis of DEGs The analysis of differentially expressed genes (DEGs) is performed with the glm method of the edgeR package (Robinson, McCarthy, and Smyth 2010). The sample comparisons used by this analysis are defined in the header lines of the targets.txt file starting with \u003cCMP\u003e.\nRun edgeR appendStep(sal) \u003c- LineWise(code = { library(edgeR) countDF \u003c- read.delim(\"results/countDFeByg.xls\", row.names = 1, check.names = FALSE) cmp \u003c- readComp(stepsWF(sal)[[\"hisat2_mapping\"]], format = \"matrix\", delim = \"-\") edgeDF \u003c- run_edgeR(countDF = countDF, targets = targetsWF(sal)[[\"hisat2_mapping\"]], cmp = cmp[[1]], independent = FALSE, mdsplot = \"\") }, step_name = \"run_edger\", dependency = \"read_counting\")  Add gene descriptions appendStep(sal) \u003c- LineWise(code = { library(\"biomaRt\") m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"https://plants.ensembl.org\") desc \u003c- getBM(attributes = c(\"tair_locus\", \"description\"), mart = m) desc \u003c- desc[!duplicated(desc[, 1]), ] descv \u003c- as.character(desc[, 2]) names(descv) \u003c- as.character(desc[, 1]) edgeDF \u003c- data.frame(edgeDF, Desc = descv[rownames(edgeDF)], check.names = FALSE) write.table(edgeDF, \"./results/edgeRglm_allcomp.xls\", quote = FALSE, sep = \"\\t\", col.names = NA) }, step_name = \"custom_annot\", dependency = \"run_edger\")  Plot DEG results Filter and plot DEG results for up and down regulated genes. The definition of up and down is given in the corresponding help file. To open it, type ?filterDEGs in the R console.\nappendStep(sal) \u003c- LineWise(code = { edgeDF \u003c- read.delim(\"results/edgeRglm_allcomp.xls\", row.names = 1, check.names = FALSE) pdf(\"results/DEGcounts.pdf\") DEG_list \u003c- filterDEGs(degDF = edgeDF, filter = c(Fold = 2, FDR = 20)) dev.off() write.table(DEG_list$Summary, \"./results/DEGcounts.xls\", quote = FALSE, sep = \"\\t\", row.names = FALSE) }, step_name = \"filter_degs\", dependency = \"custom_annot\")  Venn diagrams of DEG sets The overLapper function can compute Venn intersects for large numbers of sample sets (up to 20 or more) and plots 2-5 way Venn diagrams. A useful feature is the possibility to combine the counts from several Venn comparisons with the same number of sample sets in a single Venn diagram (here for 4 up and down DEG sets).\nappendStep(sal) \u003c- LineWise(code = { vennsetup \u003c- overLapper(DEG_list$Up[6:9], type = \"vennsets\") vennsetdown \u003c- overLapper(DEG_list$Down[6:9], type = \"vennsets\") pdf(\"results/vennplot.pdf\") vennPlot(list(vennsetup, vennsetdown), mymain = \"\", mysub = \"\", colmode = 2, ccol = c(\"blue\", \"red\")) dev.off() }, step_name = \"venn_diagram\", dependency = \"filter_degs\")  GO term enrichment analysis Obtain gene-to-GO mappings The following shows how to obtain gene-to-GO mappings from biomaRt (here for A. thaliana) and how to organize them for the downstream GO term enrichment analysis. Alternatively, the gene-to-GO mappings can be obtained for many organisms from Bioconductor’s *.db genome annotation packages or GO annotation files provided by various genome databases. For each annotation this relatively slow preprocessing step needs to be performed only once. Subsequently, the preprocessed data can be loaded with the load function as shown in the next subsection.\nappendStep(sal) \u003c- LineWise(code = { library(\"biomaRt\") # listMarts() # To choose BioMart database # listMarts(host='plants.ensembl.org') m \u003c- useMart(\"plants_mart\", host = \"https://plants.ensembl.org\") # listDatasets(m) m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"https://plants.ensembl.org\") # listAttributes(m) # Choose data types you want to # download go \u003c- getBM(attributes = c(\"go_id\", \"tair_locus\", \"namespace_1003\"), mart = m) go \u003c- go[go[, 3] != \"\", ] go[, 3] \u003c- as.character(go[, 3]) go[go[, 3] == \"molecular_function\", 3] \u003c- \"F\" go[go[, 3] == \"biological_process\", 3] \u003c- \"P\" go[go[, 3] == \"cellular_component\", 3] \u003c- \"C\" go[1:4, ] if (!dir.exists(\"./data/GO\")) dir.create(\"./data/GO\") write.table(go, \"data/GO/GOannotationsBiomart_mod.txt\", quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\") catdb \u003c- makeCATdb(myfile = \"data/GO/GOannotationsBiomart_mod.txt\", lib = NULL, org = \"\", colno = c(1, 2, 3), idconv = NULL) save(catdb, file = \"data/GO/catdb.RData\") }, step_name = \"get_go_annot\", dependency = \"filter_degs\")  Batch GO term enrichment analysis Apply the enrichment analysis to the DEG sets obtained the above differential expression analysis. Note, in the following example the FDR filter is set here to an unreasonably high value, simply because of the small size of the toy data set used in this vignette. Batch enrichment analysis of many gene sets is performed with the function. When method=all, it returns all GO terms passing the p-value cutoff specified under the cutoff arguments. When method=slim, it returns only the GO terms specified under the myslimv argument. The given example shows how a GO slim vector for a specific organism can be obtained from BioMart.\nappendStep(sal) \u003c- LineWise(code = { library(\"biomaRt\") load(\"data/GO/catdb.RData\") DEG_list \u003c- filterDEGs(degDF = edgeDF, filter = c(Fold = 2, FDR = 50), plot = FALSE) up_down \u003c- DEG_list$UporDown names(up_down) \u003c- paste(names(up_down), \"_up_down\", sep = \"\") up \u003c- DEG_list$Up names(up) \u003c- paste(names(up), \"_up\", sep = \"\") down \u003c- DEG_list$Down names(down) \u003c- paste(names(down), \"_down\", sep = \"\") DEGlist \u003c- c(up_down, up, down) DEGlist \u003c- DEGlist[sapply(DEGlist, length) \u003e 0] BatchResult \u003c- GOCluster_Report(catdb = catdb, setlist = DEGlist, method = \"all\", id_type = \"gene\", CLSZ = 2, cutoff = 0.9, gocats = c(\"MF\", \"BP\", \"CC\"), recordSpecGO = NULL) m \u003c- useMart(\"plants_mart\", dataset = \"athaliana_eg_gene\", host = \"https://plants.ensembl.org\") goslimvec \u003c- as.character(getBM(attributes = c(\"goslim_goa_accession\"), mart = m)[, 1]) BatchResultslim \u003c- GOCluster_Report(catdb = catdb, setlist = DEGlist, method = \"slim\", id_type = \"gene\", myslimv = goslimvec, CLSZ = 10, cutoff = 0.01, gocats = c(\"MF\", \"BP\", \"CC\"), recordSpecGO = NULL) write.table(BatchResultslim, \"results/GOBatchSlim.xls\", row.names = FALSE, quote = FALSE, sep = \"\\t\") }, step_name = \"go_enrich\", dependency = \"get_go_annot\")  Plot batch GO term results The data.frame generated by GOCluster can be plotted with the goBarplot function. Because of the variable size of the sample sets, it may not always be desirable to show the results from different DEG sets in the same bar plot. Plotting single sample sets is achieved by subsetting the input data frame as shown in the first line of the following example.\nappendStep(sal) \u003c- LineWise(code = { gos \u003c- BatchResultslim[grep(\"M6-V6_up_down\", BatchResultslim$CLID), ] gos \u003c- BatchResultslim pdf(\"results/GOslimbarplotMF.pdf\", height = 8, width = 10) goBarplot(gos, gocat = \"MF\") goBarplot(gos, gocat = \"BP\") goBarplot(gos, gocat = \"CC\") dev.off() }, step_name = \"go_plot\", dependency = \"go_enrich\")  Figure 5: GO Slim Barplot for MF Ontology\n  Clustering and heat maps The following example performs hierarchical clustering on the rlog transformed expression matrix subsetted by the DEGs identified in the above differential expression analysis. It uses a Pearson correlation-based distance measure and complete linkage for cluster joining.\nappendStep(sal) \u003c- LineWise(code = { library(pheatmap) geneids \u003c- unique(as.character(unlist(DEG_list[[1]]))) y \u003c- assay(rlog(dds))[geneids, ] pdf(\"results/heatmap1.pdf\") pheatmap(y, scale = \"row\", clustering_distance_rows = \"correlation\", clustering_distance_cols = \"correlation\") dev.off() }, step_name = \"heatmap\", dependency = \"go_enrich\")  Figure 6: Heat Map with Hierarchical Clustering Dendrograms of DEGs\n  Version Information appendStep(sal) \u003c- LineWise(code = { sessionInfo() }, step_name = \"sessionInfo\", dependency = \"heatmap\")  Running workflow Interactive job submissions in a single machine For running the workflow, runWF function will execute all the steps store in the workflow container. The execution will be on a single machine without submitting to a queuing system of a computer cluster.\nsal \u003c- runWF(sal)  Parallelization on clusters Alternatively, the computation can be greatly accelerated by processing many files in parallel using several compute nodes of a cluster, where a scheduling/queuing system is used for load balancing.\nThe resources list object provides the number of independent parallel cluster processes defined under the Njobs element in the list. The following example will run 18 processes in parallel using each 4 CPU cores. If the resources available on a cluster allow running all 18 processes at the same time, then the shown sample submission will utilize in a total of 72 CPU cores.\nNote, runWF can be used with most queueing systems as it is based on utilities from the batchtools package, which supports the use of template files (*.tmpl) for defining the run parameters of different schedulers. To run the following code, one needs to have both a conffile (see .batchtools.conf.R samples here) and a template file (see *.tmpl samples here) for the queueing available on a system. The following example uses the sample conffile and template files for the Slurm scheduler provided by this package.\nThe resources can be appended when the step is generated, or it is possible to add these resources later, as the following example using the addResources function:\nresources \u003c- list(conffile=\".batchtools.conf.R\", template=\"batchtools.slurm.tmpl\", Njobs=18, walltime=120, ## minutes ntasks=1, ncpus=4, memory=1024, ## Mb partition = \"short\" ) sal \u003c- addResources(sal, c(\"hisat2_mapping\"), resources = resources) sal \u003c- runWF(sal)  Visualize workflow systemPipeR workflows instances can be visualized with the plotWF function.\nplotWF(sal, rstudio = TRUE)  Checking workflow status To check the summary of the workflow, we can use:\nsal statusWF(sal)  Accessing logs report systemPipeR compiles all the workflow execution logs in one central location, making it easier to check any standard output (stdout) or standard error (stderr) for any command-line tools used on the workflow or the R code stdout.\nsal \u003c- renderLogs(sal)  Funding This project is funded by NSF award ABI-1661152.\nReferences Bolger, Anthony M, Marc Lohse, and Bjoern Usadel. 2014. “Trimmomatic: A Flexible Trimmer for Illumina Sequence Data.” Bioinformatics 30 (15): 2114–20.\n H Backman, Tyler W, and Thomas Girke. 2016. “systemPipeR: NGS workflow and report generation environment.” BMC Bioinformatics 17 (1): 388. https://doi.org/10.1186/s12859-016-1241-0.\n Howard, Brian E, Qiwen Hu, Ahmet Can Babaoglu, Manan Chandra, Monica Borghi, Xiaoping Tan, Luyan He, et al. 2013. “High-Throughput RNA Sequencing of Pseudomonas-Infected Arabidopsis Reveals Hidden Transcriptome Complexity and Novel Splice Variants.” PLoS One 8 (10): e74183. https://doi.org/10.1371/journal.pone.0074183.\n Kim, Daehwan, Ben Langmead, and Steven L Salzberg. 2015. “HISAT: A Fast Spliced Aligner with Low Memory Requirements.” Nat. Methods 12 (4): 357–60.\n Lawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Comput. Biol. 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n Robinson, M D, D J McCarthy, and G K Smyth. 2010. “EdgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\n  ","categories":"","description":"","excerpt":"pre code { white-space: pre !important; overflow-x: scroll !important; …","ref":"/tutorials/systempiper/rnaseq/systempipernaseq/","tags":"","title":"RNA-Seq Workflow Template"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/slides/","tags":"","title":"Slides"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tutorials/","tags":"","title":"Tutorials"}]